<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Back-Translation Study: MedlinePlus Health Materials</title>
    <style>
        :root {
            --primary: #2563eb;
            --primary-dark: #1d4ed8;
            --success: #16a34a;
            --warning: #ca8a04;
            --gray-50: #f9fafb;
            --gray-100: #f3f4f6;
            --gray-200: #e5e7eb;
            --gray-600: #4b5563;
            --gray-800: #1f2937;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: var(--gray-800);
            background: var(--gray-50);
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            background: linear-gradient(135deg, var(--primary) 0%, var(--primary-dark) 100%);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }

        header h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        header p {
            opacity: 0.9;
            font-size: 1.1rem;
        }

        section {
            background: white;
            margin: 2rem 0;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--primary);
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--gray-200);
        }

        h3 {
            color: var(--gray-800);
            margin: 1.5rem 0 0.75rem;
        }

        p {
            margin-bottom: 1rem;
            color: var(--gray-600);
        }

        .highlight-box {
            background: var(--gray-100);
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 4px 4px 0;
        }

        .highlight-box p {
            margin: 0;
            color: var(--gray-800);
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }

        .stat-card {
            background: var(--gray-100);
            padding: 1.5rem;
            border-radius: 8px;
            text-align: center;
        }

        .stat-card .number {
            font-size: 2rem;
            font-weight: bold;
            color: var(--primary);
        }

        .stat-card .label {
            color: var(--gray-600);
            font-size: 0.9rem;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        th, td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--gray-200);
        }

        th {
            background: var(--gray-100);
            font-weight: 600;
            color: var(--gray-800);
        }

        tr:hover {
            background: var(--gray-50);
        }

        .score-high {
            color: var(--success);
            font-weight: 600;
        }

        .score-mid {
            color: var(--warning);
            font-weight: 600;
        }

        .winner {
            background: #dcfce7;
        }

        .metric-explainer {
            display: grid;
            grid-template-columns: 120px 1fr;
            gap: 0.5rem 1rem;
            margin: 1rem 0;
            padding: 1rem;
            background: var(--gray-100);
            border-radius: 8px;
        }

        .metric-explainer dt {
            font-weight: 600;
            color: var(--primary);
        }

        .metric-explainer dd {
            color: var(--gray-600);
        }

        .flow-diagram {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.5rem;
            margin: 1.5rem 0;
            flex-wrap: wrap;
        }

        .flow-box {
            background: var(--primary);
            color: white;
            padding: 0.75rem 1.25rem;
            border-radius: 6px;
            font-weight: 500;
        }

        .flow-arrow {
            color: var(--gray-600);
            font-size: 1.5rem;
        }

        .comparison-box {
            border: 2px dashed var(--gray-200);
            padding: 0.75rem 1.25rem;
            border-radius: 6px;
            color: var(--gray-600);
        }

        ul {
            margin: 1rem 0;
            padding-left: 1.5rem;
        }

        li {
            margin: 0.5rem 0;
            color: var(--gray-600);
        }

        .key-finding {
            background: #fef3c7;
            border: 1px solid #fcd34d;
            padding: 1rem 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .key-finding strong {
            color: var(--gray-800);
        }

        footer {
            text-align: center;
            padding: 2rem;
            color: var(--gray-600);
            font-size: 0.9rem;
        }

        footer a {
            color: var(--primary);
        }

        .language-note {
            font-size: 0.85rem;
            color: var(--gray-600);
            font-style: italic;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Evaluating LLM Translation Quality for Health Materials</h1>
            <p>A back-translation study using MedlinePlus documents across 8 languages and 4 LLMs</p>
        </div>
    </header>

    <div class="container">
        <section>
            <h2>The Question</h2>
            <p><strong>Can LLMs reliably translate health education materials for multilingual patient populations?</strong></p>
            <p>We evaluated four state-of-the-art LLMs on their ability to translate real MedlinePlus health documents (vaccine information sheets and cancer education materials) into 8 languages, using back-translation and comparison with professional human translations.</p>

            <div class="stats-grid">
                <div class="stat-card">
                    <div class="number">4</div>
                    <div class="label">LLM Models Tested</div>
                </div>
                <div class="stat-card">
                    <div class="number">8</div>
                    <div class="label">Languages</div>
                </div>
                <div class="stat-card">
                    <div class="number">22</div>
                    <div class="label">Documents</div>
                </div>
                <div class="stat-card">
                    <div class="number">704</div>
                    <div class="label">Translation Pairs</div>
                </div>
            </div>
        </section>

        <section>
            <h2>Our Approach</h2>

            <h3>What is Back-Translation?</h3>
            <p>Back-translation tests how well a translation preserves meaning by translating it back to the original language and comparing the result.</p>

            <div class="flow-diagram">
                <div class="flow-box">English Original</div>
                <div class="flow-arrow">→</div>
                <div class="flow-box">Target Language</div>
                <div class="flow-arrow">→</div>
                <div class="flow-box">Back to English</div>
                <div class="flow-arrow">→</div>
                <div class="comparison-box">Compare</div>
            </div>

            <h3>Two Evaluation Angles</h3>

            <div class="highlight-box">
                <p><strong>Angle 1: Meaning Preservation</strong> — How well does the LLM's back-translation match the original English? (Higher = better round-trip fidelity)</p>
            </div>

            <div class="highlight-box">
                <p><strong>Angle 2: Professional Alignment</strong> — How similar is the LLM's translation to a professional human translation? (Higher = closer to human quality)</p>
            </div>

            <h3>Understanding the Metrics</h3>
            <dl class="metric-explainer">
                <dt>BLEU</dt>
                <dd>Word/phrase overlap (0-100). Measures exact wording similarity. Sensitive to word choice.</dd>

                <dt>LaBSE</dt>
                <dd>Semantic similarity (0-1). Measures meaning regardless of exact words. Best for "do they say the same thing?"</dd>

                <dt>BERTScore</dt>
                <dd>Contextual similarity (0-1). Balances meaning and phrasing. Good overall quality indicator.</dd>

                <dt>COMET</dt>
                <dd>Neural translation quality (0-1). Trained on human judgments. Most aligned with human preferences.</dd>
            </dl>

            <p class="language-note"><strong>Note:</strong> BLEU scores are lower for languages with different scripts (Chinese, Korean, Arabic) due to tokenization differences—this doesn't indicate worse translations. LaBSE and BERTScore are more reliable for cross-script comparison.</p>
        </section>

        <section>
            <h2>Results: Which LLM Translates Best?</h2>

            <h3>Meaning Preservation (Back-Translation Quality)</h3>
            <p>How well does each model preserve meaning through the translation round-trip?</p>

            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>BLEU</th>
                        <th>LaBSE</th>
                        <th>BERTScore</th>
                        <th>Verdict</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="winner">
                        <td><strong>Claude Opus 4.5</strong></td>
                        <td class="score-high">67.8</td>
                        <td class="score-high">0.98</td>
                        <td class="score-high">0.95</td>
                        <td>Best meaning preservation</td>
                    </tr>
                    <tr>
                        <td>GPT-5.1</td>
                        <td>63.7</td>
                        <td>0.95</td>
                        <td>0.93</td>
                        <td>Strong second</td>
                    </tr>
                    <tr>
                        <td>Gemini 3 Pro</td>
                        <td>61.2</td>
                        <td>0.92</td>
                        <td>0.91</td>
                        <td>Good</td>
                    </tr>
                    <tr>
                        <td>Kimi K2</td>
                        <td>54.7</td>
                        <td>0.93</td>
                        <td>0.92</td>
                        <td>Adequate</td>
                    </tr>
                </tbody>
            </table>

            <div class="key-finding">
                <strong>Key Finding:</strong> Claude Opus 4.5 best preserves the original meaning through translation. All models achieve high semantic similarity (LaBSE > 0.92), meaning the core health information is reliably maintained.
            </div>

            <h3>Professional Alignment (LLM vs Human Translation)</h3>
            <p>How similar are LLM translations to professional human translations?</p>

            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>BLEU</th>
                        <th>COMET</th>
                        <th>BERTScore</th>
                        <th>Verdict</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="winner">
                        <td><strong>Gemini 3 Pro</strong></td>
                        <td class="score-high">38.2</td>
                        <td class="score-high">0.87</td>
                        <td>0.84</td>
                        <td>Closest to human translators</td>
                    </tr>
                    <tr>
                        <td>Claude Opus 4.5</td>
                        <td>35.8</td>
                        <td>0.86</td>
                        <td class="score-high">0.85</td>
                        <td>Very close second</td>
                    </tr>
                    <tr>
                        <td>GPT-5.1</td>
                        <td>34.8</td>
                        <td>0.86</td>
                        <td>0.84</td>
                        <td>Competitive</td>
                    </tr>
                    <tr>
                        <td>Kimi K2</td>
                        <td>34.6</td>
                        <td>0.86</td>
                        <td>0.83</td>
                        <td>Slightly behind</td>
                    </tr>
                </tbody>
            </table>

            <div class="key-finding">
                <strong>Key Finding:</strong> Gemini 3 Pro most closely matches professional human translation style, though all models are remarkably close (COMET 0.86-0.87). The differences between models are smaller here than in meaning preservation.
            </div>
        </section>

        <section>
            <h2>Results by Language</h2>
            <p>Performance varies by language, but semantic preservation (LaBSE) remains high across all.</p>

            <table>
                <thead>
                    <tr>
                        <th>Language</th>
                        <th>Meaning (BLEU)</th>
                        <th>Meaning (LaBSE)</th>
                        <th>vs Professional (BLEU)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Spanish</td>
                        <td class="score-high">68.6</td>
                        <td class="score-high">0.95</td>
                        <td class="score-high">54.3</td>
                    </tr>
                    <tr>
                        <td>Tagalog</td>
                        <td class="score-high">68.7</td>
                        <td class="score-high">0.95</td>
                        <td>43.8</td>
                    </tr>
                    <tr>
                        <td>Vietnamese</td>
                        <td>63.1</td>
                        <td class="score-high">0.95</td>
                        <td class="score-high">50.1</td>
                    </tr>
                    <tr>
                        <td>Arabic</td>
                        <td>63.8</td>
                        <td>0.94</td>
                        <td>30.8</td>
                    </tr>
                    <tr>
                        <td>Haitian Creole</td>
                        <td>61.9</td>
                        <td class="score-high">0.95</td>
                        <td>37.2</td>
                    </tr>
                    <tr>
                        <td>Russian</td>
                        <td>57.3</td>
                        <td>0.94</td>
                        <td>33.4</td>
                    </tr>
                    <tr>
                        <td>Chinese</td>
                        <td>58.0</td>
                        <td>0.94</td>
                        <td>15.5</td>
                    </tr>
                    <tr>
                        <td>Korean</td>
                        <td>53.6</td>
                        <td>0.94</td>
                        <td>21.7</td>
                    </tr>
                </tbody>
            </table>

            <p class="language-note"><strong>Why are Chinese/Korean BLEU scores low?</strong> BLEU counts word matches, but these languages use different tokenization. The LaBSE scores (0.94) confirm meaning is preserved just as well—BLEU is just the wrong metric for these scripts.</p>
        </section>

        <section>
            <h2>Key Takeaways</h2>

            <ul>
                <li><strong>All tested LLMs preserve health information reliably</strong> — Semantic similarity scores (LaBSE 0.92-0.98) indicate core medical content is maintained across translations.</li>

                <li><strong>Claude Opus 4.5 excels at faithful translation</strong> — Best at preserving exact meaning through the round-trip, making it ideal when precision matters.</li>

                <li><strong>Gemini 3 Pro writes most like human translators</strong> — Closest stylistic match to professional translations, potentially better for natural-sounding output.</li>

                <li><strong>High-resource languages perform best</strong> — Spanish, Vietnamese, and Tagalog show strongest results, likely due to more training data.</li>

                <li><strong>Choose your metric wisely</strong> — BLEU measures wording, LaBSE measures meaning. For health materials, meaning preservation (LaBSE) matters most.</li>
            </ul>
        </section>

        <section>
            <h2>Study Details</h2>

            <h3>Models Tested</h3>
            <ul>
                <li>Claude Opus 4.5 (Anthropic)</li>
                <li>GPT-5.1 (OpenAI)</li>
                <li>Gemini 3 Pro (Google)</li>
                <li>Kimi K2 (Moonshot AI)</li>
            </ul>

            <h3>Languages</h3>
            <ul>
                <li>Spanish, Chinese (Simplified), Vietnamese, Russian</li>
                <li>Arabic, Korean, Tagalog, Haitian Creole</li>
            </ul>

            <h3>Documents</h3>
            <ul>
                <li>11 CDC Vaccine Information Statements (VIS)</li>
                <li>11 American Cancer Society patient education materials</li>
                <li>All sourced from MedlinePlus with existing professional translations</li>
            </ul>

            <h3>Metrics</h3>
            <ul>
                <li><strong>BLEU</strong> — N-gram precision (sacrebleu)</li>
                <li><strong>LaBSE</strong> — Language-agnostic sentence embeddings</li>
                <li><strong>BERTScore</strong> — Contextual embedding similarity</li>
                <li><strong>COMET</strong> — Neural MT evaluation trained on human judgments</li>
            </ul>
        </section>
    </div>

    <footer>
        <p>Back-Translation Study for MedlinePlus Health Materials</p>
        <p>Data and code available on <a href="#">GitHub</a></p>
    </footer>
</body>
</html>
