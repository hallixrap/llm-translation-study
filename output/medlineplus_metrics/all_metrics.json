[
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 51.7556972172263,
    "same_lang_chrf": 77.1735186633869,
    "same_lang_bertscore": 0.8056178689002991,
    "same_lang_comet": 0.7963155508041382,
    "cross_lang_xlm_roberta": 0.8584244251251221,
    "cross_lang_labse": 0.8920117616653442,
    "cross_lang_mbert": 0.856514573097229,
    "cross_lang_comet_qe": 0.04574868083000183,
    "backtrans_bleu": 66.16787566057833,
    "backtrans_chrf": 84.25546897919432,
    "backtrans_bertscore": 0.8507856130599976,
    "prof_backtrans_bleu": 47.516049392126874,
    "prof_backtrans_chrf": 78.51898512481132,
    "prof_backtrans_bertscore": 0.8889317512512207,
    "prof_backtrans_labse": 0.9174442291259766,
    "prof_backtrans_xlm_roberta": 0.8439019322395325,
    "llm_vs_prof_backtrans_bleu": 55.93475461225988,
    "llm_vs_prof_backtrans_chrf": 77.38504546738783,
    "llm_vs_prof_backtrans_bertscore": 0.8525217771530151,
    "llm_vs_prof_backtrans_labse": 0.871094822883606
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.111812807461849,
    "same_lang_chrf": 34.861545294055446,
    "same_lang_bertscore": 0.7701408863067627,
    "same_lang_comet": 0.8473943471908569,
    "cross_lang_xlm_roberta": 0.9167145490646362,
    "cross_lang_labse": 0.920653760433197,
    "cross_lang_mbert": 0.9141767621040344,
    "cross_lang_comet_qe": 0.0430680513381958,
    "backtrans_bleu": 59.218175475131034,
    "backtrans_chrf": 81.71240851838867,
    "backtrans_bertscore": 0.8704047799110413,
    "prof_backtrans_bleu": 36.34215653887347,
    "prof_backtrans_chrf": 73.15121432303809,
    "prof_backtrans_bertscore": 0.8669426441192627,
    "prof_backtrans_labse": 0.8636139035224915,
    "prof_backtrans_xlm_roberta": 0.7065691351890564,
    "llm_vs_prof_backtrans_bleu": 43.995866712822455,
    "llm_vs_prof_backtrans_chrf": 69.54234297102775,
    "llm_vs_prof_backtrans_bertscore": 0.8527102470397949,
    "llm_vs_prof_backtrans_labse": 0.8071398735046387
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 41.98539826220749,
    "same_lang_chrf": 58.95304073627052,
    "same_lang_bertscore": 0.8005124926567078,
    "same_lang_comet": 0.867700457572937,
    "cross_lang_xlm_roberta": 0.9511513113975525,
    "cross_lang_labse": 0.9266651272773743,
    "cross_lang_mbert": 0.9609526991844177,
    "cross_lang_comet_qe": 0.029920339584350586,
    "backtrans_bleu": 69.03478015701708,
    "backtrans_chrf": 86.01336503903438,
    "backtrans_bertscore": 0.8934739232063293,
    "prof_backtrans_bleu": 44.170824374446866,
    "prof_backtrans_chrf": 76.51892223918283,
    "prof_backtrans_bertscore": 0.886208713054657,
    "prof_backtrans_labse": 0.8744797706604004,
    "prof_backtrans_xlm_roberta": 0.7328354716300964,
    "llm_vs_prof_backtrans_bleu": 51.0226271601823,
    "llm_vs_prof_backtrans_chrf": 73.3461615014076,
    "llm_vs_prof_backtrans_bertscore": 0.8735430240631104,
    "llm_vs_prof_backtrans_labse": 0.8360356092453003
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 33.60980907686815,
    "same_lang_chrf": 63.2243574145285,
    "same_lang_bertscore": 0.7631756067276001,
    "same_lang_comet": 0.8311871290206909,
    "cross_lang_xlm_roberta": 0.9538286328315735,
    "cross_lang_labse": 0.9115307331085205,
    "cross_lang_mbert": 0.9719122648239136,
    "cross_lang_comet_qe": 0.03926987946033478,
    "backtrans_bleu": 67.00873760250705,
    "backtrans_chrf": 85.84662661175437,
    "backtrans_bertscore": 0.8910536766052246,
    "prof_backtrans_bleu": 55.844643226293634,
    "prof_backtrans_chrf": 81.78291557306332,
    "prof_backtrans_bertscore": 0.8595661520957947,
    "prof_backtrans_labse": 0.9142035841941833,
    "prof_backtrans_xlm_roberta": 0.8262695074081421,
    "llm_vs_prof_backtrans_bleu": 65.77489702687441,
    "llm_vs_prof_backtrans_chrf": 80.38776447746122,
    "llm_vs_prof_backtrans_bertscore": 0.9144079089164734,
    "llm_vs_prof_backtrans_labse": 0.926253616809845
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 29.282333149618545,
    "same_lang_chrf": 60.08473563678386,
    "same_lang_bertscore": 0.7857518792152405,
    "same_lang_comet": 0.8564556241035461,
    "cross_lang_xlm_roberta": 0.9216726422309875,
    "cross_lang_labse": 0.9235111474990845,
    "cross_lang_mbert": 0.903384268283844,
    "cross_lang_comet_qe": -0.011615261435508728,
    "backtrans_bleu": 53.84643420747066,
    "backtrans_chrf": 80.53950259396785,
    "backtrans_bertscore": 0.8636257648468018,
    "prof_backtrans_bleu": 33.802590131915515,
    "prof_backtrans_chrf": 71.54129384790726,
    "prof_backtrans_bertscore": 0.8717546463012695,
    "prof_backtrans_labse": 0.8682680726051331,
    "prof_backtrans_xlm_roberta": 0.7183210253715515,
    "llm_vs_prof_backtrans_bleu": 41.45796597013109,
    "llm_vs_prof_backtrans_chrf": 69.4085660576609,
    "llm_vs_prof_backtrans_bertscore": 0.8369327187538147,
    "llm_vs_prof_backtrans_labse": 0.843778133392334
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 28.258968813070133,
    "same_lang_chrf": 49.051286844924185,
    "same_lang_bertscore": 0.8074803352355957,
    "same_lang_comet": 0.8866124153137207,
    "cross_lang_xlm_roberta": 0.9211971163749695,
    "cross_lang_labse": 0.9192870855331421,
    "cross_lang_mbert": 0.9045774936676025,
    "cross_lang_comet_qe": 0.07194662094116211,
    "backtrans_bleu": 58.8913944151939,
    "backtrans_chrf": 82.92475158954721,
    "backtrans_bertscore": 0.8712559938430786,
    "prof_backtrans_bleu": 41.16994393519332,
    "prof_backtrans_chrf": 76.07145469671129,
    "prof_backtrans_bertscore": 0.8916475772857666,
    "prof_backtrans_labse": 0.9019782543182373,
    "prof_backtrans_xlm_roberta": 0.7848224639892578,
    "llm_vs_prof_backtrans_bleu": 48.64011960040079,
    "llm_vs_prof_backtrans_chrf": 74.49315001332242,
    "llm_vs_prof_backtrans_bertscore": 0.864655077457428,
    "llm_vs_prof_backtrans_labse": 0.8592883944511414
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 30.393709317685573,
    "same_lang_chrf": 59.12042832576675,
    "same_lang_bertscore": 0.7889317274093628,
    "same_lang_comet": 0.792597770690918,
    "cross_lang_xlm_roberta": 0.9223682880401611,
    "cross_lang_labse": 0.918419599533081,
    "cross_lang_mbert": 0.8798784613609314,
    "cross_lang_comet_qe": 0.032227858901023865,
    "backtrans_bleu": 56.52038879867159,
    "backtrans_chrf": 79.29974297742633,
    "backtrans_bertscore": 0.8739911913871765,
    "prof_backtrans_bleu": 40.43313738704507,
    "prof_backtrans_chrf": 73.18862026754512,
    "prof_backtrans_bertscore": 0.867831826210022,
    "prof_backtrans_labse": 0.8892366290092468,
    "prof_backtrans_xlm_roberta": 0.721871554851532,
    "llm_vs_prof_backtrans_bleu": 47.72941885275194,
    "llm_vs_prof_backtrans_chrf": 69.35118759123145,
    "llm_vs_prof_backtrans_bertscore": 0.8974302411079407,
    "llm_vs_prof_backtrans_labse": 0.8431286215782166
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 49.40675393691729,
    "same_lang_chrf": 77.16735039713721,
    "same_lang_bertscore": 0.8374152779579163,
    "same_lang_comet": 0.8190542459487915,
    "cross_lang_xlm_roberta": 0.945878267288208,
    "cross_lang_labse": 0.9882794618606567,
    "cross_lang_mbert": 0.9414485692977905,
    "cross_lang_comet_qe": -0.18858224153518677,
    "backtrans_bleu": 66.70476035108298,
    "backtrans_chrf": 86.12432471887634,
    "backtrans_bertscore": 0.9049869179725647,
    "prof_backtrans_bleu": 46.91082719296151,
    "prof_backtrans_chrf": 77.81610674800244,
    "prof_backtrans_bertscore": 0.8797521591186523,
    "prof_backtrans_labse": 0.9091085195541382,
    "prof_backtrans_xlm_roberta": 0.8657712340354919,
    "llm_vs_prof_backtrans_bleu": 52.23045898984861,
    "llm_vs_prof_backtrans_chrf": 76.94822340061546,
    "llm_vs_prof_backtrans_bertscore": 0.8917756080627441,
    "llm_vs_prof_backtrans_labse": 0.9031244516372681
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 12.321470992750774,
    "same_lang_chrf": 32.461500310530226,
    "same_lang_bertscore": 0.7987174987792969,
    "same_lang_comet": 0.8452638387680054,
    "cross_lang_xlm_roberta": 0.9706042408943176,
    "cross_lang_labse": 0.9824488162994385,
    "cross_lang_mbert": 0.9345743060112,
    "cross_lang_comet_qe": -0.138790562748909,
    "backtrans_bleu": 71.07300964924227,
    "backtrans_chrf": 86.12513275176174,
    "backtrans_bertscore": 0.905266284942627,
    "prof_backtrans_bleu": 51.912761145164644,
    "prof_backtrans_chrf": 79.67234141797852,
    "prof_backtrans_bertscore": 0.8571138978004456,
    "prof_backtrans_labse": 0.8947300910949707,
    "prof_backtrans_xlm_roberta": 0.6991708278656006,
    "llm_vs_prof_backtrans_bleu": 52.73460010859417,
    "llm_vs_prof_backtrans_chrf": 73.95760779516887,
    "llm_vs_prof_backtrans_bertscore": 0.8713780045509338,
    "llm_vs_prof_backtrans_labse": 0.8923927545547485
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 43.32872774810813,
    "same_lang_chrf": 62.99544044904276,
    "same_lang_bertscore": 0.8231875896453857,
    "same_lang_comet": 0.808927059173584,
    "cross_lang_xlm_roberta": 0.9951134920120239,
    "cross_lang_labse": 0.9927815198898315,
    "cross_lang_mbert": 0.9893253445625305,
    "cross_lang_comet_qe": -0.17073021829128265,
    "backtrans_bleu": 69.08422574498103,
    "backtrans_chrf": 86.26282000838336,
    "backtrans_bertscore": 0.9436092972755432,
    "prof_backtrans_bleu": 58.346336732601024,
    "prof_backtrans_chrf": 81.9044006694933,
    "prof_backtrans_bertscore": 0.8795825242996216,
    "prof_backtrans_labse": 0.8950819969177246,
    "prof_backtrans_xlm_roberta": 0.7328354716300964,
    "llm_vs_prof_backtrans_bleu": 60.82991110085328,
    "llm_vs_prof_backtrans_chrf": 79.99853842266774,
    "llm_vs_prof_backtrans_bertscore": 0.8819965720176697,
    "llm_vs_prof_backtrans_labse": 0.8840588331222534
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 40.55631291541255,
    "same_lang_chrf": 72.02206399937316,
    "same_lang_bertscore": 0.7937833070755005,
    "same_lang_comet": 0.8399088382720947,
    "cross_lang_xlm_roberta": 0.9084653854370117,
    "cross_lang_labse": 0.9204864501953125,
    "cross_lang_mbert": 0.9677956700325012,
    "cross_lang_comet_qe": 0.02887958288192749,
    "backtrans_bleu": 77.28230235573706,
    "backtrans_chrf": 90.45649346545493,
    "backtrans_bertscore": 0.89020836353302,
    "prof_backtrans_bleu": 49.17321097949757,
    "prof_backtrans_chrf": 78.76917372977512,
    "prof_backtrans_bertscore": 0.8615847826004028,
    "prof_backtrans_labse": 0.9259248375892639,
    "prof_backtrans_xlm_roberta": 0.8224517107009888,
    "llm_vs_prof_backtrans_bleu": 56.7901719297619,
    "llm_vs_prof_backtrans_chrf": 76.32220698387955,
    "llm_vs_prof_backtrans_bertscore": 0.919740617275238,
    "llm_vs_prof_backtrans_labse": 0.9396235942840576
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 26.352463648848822,
    "same_lang_chrf": 58.686778732552845,
    "same_lang_bertscore": 0.8181667327880859,
    "same_lang_comet": 0.8451697826385498,
    "cross_lang_xlm_roberta": 0.9926168322563171,
    "cross_lang_labse": 0.9893061518669128,
    "cross_lang_mbert": 0.9851722717285156,
    "cross_lang_comet_qe": -0.2014980912208557,
    "backtrans_bleu": 66.69971827717188,
    "backtrans_chrf": 85.97970564990713,
    "backtrans_bertscore": 0.9314555525779724,
    "prof_backtrans_bleu": 31.581521091623042,
    "prof_backtrans_chrf": 68.26262965655391,
    "prof_backtrans_bertscore": 0.8666800260543823,
    "prof_backtrans_labse": 0.881033718585968,
    "prof_backtrans_xlm_roberta": 0.7205842733383179,
    "llm_vs_prof_backtrans_bleu": 30.957880726340278,
    "llm_vs_prof_backtrans_chrf": 60.999464120491,
    "llm_vs_prof_backtrans_bertscore": 0.8755811452865601,
    "llm_vs_prof_backtrans_labse": 0.8756292462348938
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 26.070540492471235,
    "same_lang_chrf": 43.29033793384569,
    "same_lang_bertscore": 0.8258230686187744,
    "same_lang_comet": 0.8818743228912354,
    "cross_lang_xlm_roberta": 0.9785556793212891,
    "cross_lang_labse": 0.9587119221687317,
    "cross_lang_mbert": 0.9425517320632935,
    "cross_lang_comet_qe": -0.10522614419460297,
    "backtrans_bleu": 65.58559523646198,
    "backtrans_chrf": 84.28443322692935,
    "backtrans_bertscore": 0.9074550867080688,
    "prof_backtrans_bleu": 47.718034502191976,
    "prof_backtrans_chrf": 77.72862859284194,
    "prof_backtrans_bertscore": 0.8618688583374023,
    "prof_backtrans_labse": 0.9049911499023438,
    "prof_backtrans_xlm_roberta": 0.77048259973526,
    "llm_vs_prof_backtrans_bleu": 51.347983542983464,
    "llm_vs_prof_backtrans_chrf": 74.92710309518796,
    "llm_vs_prof_backtrans_bertscore": 0.8752781748771667,
    "llm_vs_prof_backtrans_labse": 0.910736083984375
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 25.382358680687293,
    "same_lang_chrf": 55.70467616257123,
    "same_lang_bertscore": 0.8136864900588989,
    "same_lang_comet": 0.7629835605621338,
    "cross_lang_xlm_roberta": 0.9894954562187195,
    "cross_lang_labse": 0.9912797808647156,
    "cross_lang_mbert": 0.971518337726593,
    "cross_lang_comet_qe": -0.16183683276176453,
    "backtrans_bleu": 69.61606497241021,
    "backtrans_chrf": 86.54224362672124,
    "backtrans_bertscore": 0.9263986349105835,
    "prof_backtrans_bleu": 37.564630185028086,
    "prof_backtrans_chrf": 71.46320197438175,
    "prof_backtrans_bertscore": 0.8807528614997864,
    "prof_backtrans_labse": 0.8643653392791748,
    "prof_backtrans_xlm_roberta": 0.7394330501556396,
    "llm_vs_prof_backtrans_bleu": 38.53568452259862,
    "llm_vs_prof_backtrans_chrf": 65.01593587978222,
    "llm_vs_prof_backtrans_bertscore": 0.8796796798706055,
    "llm_vs_prof_backtrans_labse": 0.854512095451355
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 51.23339678763165,
    "same_lang_chrf": 76.73469026649984,
    "same_lang_bertscore": 0.8112735748291016,
    "same_lang_comet": 0.8002793788909912,
    "cross_lang_xlm_roberta": 0.8801255226135254,
    "cross_lang_labse": 0.9153045415878296,
    "cross_lang_mbert": 0.8897619247436523,
    "cross_lang_comet_qe": 0.06337928771972656,
    "backtrans_bleu": 64.12661697583167,
    "backtrans_chrf": 83.92741468784136,
    "backtrans_bertscore": 0.8651530146598816,
    "prof_backtrans_bleu": 43.451373517032465,
    "prof_backtrans_chrf": 76.62015373828955,
    "prof_backtrans_bertscore": 0.8772813081741333,
    "prof_backtrans_labse": 0.9106903076171875,
    "prof_backtrans_xlm_roberta": 0.8765947818756104,
    "llm_vs_prof_backtrans_bleu": 52.786897217694936,
    "llm_vs_prof_backtrans_chrf": 76.57559995743516,
    "llm_vs_prof_backtrans_bertscore": 0.863091230392456,
    "llm_vs_prof_backtrans_labse": 0.8686243295669556
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.695857360434392,
    "same_lang_chrf": 34.62273884845883,
    "same_lang_bertscore": 0.7673612236976624,
    "same_lang_comet": 0.8460841178894043,
    "cross_lang_xlm_roberta": 0.8294429183006287,
    "cross_lang_labse": 0.8775820136070251,
    "cross_lang_mbert": 0.8001034259796143,
    "cross_lang_comet_qe": 0.10043542087078094,
    "backtrans_bleu": 77.3780074334235,
    "backtrans_chrf": 90.42296229309144,
    "backtrans_bertscore": 0.8653176426887512,
    "prof_backtrans_bleu": 39.419048101700994,
    "prof_backtrans_chrf": 75.04649444734417,
    "prof_backtrans_bertscore": 0.8777322173118591,
    "prof_backtrans_labse": 0.8975719809532166,
    "prof_backtrans_xlm_roberta": 0.6955174207687378,
    "llm_vs_prof_backtrans_bleu": 42.159880141145834,
    "llm_vs_prof_backtrans_chrf": 66.84736631150238,
    "llm_vs_prof_backtrans_bertscore": 0.8801653385162354,
    "llm_vs_prof_backtrans_labse": 0.8458448052406311
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 48.136035448124225,
    "same_lang_chrf": 65.64212793013951,
    "same_lang_bertscore": 0.8005632758140564,
    "same_lang_comet": 0.868125319480896,
    "cross_lang_xlm_roberta": 0.8123422861099243,
    "cross_lang_labse": 0.8996231555938721,
    "cross_lang_mbert": 0.8073202967643738,
    "cross_lang_comet_qe": 0.14079493284225464,
    "backtrans_bleu": 68.59298497593029,
    "backtrans_chrf": 84.68427080648404,
    "backtrans_bertscore": 0.8709501028060913,
    "prof_backtrans_bleu": 52.833955475006576,
    "prof_backtrans_chrf": 80.28733585427756,
    "prof_backtrans_bertscore": 0.8561863303184509,
    "prof_backtrans_labse": 0.8663105964660645,
    "prof_backtrans_xlm_roberta": 0.7128536701202393,
    "llm_vs_prof_backtrans_bleu": 60.96945463202584,
    "llm_vs_prof_backtrans_chrf": 76.48124617716094,
    "llm_vs_prof_backtrans_bertscore": 0.9162841439247131,
    "llm_vs_prof_backtrans_labse": 0.844064474105835
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 40.620248371175414,
    "same_lang_chrf": 72.51158008112498,
    "same_lang_bertscore": 0.7906140089035034,
    "same_lang_comet": 0.8437223434448242,
    "cross_lang_xlm_roberta": 0.8988773822784424,
    "cross_lang_labse": 0.9063265323638916,
    "cross_lang_mbert": 0.8921743035316467,
    "cross_lang_comet_qe": 0.063642717897892,
    "backtrans_bleu": 65.75711986913234,
    "backtrans_chrf": 85.65915114476591,
    "backtrans_bertscore": 0.8673436045646667,
    "prof_backtrans_bleu": 47.08667964350573,
    "prof_backtrans_chrf": 76.80702428851333,
    "prof_backtrans_bertscore": 0.8602352142333984,
    "prof_backtrans_labse": 0.9004240036010742,
    "prof_backtrans_xlm_roberta": 0.8272785544395447,
    "llm_vs_prof_backtrans_bleu": 55.831741706072506,
    "llm_vs_prof_backtrans_chrf": 74.92638585854453,
    "llm_vs_prof_backtrans_bertscore": 0.9086542129516602,
    "llm_vs_prof_backtrans_labse": 0.9094111323356628
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 30.289082439720406,
    "same_lang_chrf": 61.524832400329885,
    "same_lang_bertscore": 0.7939459681510925,
    "same_lang_comet": 0.8532227277755737,
    "cross_lang_xlm_roberta": 0.8162102699279785,
    "cross_lang_labse": 0.8979890942573547,
    "cross_lang_mbert": 0.8257597088813782,
    "cross_lang_comet_qe": 0.08763781189918518,
    "backtrans_bleu": 67.56233820988393,
    "backtrans_chrf": 85.53665741697341,
    "backtrans_bertscore": 0.8600788712501526,
    "prof_backtrans_bleu": 28.013164155239526,
    "prof_backtrans_chrf": 66.21676142748014,
    "prof_backtrans_bertscore": 0.8638322353363037,
    "prof_backtrans_labse": 0.8824104070663452,
    "prof_backtrans_xlm_roberta": 0.7205842733383179,
    "llm_vs_prof_backtrans_bleu": 31.36747021529214,
    "llm_vs_prof_backtrans_chrf": 59.31491957531002,
    "llm_vs_prof_backtrans_bertscore": 0.844493567943573,
    "llm_vs_prof_backtrans_labse": 0.825242280960083
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 24.866123031665104,
    "same_lang_chrf": 44.41119680323067,
    "same_lang_bertscore": 0.7819989323616028,
    "same_lang_comet": 0.8791589736938477,
    "cross_lang_xlm_roberta": 0.8545781373977661,
    "cross_lang_labse": 0.877770185470581,
    "cross_lang_mbert": 0.8451600670814514,
    "cross_lang_comet_qe": 0.11832991242408752,
    "backtrans_bleu": 68.63253451376654,
    "backtrans_chrf": 86.55986628443115,
    "backtrans_bertscore": 0.865879476070404,
    "prof_backtrans_bleu": 41.69930205909015,
    "prof_backtrans_chrf": 74.85778516375095,
    "prof_backtrans_bertscore": 0.859860897064209,
    "prof_backtrans_labse": 0.9019161462783813,
    "prof_backtrans_xlm_roberta": 0.7836892008781433,
    "llm_vs_prof_backtrans_bleu": 44.20765580902747,
    "llm_vs_prof_backtrans_chrf": 70.07646083604557,
    "llm_vs_prof_backtrans_bertscore": 0.9041844606399536,
    "llm_vs_prof_backtrans_labse": 0.9045701622962952
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 31.03337087712261,
    "same_lang_chrf": 60.640708956645085,
    "same_lang_bertscore": 0.8019455075263977,
    "same_lang_comet": 0.7592012882232666,
    "cross_lang_xlm_roberta": 0.9005112648010254,
    "cross_lang_labse": 0.9176322221755981,
    "cross_lang_mbert": 0.8901988863945007,
    "cross_lang_comet_qe": 0.05309440940618515,
    "backtrans_bleu": 62.889047290729685,
    "backtrans_chrf": 81.8266677977461,
    "backtrans_bertscore": 0.8667332530021667,
    "prof_backtrans_bleu": 30.804515854035397,
    "prof_backtrans_chrf": 67.60619281899865,
    "prof_backtrans_bertscore": 0.8612245917320251,
    "prof_backtrans_labse": 0.8742370009422302,
    "prof_backtrans_xlm_roberta": 0.7572885751724243,
    "llm_vs_prof_backtrans_bleu": 37.38819973113022,
    "llm_vs_prof_backtrans_chrf": 64.48462461385522,
    "llm_vs_prof_backtrans_bertscore": 0.8918569684028625,
    "llm_vs_prof_backtrans_labse": 0.8340559005737305
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 50.82755045861295,
    "same_lang_chrf": 76.58479627520711,
    "same_lang_bertscore": 0.8230531811714172,
    "same_lang_comet": 0.798628568649292,
    "cross_lang_xlm_roberta": 0.883102297782898,
    "cross_lang_labse": 0.9158579111099243,
    "cross_lang_mbert": 0.8730949759483337,
    "cross_lang_comet_qe": 0.07712166011333466,
    "backtrans_bleu": 66.23237826166226,
    "backtrans_chrf": 84.89529813961808,
    "backtrans_bertscore": 0.8653302788734436,
    "prof_backtrans_bleu": 40.024097431487014,
    "prof_backtrans_chrf": 67.17249581390354,
    "prof_backtrans_bertscore": 0.8565254807472229,
    "prof_backtrans_labse": 0.8487459421157837,
    "prof_backtrans_xlm_roberta": 0.8508239388465881,
    "llm_vs_prof_backtrans_bleu": 46.302281476780436,
    "llm_vs_prof_backtrans_chrf": 73.09414379545451,
    "llm_vs_prof_backtrans_bertscore": 0.8626484274864197,
    "llm_vs_prof_backtrans_labse": 0.787203311920166
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 4.570176987935415,
    "same_lang_chrf": 32.65428048451469,
    "same_lang_bertscore": 0.7615581154823303,
    "same_lang_comet": 0.8394445776939392,
    "cross_lang_xlm_roberta": 0.8215794563293457,
    "cross_lang_labse": 0.8984723091125488,
    "cross_lang_mbert": 0.8014126420021057,
    "cross_lang_comet_qe": 0.12300927937030792,
    "backtrans_bleu": 52.452501940008545,
    "backtrans_chrf": 79.30333799108966,
    "backtrans_bertscore": 0.8710490465164185,
    "prof_backtrans_bleu": 33.419690920556,
    "prof_backtrans_chrf": 67.62358520207982,
    "prof_backtrans_bertscore": 0.8388540744781494,
    "prof_backtrans_labse": 0.8119955658912659,
    "prof_backtrans_xlm_roberta": 0.7178862690925598,
    "llm_vs_prof_backtrans_bleu": 42.65196503461957,
    "llm_vs_prof_backtrans_chrf": 70.36545797584375,
    "llm_vs_prof_backtrans_bertscore": 0.8611196875572205,
    "llm_vs_prof_backtrans_labse": 0.688267171382904
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 41.889564668149724,
    "same_lang_chrf": 61.07436331654764,
    "same_lang_bertscore": 0.7992010116577148,
    "same_lang_comet": 0.871049165725708,
    "cross_lang_xlm_roberta": 0.8976550102233887,
    "cross_lang_labse": 0.9226245880126953,
    "cross_lang_mbert": 0.8571248054504395,
    "cross_lang_comet_qe": 0.08975262939929962,
    "backtrans_bleu": 54.917863602391414,
    "backtrans_chrf": 80.09183431297214,
    "backtrans_bertscore": 0.8676813244819641,
    "prof_backtrans_bleu": 52.40207404588162,
    "prof_backtrans_chrf": 79.97248381392399,
    "prof_backtrans_bertscore": 0.8615782856941223,
    "prof_backtrans_labse": 0.8956300020217896,
    "prof_backtrans_xlm_roberta": 0.7328354716300964,
    "llm_vs_prof_backtrans_bleu": 56.308227081550896,
    "llm_vs_prof_backtrans_chrf": 77.74048492457894,
    "llm_vs_prof_backtrans_bertscore": 0.9037752747535706,
    "llm_vs_prof_backtrans_labse": 0.864962637424469
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 36.83560589291263,
    "same_lang_chrf": 68.57162899009354,
    "same_lang_bertscore": 0.7851109504699707,
    "same_lang_comet": 0.8443305492401123,
    "cross_lang_xlm_roberta": 0.8953552842140198,
    "cross_lang_labse": 0.9174976944923401,
    "cross_lang_mbert": 0.8694735169410706,
    "cross_lang_comet_qe": 0.11760792136192322,
    "backtrans_bleu": 61.451936848386005,
    "backtrans_chrf": 81.23986981398468,
    "backtrans_bertscore": 0.8666366934776306,
    "prof_backtrans_bleu": 50.13437651540323,
    "prof_backtrans_chrf": 77.58183673100271,
    "prof_backtrans_bertscore": 0.8673779964447021,
    "prof_backtrans_labse": 0.9271184206008911,
    "prof_backtrans_xlm_roberta": 0.8304744362831116,
    "llm_vs_prof_backtrans_bleu": 59.73852246384234,
    "llm_vs_prof_backtrans_chrf": 76.51418770711693,
    "llm_vs_prof_backtrans_bertscore": 0.9086857438087463,
    "llm_vs_prof_backtrans_labse": 0.9332667589187622
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 26.509131855958067,
    "same_lang_chrf": 57.47285853738462,
    "same_lang_bertscore": 0.7846939563751221,
    "same_lang_comet": 0.8563725352287292,
    "cross_lang_xlm_roberta": 0.9022783637046814,
    "cross_lang_labse": 0.9098010063171387,
    "cross_lang_mbert": 0.8819027543067932,
    "cross_lang_comet_qe": 0.02010352909564972,
    "backtrans_bleu": 50.86381152530574,
    "backtrans_chrf": 77.8350314779861,
    "backtrans_bertscore": 0.8662095069885254,
    "prof_backtrans_bleu": 22.235415486199667,
    "prof_backtrans_chrf": 56.045076655669945,
    "prof_backtrans_bertscore": 0.862572968006134,
    "prof_backtrans_labse": 0.8683159351348877,
    "prof_backtrans_xlm_roberta": 0.7424797415733337,
    "llm_vs_prof_backtrans_bleu": 27.748498828985916,
    "llm_vs_prof_backtrans_chrf": 60.78529150646451,
    "llm_vs_prof_backtrans_bertscore": 0.8486311435699463,
    "llm_vs_prof_backtrans_labse": 0.8354740142822266
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 25.735720985110458,
    "same_lang_chrf": 43.09898077079171,
    "same_lang_bertscore": 0.800412118434906,
    "same_lang_comet": 0.8822849988937378,
    "cross_lang_xlm_roberta": 0.8536058664321899,
    "cross_lang_labse": 0.8953673243522644,
    "cross_lang_mbert": 0.8115696907043457,
    "cross_lang_comet_qe": 0.21335090696811676,
    "backtrans_bleu": 48.77299209070313,
    "backtrans_chrf": 77.67005466234878,
    "backtrans_bertscore": 0.8706615567207336,
    "prof_backtrans_bleu": 43.260488086595124,
    "prof_backtrans_chrf": 76.09523427531535,
    "prof_backtrans_bertscore": 0.8655582666397095,
    "prof_backtrans_labse": 0.8895124197006226,
    "prof_backtrans_xlm_roberta": 0.7848224639892578,
    "llm_vs_prof_backtrans_bleu": 49.220517960822065,
    "llm_vs_prof_backtrans_chrf": 73.23180742744643,
    "llm_vs_prof_backtrans_bertscore": 0.8995445966720581,
    "llm_vs_prof_backtrans_labse": 0.8470508456230164
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 23.74703614481657,
    "same_lang_chrf": 53.880345839179746,
    "same_lang_bertscore": 0.7935469746589661,
    "same_lang_comet": 0.7384502291679382,
    "cross_lang_xlm_roberta": 0.9054600596427917,
    "cross_lang_labse": 0.9104903936386108,
    "cross_lang_mbert": 0.8666310906410217,
    "cross_lang_comet_qe": -0.010904356837272644,
    "backtrans_bleu": 49.59520783282238,
    "backtrans_chrf": 75.72425167713386,
    "backtrans_bertscore": 0.8585698008537292,
    "prof_backtrans_bleu": 33.043750145793865,
    "prof_backtrans_chrf": 68.34850552992515,
    "prof_backtrans_bertscore": 0.8620432615280151,
    "prof_backtrans_labse": 0.8973500728607178,
    "prof_backtrans_xlm_roberta": 0.8120179772377014,
    "llm_vs_prof_backtrans_bleu": 41.9833920787234,
    "llm_vs_prof_backtrans_chrf": 65.52802450283185,
    "llm_vs_prof_backtrans_bertscore": 0.8805176019668579,
    "llm_vs_prof_backtrans_labse": 0.8543087840080261
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 53.64590980536715,
    "same_lang_chrf": 77.88057905425634,
    "same_lang_bertscore": 0.9047036170959473,
    "same_lang_comet": 0.8268840909004211,
    "cross_lang_xlm_roberta": 0.9680021405220032,
    "cross_lang_labse": 0.9933459162712097,
    "cross_lang_mbert": 0.9160745143890381,
    "cross_lang_comet_qe": -0.16977183520793915,
    "backtrans_bleu": 67.43173714657574,
    "backtrans_chrf": 85.049142577004,
    "backtrans_bertscore": 0.9104852676391602,
    "prof_backtrans_bleu": 50.37824524494546,
    "prof_backtrans_chrf": 78.57055126474299,
    "prof_backtrans_bertscore": 0.8884814977645874,
    "prof_backtrans_labse": 0.9604958891868591,
    "prof_backtrans_xlm_roberta": 0.9595763683319092,
    "llm_vs_prof_backtrans_bleu": 58.649749718548044,
    "llm_vs_prof_backtrans_chrf": 81.40080347458002,
    "llm_vs_prof_backtrans_bertscore": 0.909122109413147,
    "llm_vs_prof_backtrans_labse": 0.9601277112960815
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.666108500958153,
    "same_lang_chrf": 42.15401081106454,
    "same_lang_bertscore": 0.7878697514533997,
    "same_lang_comet": 0.8297427892684937,
    "cross_lang_xlm_roberta": 0.9406573176383972,
    "cross_lang_labse": 0.9086522459983826,
    "cross_lang_mbert": 0.8763282895088196,
    "cross_lang_comet_qe": 0.13127727806568146,
    "backtrans_bleu": 71.06340310241977,
    "backtrans_chrf": 86.26448352689026,
    "backtrans_bertscore": 0.866309642791748,
    "prof_backtrans_bleu": 62.706416752400266,
    "prof_backtrans_chrf": 83.88861609314775,
    "prof_backtrans_bertscore": 0.8542720079421997,
    "prof_backtrans_labse": 0.8723856210708618,
    "prof_backtrans_xlm_roberta": 0.7022980451583862,
    "llm_vs_prof_backtrans_bleu": 71.59479166929366,
    "llm_vs_prof_backtrans_chrf": 84.0820047842546,
    "llm_vs_prof_backtrans_bertscore": 0.9043282866477966,
    "llm_vs_prof_backtrans_labse": 0.9003337025642395
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 39.70252116530943,
    "same_lang_chrf": 59.2065339115797,
    "same_lang_bertscore": 0.8392972350120544,
    "same_lang_comet": 0.873784065246582,
    "cross_lang_xlm_roberta": 0.993752658367157,
    "cross_lang_labse": 0.984425961971283,
    "cross_lang_mbert": 0.9652869701385498,
    "cross_lang_comet_qe": -0.22700472176074982,
    "backtrans_bleu": 60.72727124970836,
    "backtrans_chrf": 81.64702302006722,
    "backtrans_bertscore": 0.925251305103302,
    "prof_backtrans_bleu": 58.436132020219226,
    "prof_backtrans_chrf": 81.19398571012583,
    "prof_backtrans_bertscore": 0.898970365524292,
    "prof_backtrans_labse": 0.9435184001922607,
    "prof_backtrans_xlm_roberta": 0.6750977635383606,
    "llm_vs_prof_backtrans_bleu": 59.32009081384654,
    "llm_vs_prof_backtrans_chrf": 80.3395992024328,
    "llm_vs_prof_backtrans_bertscore": 0.8984749913215637,
    "llm_vs_prof_backtrans_labse": 0.9513912200927734
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 33.587854875308125,
    "same_lang_chrf": 60.327131289358746,
    "same_lang_bertscore": 0.7766749262809753,
    "same_lang_comet": 0.8139366507530212,
    "cross_lang_xlm_roberta": 0.9873439073562622,
    "cross_lang_labse": 0.9407364726066589,
    "cross_lang_mbert": 0.9895098209381104,
    "cross_lang_comet_qe": 0.13869693875312805,
    "backtrans_bleu": 65.75033347935691,
    "backtrans_chrf": 83.89157952867731,
    "backtrans_bertscore": 0.8658997416496277,
    "prof_backtrans_bleu": 52.19881528147749,
    "prof_backtrans_chrf": 79.95984210875929,
    "prof_backtrans_bertscore": 0.8754745125770569,
    "prof_backtrans_labse": 0.8894956111907959,
    "prof_backtrans_xlm_roberta": 0.7786083221435547,
    "llm_vs_prof_backtrans_bleu": 54.59178755694177,
    "llm_vs_prof_backtrans_chrf": 74.17080603314282,
    "llm_vs_prof_backtrans_bertscore": 0.8593880534172058,
    "llm_vs_prof_backtrans_labse": 0.8433690071105957
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 29.27444785638465,
    "same_lang_chrf": 64.76126965363417,
    "same_lang_bertscore": 0.7818218469619751,
    "same_lang_comet": 0.826142430305481,
    "cross_lang_xlm_roberta": 0.9193868041038513,
    "cross_lang_labse": 0.8929852247238159,
    "cross_lang_mbert": 0.8340988159179688,
    "cross_lang_comet_qe": 0.1639641523361206,
    "backtrans_bleu": 62.212326641978926,
    "backtrans_chrf": 82.70264237437878,
    "backtrans_bertscore": 0.859483540058136,
    "prof_backtrans_bleu": 35.30961392262088,
    "prof_backtrans_chrf": 72.52964056997516,
    "prof_backtrans_bertscore": 0.8918601274490356,
    "prof_backtrans_labse": 0.9332689642906189,
    "prof_backtrans_xlm_roberta": 0.721053957939148,
    "llm_vs_prof_backtrans_bleu": 41.90366622688357,
    "llm_vs_prof_backtrans_chrf": 72.01723154800312,
    "llm_vs_prof_backtrans_bertscore": 0.836689829826355,
    "llm_vs_prof_backtrans_labse": 0.850983202457428
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 9.429275660074026,
    "same_lang_chrf": 36.32054201212581,
    "same_lang_bertscore": 0.722585141658783,
    "same_lang_comet": 0.7936012744903564,
    "cross_lang_xlm_roberta": 0.9336873888969421,
    "cross_lang_labse": 0.9068390130996704,
    "cross_lang_mbert": 0.9147934317588806,
    "cross_lang_comet_qe": 0.15144343674182892,
    "backtrans_bleu": 60.61686540528128,
    "backtrans_chrf": 82.14029543208567,
    "backtrans_bertscore": 0.8576945066452026,
    "prof_backtrans_bleu": 47.043502460072325,
    "prof_backtrans_chrf": 74.86945014190091,
    "prof_backtrans_bertscore": 0.8618319630622864,
    "prof_backtrans_labse": 0.9008890986442566,
    "prof_backtrans_xlm_roberta": 0.6233586668968201,
    "llm_vs_prof_backtrans_bleu": 50.309683332383955,
    "llm_vs_prof_backtrans_chrf": 73.48474889244096,
    "llm_vs_prof_backtrans_bertscore": 0.8719890117645264,
    "llm_vs_prof_backtrans_labse": 0.8649116158485413
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 41.31592141942367,
    "same_lang_chrf": 67.57568255630805,
    "same_lang_bertscore": 0.8610560894012451,
    "same_lang_comet": 0.7622138261795044,
    "cross_lang_xlm_roberta": 0.9837337732315063,
    "cross_lang_labse": 0.9373959898948669,
    "cross_lang_mbert": 0.9092597961425781,
    "cross_lang_comet_qe": 0.1175302192568779,
    "backtrans_bleu": 57.53680348029519,
    "backtrans_chrf": 80.46337566353479,
    "backtrans_bertscore": 0.8651665449142456,
    "prof_backtrans_bleu": 52.616144647765495,
    "prof_backtrans_chrf": 79.00007138993224,
    "prof_backtrans_bertscore": 0.897840678691864,
    "prof_backtrans_labse": 0.9373388290405273,
    "prof_backtrans_xlm_roberta": 0.6732903718948364,
    "llm_vs_prof_backtrans_bleu": 55.80151383550072,
    "llm_vs_prof_backtrans_chrf": 76.44866908892325,
    "llm_vs_prof_backtrans_bertscore": 0.8606440424919128,
    "llm_vs_prof_backtrans_labse": 0.9009819626808167
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 51.962914988657104,
    "same_lang_chrf": 78.01592815818626,
    "same_lang_bertscore": 0.9087029099464417,
    "same_lang_comet": 0.8392084240913391,
    "cross_lang_xlm_roberta": 0.9807149767875671,
    "cross_lang_labse": 0.9740016460418701,
    "cross_lang_mbert": 0.9482800960540771,
    "cross_lang_comet_qe": 0.20107698440551758,
    "backtrans_bleu": 63.58360990813497,
    "backtrans_chrf": 83.4551554543984,
    "backtrans_bertscore": 0.8914930820465088,
    "prof_backtrans_bleu": 56.350006079333966,
    "prof_backtrans_chrf": 80.4598193286688,
    "prof_backtrans_bertscore": 0.8863595724105835,
    "prof_backtrans_labse": 0.9484900236129761,
    "prof_backtrans_xlm_roberta": 0.9145277142524719,
    "llm_vs_prof_backtrans_bleu": 64.3666170677901,
    "llm_vs_prof_backtrans_chrf": 83.7801394286524,
    "llm_vs_prof_backtrans_bertscore": 0.9494471549987793,
    "llm_vs_prof_backtrans_labse": 0.9606589078903198
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.443237854171045,
    "same_lang_chrf": 42.843375525988755,
    "same_lang_bertscore": 0.8502190113067627,
    "same_lang_comet": 0.8580366969108582,
    "cross_lang_xlm_roberta": 0.9980912804603577,
    "cross_lang_labse": 0.9885267019271851,
    "cross_lang_mbert": 0.980705976486206,
    "cross_lang_comet_qe": -0.06007362902164459,
    "backtrans_bleu": 69.76693726536548,
    "backtrans_chrf": 86.39350218042256,
    "backtrans_bertscore": 0.9237720370292664,
    "prof_backtrans_bleu": 56.85261530899585,
    "prof_backtrans_chrf": 81.72116607135621,
    "prof_backtrans_bertscore": 0.8829954266548157,
    "prof_backtrans_labse": 0.9188125133514404,
    "prof_backtrans_xlm_roberta": 0.6944184303283691,
    "llm_vs_prof_backtrans_bleu": 60.981341556376776,
    "llm_vs_prof_backtrans_chrf": 81.06308126257849,
    "llm_vs_prof_backtrans_bertscore": 0.892067015171051,
    "llm_vs_prof_backtrans_labse": 0.9266791939735413
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 43.96833612291557,
    "same_lang_chrf": 63.991946006800205,
    "same_lang_bertscore": 0.8558799028396606,
    "same_lang_comet": 0.8770189881324768,
    "cross_lang_xlm_roberta": 0.9936456084251404,
    "cross_lang_labse": 0.994341254234314,
    "cross_lang_mbert": 0.9221040606498718,
    "cross_lang_comet_qe": -0.1343175321817398,
    "backtrans_bleu": 68.25760376450476,
    "backtrans_chrf": 85.21712370898513,
    "backtrans_bertscore": 0.9359877705574036,
    "prof_backtrans_bleu": 56.665106439492035,
    "prof_backtrans_chrf": 80.65006937768783,
    "prof_backtrans_bertscore": 0.9006466269493103,
    "prof_backtrans_labse": 0.9426825046539307,
    "prof_backtrans_xlm_roberta": 0.7188182473182678,
    "llm_vs_prof_backtrans_bleu": 61.865159647982836,
    "llm_vs_prof_backtrans_chrf": 82.74186479951544,
    "llm_vs_prof_backtrans_bertscore": 0.9108989238739014,
    "llm_vs_prof_backtrans_labse": 0.9436033368110657
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 39.53392723753932,
    "same_lang_chrf": 67.27219919609782,
    "same_lang_bertscore": 0.8263171315193176,
    "same_lang_comet": 0.8415111303329468,
    "cross_lang_xlm_roberta": 0.9984718561172485,
    "cross_lang_labse": 0.9938390254974365,
    "cross_lang_mbert": 0.9972931742668152,
    "cross_lang_comet_qe": -0.213862806558609,
    "backtrans_bleu": 68.48671483503996,
    "backtrans_chrf": 85.66762530087897,
    "backtrans_bertscore": 0.9344966411590576,
    "prof_backtrans_bleu": 47.92863209078938,
    "prof_backtrans_chrf": 78.2362463551624,
    "prof_backtrans_bertscore": 0.8747797608375549,
    "prof_backtrans_labse": 0.8887154459953308,
    "prof_backtrans_xlm_roberta": 0.7875234484672546,
    "llm_vs_prof_backtrans_bleu": 50.69811482914754,
    "llm_vs_prof_backtrans_chrf": 72.23698016473236,
    "llm_vs_prof_backtrans_bertscore": 0.8694097399711609,
    "llm_vs_prof_backtrans_labse": 0.8880530595779419
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 28.00296497628773,
    "same_lang_chrf": 64.90854854750813,
    "same_lang_bertscore": 0.8446374535560608,
    "same_lang_comet": 0.8546396493911743,
    "cross_lang_xlm_roberta": 0.9965110421180725,
    "cross_lang_labse": 0.9938409328460693,
    "cross_lang_mbert": 0.9353506565093994,
    "cross_lang_comet_qe": -0.12287691235542297,
    "backtrans_bleu": 66.77610273618684,
    "backtrans_chrf": 85.33226123623857,
    "backtrans_bertscore": 0.9412166476249695,
    "prof_backtrans_bleu": 41.55965350629387,
    "prof_backtrans_chrf": 74.00161542513766,
    "prof_backtrans_bertscore": 0.8929007649421692,
    "prof_backtrans_labse": 0.9337607622146606,
    "prof_backtrans_xlm_roberta": 0.7131775617599487,
    "llm_vs_prof_backtrans_bleu": 43.7785581888642,
    "llm_vs_prof_backtrans_chrf": 74.08260826075563,
    "llm_vs_prof_backtrans_bertscore": 0.8926529288291931,
    "llm_vs_prof_backtrans_labse": 0.9301747679710388
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 8.655766929596098,
    "same_lang_chrf": 34.77157259499807,
    "same_lang_bertscore": 0.756019651889801,
    "same_lang_comet": 0.7958639860153198,
    "cross_lang_xlm_roberta": 0.9968339204788208,
    "cross_lang_labse": 0.9912099838256836,
    "cross_lang_mbert": 0.9803659915924072,
    "cross_lang_comet_qe": -0.13624373078346252,
    "backtrans_bleu": 69.67117670749582,
    "backtrans_chrf": 86.12711088633466,
    "backtrans_bertscore": 0.9203139543533325,
    "prof_backtrans_bleu": 43.979800619841825,
    "prof_backtrans_chrf": 75.73695923802686,
    "prof_backtrans_bertscore": 0.8616493940353394,
    "prof_backtrans_labse": 0.9183934330940247,
    "prof_backtrans_xlm_roberta": 0.6309583187103271,
    "llm_vs_prof_backtrans_bleu": 47.498382242556275,
    "llm_vs_prof_backtrans_chrf": 70.71356510545279,
    "llm_vs_prof_backtrans_bertscore": 0.8688106536865234,
    "llm_vs_prof_backtrans_labse": 0.9238236546516418
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 37.05077648392189,
    "same_lang_chrf": 64.38737404718916,
    "same_lang_bertscore": 0.8503273129463196,
    "same_lang_comet": 0.7578732967376709,
    "cross_lang_xlm_roberta": 0.9969761371612549,
    "cross_lang_labse": 0.9946030974388123,
    "cross_lang_mbert": 0.941217839717865,
    "cross_lang_comet_qe": -0.19065262377262115,
    "backtrans_bleu": 71.00518045047433,
    "backtrans_chrf": 86.17198811244941,
    "backtrans_bertscore": 0.9551671743392944,
    "prof_backtrans_bleu": 51.730939943614196,
    "prof_backtrans_chrf": 78.06082261383342,
    "prof_backtrans_bertscore": 0.8949906229972839,
    "prof_backtrans_labse": 0.9403359293937683,
    "prof_backtrans_xlm_roberta": 0.6668949723243713,
    "llm_vs_prof_backtrans_bleu": 55.66746362349702,
    "llm_vs_prof_backtrans_chrf": 77.92633770542726,
    "llm_vs_prof_backtrans_bertscore": 0.8994801640510559,
    "llm_vs_prof_backtrans_labse": 0.9435626864433289
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 47.25371362539366,
    "same_lang_chrf": 76.60793547130498,
    "same_lang_bertscore": 0.7993576526641846,
    "same_lang_comet": 0.7631529569625854,
    "cross_lang_xlm_roberta": 0.8943352103233337,
    "cross_lang_labse": 0.8369896411895752,
    "cross_lang_mbert": 0.8535345196723938,
    "cross_lang_comet_qe": 0.28080371022224426,
    "backtrans_bleu": 58.8149103096154,
    "backtrans_chrf": 83.9122398421054,
    "backtrans_bertscore": 0.8542391061782837,
    "prof_backtrans_bleu": 52.5334271395236,
    "prof_backtrans_chrf": 78.71876108804203,
    "prof_backtrans_bertscore": 0.8600688576698303,
    "prof_backtrans_labse": 0.8887189030647278,
    "prof_backtrans_xlm_roberta": 0.9124841094017029,
    "llm_vs_prof_backtrans_bleu": 56.9199817604303,
    "llm_vs_prof_backtrans_chrf": 80.13819401267764,
    "llm_vs_prof_backtrans_bertscore": 0.945832371711731,
    "llm_vs_prof_backtrans_labse": 0.9046719670295715
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 15.45279238582381,
    "same_lang_chrf": 43.23290230756261,
    "same_lang_bertscore": 0.7819870114326477,
    "same_lang_comet": 0.8275099992752075,
    "cross_lang_xlm_roberta": 0.9075275659561157,
    "cross_lang_labse": 0.8210341930389404,
    "cross_lang_mbert": 0.8567297458648682,
    "cross_lang_comet_qe": 0.2477443367242813,
    "backtrans_bleu": 61.8353515972487,
    "backtrans_chrf": 85.7094878870714,
    "backtrans_bertscore": 0.8529725074768066,
    "prof_backtrans_bleu": 57.74371913278319,
    "prof_backtrans_chrf": 82.5561369289011,
    "prof_backtrans_bertscore": 0.8849329352378845,
    "prof_backtrans_labse": 0.9220644235610962,
    "prof_backtrans_xlm_roberta": 0.6969006657600403,
    "llm_vs_prof_backtrans_bleu": 59.90882272731088,
    "llm_vs_prof_backtrans_chrf": 78.1157461579699,
    "llm_vs_prof_backtrans_bertscore": 0.86357182264328,
    "llm_vs_prof_backtrans_labse": 0.7834582328796387
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 48.808404544953454,
    "same_lang_chrf": 67.69682831367803,
    "same_lang_bertscore": 0.7928656339645386,
    "same_lang_comet": 0.8435776233673096,
    "cross_lang_xlm_roberta": 0.9301738739013672,
    "cross_lang_labse": 0.9077686667442322,
    "cross_lang_mbert": 0.8530358672142029,
    "cross_lang_comet_qe": 0.24253176152706146,
    "backtrans_bleu": 73.66312379536748,
    "backtrans_chrf": 87.98617728725556,
    "backtrans_bertscore": 0.8711760640144348,
    "prof_backtrans_bleu": 65.7351462728567,
    "prof_backtrans_chrf": 85.30854349356449,
    "prof_backtrans_bertscore": 0.8599764108657837,
    "prof_backtrans_labse": 0.8500646948814392,
    "prof_backtrans_xlm_roberta": 0.6900710463523865,
    "llm_vs_prof_backtrans_bleu": 80.18497632309735,
    "llm_vs_prof_backtrans_chrf": 89.30365165881844,
    "llm_vs_prof_backtrans_bertscore": 0.9585590362548828,
    "llm_vs_prof_backtrans_labse": 0.9405954480171204
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 37.95208289513938,
    "same_lang_chrf": 64.47070744324999,
    "same_lang_bertscore": 0.7952275276184082,
    "same_lang_comet": 0.8364724516868591,
    "cross_lang_xlm_roberta": 0.9580910205841064,
    "cross_lang_labse": 0.8543575406074524,
    "cross_lang_mbert": 0.9529763460159302,
    "cross_lang_comet_qe": 0.15291692316532135,
    "backtrans_bleu": 64.05963254657996,
    "backtrans_chrf": 86.31807142368011,
    "backtrans_bertscore": 0.8749812245368958,
    "prof_backtrans_bleu": 59.20827283708996,
    "prof_backtrans_chrf": 83.08478361126365,
    "prof_backtrans_bertscore": 0.8580474257469177,
    "prof_backtrans_labse": 0.8787938356399536,
    "prof_backtrans_xlm_roberta": 0.8418365120887756,
    "llm_vs_prof_backtrans_bleu": 61.82780756076343,
    "llm_vs_prof_backtrans_chrf": 76.3679793818545,
    "llm_vs_prof_backtrans_bertscore": 0.9307934641838074,
    "llm_vs_prof_backtrans_labse": 0.883129894733429
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 29.264810291470987,
    "same_lang_chrf": 65.45271751840106,
    "same_lang_bertscore": 0.7907151579856873,
    "same_lang_comet": 0.830750584602356,
    "cross_lang_xlm_roberta": 0.8921497464179993,
    "cross_lang_labse": 0.8845536708831787,
    "cross_lang_mbert": 0.8388753533363342,
    "cross_lang_comet_qe": 0.255693256855011,
    "backtrans_bleu": 68.23733801239146,
    "backtrans_chrf": 86.19867622760158,
    "backtrans_bertscore": 0.859269917011261,
    "prof_backtrans_bleu": 4.539994760848994,
    "prof_backtrans_chrf": 25.847965741524497,
    "prof_backtrans_bertscore": 0.8647036552429199,
    "prof_backtrans_labse": 0.898261547088623,
    "prof_backtrans_xlm_roberta": 0.6848631501197815,
    "llm_vs_prof_backtrans_bleu": 14.452502071262451,
    "llm_vs_prof_backtrans_chrf": 51.403836002005384,
    "llm_vs_prof_backtrans_bertscore": 0.8693248629570007,
    "llm_vs_prof_backtrans_labse": 0.9109548330307007
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 7.462609207082691,
    "same_lang_chrf": 33.05528928259992,
    "same_lang_bertscore": 0.7119940519332886,
    "same_lang_comet": 0.8001673221588135,
    "cross_lang_xlm_roberta": 0.9045461416244507,
    "cross_lang_labse": 0.8497408628463745,
    "cross_lang_mbert": 0.8570797443389893,
    "cross_lang_comet_qe": 0.24205204844474792,
    "backtrans_bleu": 63.839929994186,
    "backtrans_chrf": 85.40552669552527,
    "backtrans_bertscore": 0.8550328016281128,
    "prof_backtrans_bleu": 5.977204742133808,
    "prof_backtrans_chrf": 33.54638656774944,
    "prof_backtrans_bertscore": 0.833325207233429,
    "prof_backtrans_labse": 0.8497178554534912,
    "prof_backtrans_xlm_roberta": 0.6371814012527466,
    "llm_vs_prof_backtrans_bleu": 6.460905957864543,
    "llm_vs_prof_backtrans_chrf": 48.64730593029488,
    "llm_vs_prof_backtrans_bertscore": 0.8197497129440308,
    "llm_vs_prof_backtrans_labse": 0.771359384059906
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 40.26490463533347,
    "same_lang_chrf": 65.70306819697673,
    "same_lang_bertscore": 0.800553023815155,
    "same_lang_comet": 0.8065574169158936,
    "cross_lang_xlm_roberta": 0.8651118278503418,
    "cross_lang_labse": 0.9017486572265625,
    "cross_lang_mbert": 0.8731016516685486,
    "cross_lang_comet_qe": 0.21621067821979523,
    "backtrans_bleu": 61.9168034252843,
    "backtrans_chrf": 80.58395186199083,
    "backtrans_bertscore": 0.8711122870445251,
    "prof_backtrans_bleu": 50.02089055857172,
    "prof_backtrans_chrf": 77.78099731527367,
    "prof_backtrans_bertscore": 0.898384690284729,
    "prof_backtrans_labse": 0.9408296942710876,
    "prof_backtrans_xlm_roberta": 0.6773853302001953,
    "llm_vs_prof_backtrans_bleu": 54.91222063557704,
    "llm_vs_prof_backtrans_chrf": 76.20302795521813,
    "llm_vs_prof_backtrans_bertscore": 0.8689036965370178,
    "llm_vs_prof_backtrans_labse": 0.8724390268325806
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 53.63547416301631,
    "same_lang_chrf": 77.71210233484493,
    "same_lang_bertscore": 0.8037183880805969,
    "same_lang_comet": 0.7701795101165771,
    "cross_lang_xlm_roberta": 0.8982701301574707,
    "cross_lang_labse": 0.8959628343582153,
    "cross_lang_mbert": 0.8065537214279175,
    "cross_lang_comet_qe": 0.2567250728607178,
    "backtrans_bleu": 62.537966454594944,
    "backtrans_chrf": 82.20550313498188,
    "backtrans_bertscore": 0.8587857484817505,
    "prof_backtrans_bleu": 47.906619953760355,
    "prof_backtrans_chrf": 75.19478680519252,
    "prof_backtrans_bertscore": 0.859442949295044,
    "prof_backtrans_labse": 0.896572470664978,
    "prof_backtrans_xlm_roberta": 0.9336886405944824,
    "llm_vs_prof_backtrans_bleu": 59.16650013149521,
    "llm_vs_prof_backtrans_chrf": 77.30896283461973,
    "llm_vs_prof_backtrans_bertscore": 0.8931125998497009,
    "llm_vs_prof_backtrans_labse": 0.9443954229354858
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 10.770497665137007,
    "same_lang_chrf": 42.042469099830086,
    "same_lang_bertscore": 0.7864000797271729,
    "same_lang_comet": 0.8274719715118408,
    "cross_lang_xlm_roberta": 0.9246577620506287,
    "cross_lang_labse": 0.9069193005561829,
    "cross_lang_mbert": 0.8467649221420288,
    "cross_lang_comet_qe": 0.2269183248281479,
    "backtrans_bleu": 47.36736127320914,
    "backtrans_chrf": 75.85005211806822,
    "backtrans_bertscore": 0.8618382215499878,
    "prof_backtrans_bleu": 31.959987489044305,
    "prof_backtrans_chrf": 68.06248698824919,
    "prof_backtrans_bertscore": 0.8798856735229492,
    "prof_backtrans_labse": 0.9279409646987915,
    "prof_backtrans_xlm_roberta": 0.6986306309700012,
    "llm_vs_prof_backtrans_bleu": 42.89401529193341,
    "llm_vs_prof_backtrans_chrf": 70.85788441123985,
    "llm_vs_prof_backtrans_bertscore": 0.863305389881134,
    "llm_vs_prof_backtrans_labse": 0.8811461329460144
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 44.54044302315167,
    "same_lang_chrf": 63.73521202137902,
    "same_lang_bertscore": 0.7933858633041382,
    "same_lang_comet": 0.8459478616714478,
    "cross_lang_xlm_roberta": 0.9268856644630432,
    "cross_lang_labse": 0.9065637588500977,
    "cross_lang_mbert": 0.7818676233291626,
    "cross_lang_comet_qe": 0.1892048418521881,
    "backtrans_bleu": 56.52077820819468,
    "backtrans_chrf": 78.79978985217258,
    "backtrans_bertscore": 0.8694148063659668,
    "prof_backtrans_bleu": 46.993168802185075,
    "prof_backtrans_chrf": 74.61609030987924,
    "prof_backtrans_bertscore": 0.8643674254417419,
    "prof_backtrans_labse": 0.8655959367752075,
    "prof_backtrans_xlm_roberta": 0.6916912794113159,
    "llm_vs_prof_backtrans_bleu": 59.104720182268316,
    "llm_vs_prof_backtrans_chrf": 78.01925428746857,
    "llm_vs_prof_backtrans_bertscore": 0.9011968970298767,
    "llm_vs_prof_backtrans_labse": 0.9466180205345154
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 38.03780396718045,
    "same_lang_chrf": 65.62361708141529,
    "same_lang_bertscore": 0.7942271828651428,
    "same_lang_comet": 0.8356761932373047,
    "cross_lang_xlm_roberta": 0.9302048087120056,
    "cross_lang_labse": 0.9084519743919373,
    "cross_lang_mbert": 0.9272091388702393,
    "cross_lang_comet_qe": 0.1292351931333542,
    "backtrans_bleu": 60.50963258479813,
    "backtrans_chrf": 80.7445286460179,
    "backtrans_bertscore": 0.8702671527862549,
    "prof_backtrans_bleu": 42.3704010327941,
    "prof_backtrans_chrf": 74.0139543204432,
    "prof_backtrans_bertscore": 0.8747501969337463,
    "prof_backtrans_labse": 0.8864990472793579,
    "prof_backtrans_xlm_roberta": 0.7763006091117859,
    "llm_vs_prof_backtrans_bleu": 48.88367002862818,
    "llm_vs_prof_backtrans_chrf": 71.33022609582548,
    "llm_vs_prof_backtrans_bertscore": 0.8729448914527893,
    "llm_vs_prof_backtrans_labse": 0.8505309820175171
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 27.174980916441648,
    "same_lang_chrf": 61.90222244523624,
    "same_lang_bertscore": 0.7768269777297974,
    "same_lang_comet": 0.8315409421920776,
    "cross_lang_xlm_roberta": 0.7511417865753174,
    "cross_lang_labse": 0.8803396224975586,
    "cross_lang_mbert": 0.7955172061920166,
    "cross_lang_comet_qe": 0.2640780508518219,
    "backtrans_bleu": 53.585632467851404,
    "backtrans_chrf": 78.31771118861025,
    "backtrans_bertscore": 0.8607100248336792,
    "prof_backtrans_bleu": 28.639413116465473,
    "prof_backtrans_chrf": 65.37315725179944,
    "prof_backtrans_bertscore": 0.8569110035896301,
    "prof_backtrans_labse": 0.8959456086158752,
    "prof_backtrans_xlm_roberta": 0.7071510553359985,
    "llm_vs_prof_backtrans_bleu": 36.018357211635085,
    "llm_vs_prof_backtrans_chrf": 63.34391682532689,
    "llm_vs_prof_backtrans_bertscore": 0.8694481253623962,
    "llm_vs_prof_backtrans_labse": 0.9328036308288574
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 8.04173464816271,
    "same_lang_chrf": 34.1807835858167,
    "same_lang_bertscore": 0.7239373326301575,
    "same_lang_comet": 0.7925978899002075,
    "cross_lang_xlm_roberta": 0.920359194278717,
    "cross_lang_labse": 0.897254228591919,
    "cross_lang_mbert": 0.8476641774177551,
    "cross_lang_comet_qe": 0.21370790898799896,
    "backtrans_bleu": 46.95313583636828,
    "backtrans_chrf": 75.67659511171956,
    "backtrans_bertscore": 0.862837016582489,
    "prof_backtrans_bleu": 12.72344269493566,
    "prof_backtrans_chrf": 42.62103597060181,
    "prof_backtrans_bertscore": 0.8225268125534058,
    "prof_backtrans_labse": 0.8115969896316528,
    "prof_backtrans_xlm_roberta": 0.6090054512023926,
    "llm_vs_prof_backtrans_bleu": 19.005373097936346,
    "llm_vs_prof_backtrans_chrf": 59.192537943754296,
    "llm_vs_prof_backtrans_bertscore": 0.8257600665092468,
    "llm_vs_prof_backtrans_labse": 0.8097253441810608
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 33.409755743499076,
    "same_lang_chrf": 60.612488478729844,
    "same_lang_bertscore": 0.7943634986877441,
    "same_lang_comet": 0.7624394297599792,
    "cross_lang_xlm_roberta": 0.9360359907150269,
    "cross_lang_labse": 0.8997209072113037,
    "cross_lang_mbert": 0.8256990909576416,
    "cross_lang_comet_qe": -0.03190390020608902,
    "backtrans_bleu": 38.080800122837665,
    "backtrans_chrf": 67.69640432858756,
    "backtrans_bertscore": 0.8538167476654053,
    "prof_backtrans_bleu": 42.64026890470254,
    "prof_backtrans_chrf": 73.4496227630004,
    "prof_backtrans_bertscore": 0.879690408706665,
    "prof_backtrans_labse": 0.9388316869735718,
    "prof_backtrans_xlm_roberta": 0.6687764525413513,
    "llm_vs_prof_backtrans_bleu": 37.96170156878475,
    "llm_vs_prof_backtrans_chrf": 66.36802489934423,
    "llm_vs_prof_backtrans_bertscore": 0.8862890005111694,
    "llm_vs_prof_backtrans_labse": 0.8687961101531982
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 48.504077148820855,
    "same_lang_chrf": 75.570517988935,
    "same_lang_bertscore": 0.8401101231575012,
    "same_lang_comet": 0.8266689777374268,
    "cross_lang_xlm_roberta": 0.9769091010093689,
    "cross_lang_labse": 0.9831110239028931,
    "cross_lang_mbert": 0.9155274033546448,
    "cross_lang_comet_qe": -0.23810186982154846,
    "backtrans_bleu": 53.50887492051202,
    "backtrans_chrf": 80.36921640015433,
    "backtrans_bertscore": 0.8785608410835266,
    "prof_backtrans_bleu": 57.38222833054221,
    "prof_backtrans_chrf": 81.46213141629026,
    "prof_backtrans_bertscore": 0.8590983748435974,
    "prof_backtrans_labse": 0.8939685225486755,
    "prof_backtrans_xlm_roberta": 0.7865996360778809,
    "llm_vs_prof_backtrans_bleu": 61.96419941007823,
    "llm_vs_prof_backtrans_chrf": 81.11779841119854,
    "llm_vs_prof_backtrans_bertscore": 0.8902245163917542,
    "llm_vs_prof_backtrans_labse": 0.8762181401252747
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.626600739703152,
    "same_lang_chrf": 37.505181521927675,
    "same_lang_bertscore": 0.7879485487937927,
    "same_lang_comet": 0.866536021232605,
    "cross_lang_xlm_roberta": 0.7844581007957458,
    "cross_lang_labse": 0.898856520652771,
    "cross_lang_mbert": 0.8049116730690002,
    "cross_lang_comet_qe": 0.16201908886432648,
    "backtrans_bleu": 66.31647783294962,
    "backtrans_chrf": 85.30857138120145,
    "backtrans_bertscore": 0.8804855942726135,
    "prof_backtrans_bleu": 42.38465469135302,
    "prof_backtrans_chrf": 74.0997724831,
    "prof_backtrans_bertscore": 0.8562413454055786,
    "prof_backtrans_labse": 0.87626051902771,
    "prof_backtrans_xlm_roberta": 0.651984691619873,
    "llm_vs_prof_backtrans_bleu": 53.06466234629516,
    "llm_vs_prof_backtrans_chrf": 74.52093704237339,
    "llm_vs_prof_backtrans_bertscore": 0.9360419511795044,
    "llm_vs_prof_backtrans_labse": 0.9257277846336365
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 41.930991558550915,
    "same_lang_chrf": 58.60802498909107,
    "same_lang_bertscore": 0.7866294384002686,
    "same_lang_comet": 0.8583849668502808,
    "cross_lang_xlm_roberta": 0.7805590033531189,
    "cross_lang_labse": 0.8970217704772949,
    "cross_lang_mbert": 0.8335544466972351,
    "cross_lang_comet_qe": 0.09987504035234451,
    "backtrans_bleu": 62.97536930302086,
    "backtrans_chrf": 83.52233189920285,
    "backtrans_bertscore": 0.8809992074966431,
    "prof_backtrans_bleu": 55.056615012314445,
    "prof_backtrans_chrf": 79.27395907422375,
    "prof_backtrans_bertscore": 0.8718580603599548,
    "prof_backtrans_labse": 0.8797478675842285,
    "prof_backtrans_xlm_roberta": 0.681367814540863,
    "llm_vs_prof_backtrans_bleu": 73.09174306391621,
    "llm_vs_prof_backtrans_chrf": 84.85323402722032,
    "llm_vs_prof_backtrans_bertscore": 0.9215811491012573,
    "llm_vs_prof_backtrans_labse": 0.9308943152427673
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 38.38068673173136,
    "same_lang_chrf": 67.45586051932764,
    "same_lang_bertscore": 0.7807835340499878,
    "same_lang_comet": 0.8370248079299927,
    "cross_lang_xlm_roberta": 0.7775295376777649,
    "cross_lang_labse": 0.8946874141693115,
    "cross_lang_mbert": 0.8162428736686707,
    "cross_lang_comet_qe": 0.1643110066652298,
    "backtrans_bleu": 65.57200853748006,
    "backtrans_chrf": 84.49127416286689,
    "backtrans_bertscore": 0.8914812207221985,
    "prof_backtrans_bleu": 63.5551269477742,
    "prof_backtrans_chrf": 84.1646070894204,
    "prof_backtrans_bertscore": 0.9118386507034302,
    "prof_backtrans_labse": 0.9628725051879883,
    "prof_backtrans_xlm_roberta": 0.8961442112922668,
    "llm_vs_prof_backtrans_bleu": 62.77421927458162,
    "llm_vs_prof_backtrans_chrf": 81.70514512567753,
    "llm_vs_prof_backtrans_bertscore": 0.895073652267456,
    "llm_vs_prof_backtrans_labse": 0.8760583400726318
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 31.31916318588678,
    "same_lang_chrf": 63.87578010533906,
    "same_lang_bertscore": 0.7994747161865234,
    "same_lang_comet": 0.8686203956604004,
    "cross_lang_xlm_roberta": 0.7839156985282898,
    "cross_lang_labse": 0.8904147148132324,
    "cross_lang_mbert": 0.8130314350128174,
    "cross_lang_comet_qe": 0.10318638384342194,
    "backtrans_bleu": 49.63908090382982,
    "backtrans_chrf": 78.12782421783604,
    "backtrans_bertscore": 0.8771875500679016,
    "prof_backtrans_bleu": 46.16321852615335,
    "prof_backtrans_chrf": 74.53301334577425,
    "prof_backtrans_bertscore": 0.8673501014709473,
    "prof_backtrans_labse": 0.8562330007553101,
    "prof_backtrans_xlm_roberta": 0.6955921649932861,
    "llm_vs_prof_backtrans_bleu": 56.669860391674675,
    "llm_vs_prof_backtrans_chrf": 78.96994623924812,
    "llm_vs_prof_backtrans_bertscore": 0.9262924194335938,
    "llm_vs_prof_backtrans_labse": 0.9572558403015137
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 29.700214591860664,
    "same_lang_chrf": 50.47887824367074,
    "same_lang_bertscore": 0.80670166015625,
    "same_lang_comet": 0.8766863942146301,
    "cross_lang_xlm_roberta": 0.7940953373908997,
    "cross_lang_labse": 0.8992015719413757,
    "cross_lang_mbert": 0.820553719997406,
    "cross_lang_comet_qe": 0.1209549605846405,
    "backtrans_bleu": 48.60858257217234,
    "backtrans_chrf": 77.19015586812905,
    "backtrans_bertscore": 0.8759874701499939,
    "prof_backtrans_bleu": 52.51635817631035,
    "prof_backtrans_chrf": 77.95700765132754,
    "prof_backtrans_bertscore": 0.9031395316123962,
    "prof_backtrans_labse": 0.9382959008216858,
    "prof_backtrans_xlm_roberta": 0.8236789703369141,
    "llm_vs_prof_backtrans_bleu": 53.35964099598767,
    "llm_vs_prof_backtrans_chrf": 77.01659249112217,
    "llm_vs_prof_backtrans_bertscore": 0.8746475577354431,
    "llm_vs_prof_backtrans_labse": 0.8813778758049011
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 30.771151379170394,
    "same_lang_chrf": 58.958541847600884,
    "same_lang_bertscore": 0.7975393533706665,
    "same_lang_comet": 0.7871284484863281,
    "cross_lang_xlm_roberta": 0.7997909188270569,
    "cross_lang_labse": 0.903456449508667,
    "cross_lang_mbert": 0.7968659400939941,
    "cross_lang_comet_qe": 0.09072762727737427,
    "backtrans_bleu": 53.82540311702253,
    "backtrans_chrf": 79.5849421086573,
    "backtrans_bertscore": 0.8640413284301758,
    "prof_backtrans_bleu": 45.41398002908028,
    "prof_backtrans_chrf": 75.13024059240423,
    "prof_backtrans_bertscore": 0.8886325359344482,
    "prof_backtrans_labse": 0.9314506649971008,
    "prof_backtrans_xlm_roberta": 0.8407979011535645,
    "llm_vs_prof_backtrans_bleu": 54.20652114913825,
    "llm_vs_prof_backtrans_chrf": 73.34395567099975,
    "llm_vs_prof_backtrans_bertscore": 0.883471667766571,
    "llm_vs_prof_backtrans_labse": 0.8762245178222656
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 51.356702334505584,
    "same_lang_chrf": 75.7814876247667,
    "same_lang_bertscore": 0.8327530026435852,
    "same_lang_comet": 0.8094872236251831,
    "cross_lang_xlm_roberta": 0.9597058892250061,
    "cross_lang_labse": 0.9628922939300537,
    "cross_lang_mbert": 0.959661066532135,
    "cross_lang_comet_qe": 0.148189514875412,
    "backtrans_bleu": 73.8802988281292,
    "backtrans_chrf": 88.07924505090674,
    "backtrans_bertscore": 0.8918524384498596,
    "prof_backtrans_bleu": 50.5595150584504,
    "prof_backtrans_chrf": 78.02023655893342,
    "prof_backtrans_bertscore": 0.8781416416168213,
    "prof_backtrans_labse": 0.8901453614234924,
    "prof_backtrans_xlm_roberta": 0.7767046093940735,
    "llm_vs_prof_backtrans_bleu": 65.00905416560468,
    "llm_vs_prof_backtrans_chrf": 82.11753739683729,
    "llm_vs_prof_backtrans_bertscore": 0.9482288956642151,
    "llm_vs_prof_backtrans_labse": 0.9366721510887146
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 10.985155892224505,
    "same_lang_chrf": 36.14365522943655,
    "same_lang_bertscore": 0.805720865726471,
    "same_lang_comet": 0.8521110415458679,
    "cross_lang_xlm_roberta": 0.9845348596572876,
    "cross_lang_labse": 0.9909228682518005,
    "cross_lang_mbert": 0.9389051198959351,
    "cross_lang_comet_qe": -0.07701858878135681,
    "backtrans_bleu": 61.10760678806967,
    "backtrans_chrf": 81.67454865400597,
    "backtrans_bertscore": 0.9061910510063171,
    "prof_backtrans_bleu": 55.71958618181051,
    "prof_backtrans_chrf": 79.96574487544031,
    "prof_backtrans_bertscore": 0.8616438508033752,
    "prof_backtrans_labse": 0.8725121021270752,
    "prof_backtrans_xlm_roberta": 0.6510714292526245,
    "llm_vs_prof_backtrans_bleu": 61.69278138435302,
    "llm_vs_prof_backtrans_chrf": 81.92284121026617,
    "llm_vs_prof_backtrans_bertscore": 0.8792228698730469,
    "llm_vs_prof_backtrans_labse": 0.868597686290741
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 45.40687966437673,
    "same_lang_chrf": 63.526239580120006,
    "same_lang_bertscore": 0.8317790031433105,
    "same_lang_comet": 0.8363274335861206,
    "cross_lang_xlm_roberta": 0.9942200183868408,
    "cross_lang_labse": 0.9959243535995483,
    "cross_lang_mbert": 0.9790260791778564,
    "cross_lang_comet_qe": -0.12384545058012009,
    "backtrans_bleu": 70.71279521129094,
    "backtrans_chrf": 86.76672952767221,
    "backtrans_bertscore": 0.9377050995826721,
    "prof_backtrans_bleu": 49.83060159723253,
    "prof_backtrans_chrf": 76.95153763536655,
    "prof_backtrans_bertscore": 0.8710617423057556,
    "prof_backtrans_labse": 0.8862178325653076,
    "prof_backtrans_xlm_roberta": 0.6617634892463684,
    "llm_vs_prof_backtrans_bleu": 54.493321612015244,
    "llm_vs_prof_backtrans_chrf": 79.51454214913853,
    "llm_vs_prof_backtrans_bertscore": 0.8724267482757568,
    "llm_vs_prof_backtrans_labse": 0.8893527984619141
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 37.507552153395345,
    "same_lang_chrf": 68.53133607098249,
    "same_lang_bertscore": 0.8242999911308289,
    "same_lang_comet": 0.860276460647583,
    "cross_lang_xlm_roberta": 0.9862512946128845,
    "cross_lang_labse": 0.9646472930908203,
    "cross_lang_mbert": 0.9798574447631836,
    "cross_lang_comet_qe": 0.10761773586273193,
    "backtrans_bleu": 72.87398540246373,
    "backtrans_chrf": 87.9670369449445,
    "backtrans_bertscore": 0.9022191166877747,
    "prof_backtrans_bleu": 53.34093540903466,
    "prof_backtrans_chrf": 79.2215663258201,
    "prof_backtrans_bertscore": 0.9034150242805481,
    "prof_backtrans_labse": 0.9627938270568848,
    "prof_backtrans_xlm_roberta": 0.8998686671257019,
    "llm_vs_prof_backtrans_bleu": 59.76181920649273,
    "llm_vs_prof_backtrans_chrf": 79.16244400829416,
    "llm_vs_prof_backtrans_bertscore": 0.8809677362442017,
    "llm_vs_prof_backtrans_labse": 0.9401272535324097
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 37.99689362154508,
    "same_lang_chrf": 67.21636797581866,
    "same_lang_bertscore": 0.8201625347137451,
    "same_lang_comet": 0.8792393803596497,
    "cross_lang_xlm_roberta": 0.9712916612625122,
    "cross_lang_labse": 0.9757954478263855,
    "cross_lang_mbert": 0.950269341468811,
    "cross_lang_comet_qe": 0.08536472171545029,
    "backtrans_bleu": 76.28793503284214,
    "backtrans_chrf": 90.69279385251764,
    "backtrans_bertscore": 0.90959233045578,
    "prof_backtrans_bleu": 41.370763340610615,
    "prof_backtrans_chrf": 71.29864907978612,
    "prof_backtrans_bertscore": 0.8695161938667297,
    "prof_backtrans_labse": 0.8626182079315186,
    "prof_backtrans_xlm_roberta": 0.6819407939910889,
    "llm_vs_prof_backtrans_bleu": 47.239719962493574,
    "llm_vs_prof_backtrans_chrf": 69.13926738570734,
    "llm_vs_prof_backtrans_bertscore": 0.917238712310791,
    "llm_vs_prof_backtrans_labse": 0.9047026634216309
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 31.914397919433373,
    "same_lang_chrf": 51.28375343297914,
    "same_lang_bertscore": 0.876504123210907,
    "same_lang_comet": 0.8948150277137756,
    "cross_lang_xlm_roberta": 0.9862500429153442,
    "cross_lang_labse": 0.9915446043014526,
    "cross_lang_mbert": 0.9163497090339661,
    "cross_lang_comet_qe": -0.03278758376836777,
    "backtrans_bleu": 65.8114672013466,
    "backtrans_chrf": 83.89495019654136,
    "backtrans_bertscore": 0.9123626351356506,
    "prof_backtrans_bleu": 54.29579016289278,
    "prof_backtrans_chrf": 78.9488877132652,
    "prof_backtrans_bertscore": 0.8893141746520996,
    "prof_backtrans_labse": 0.9237930774688721,
    "prof_backtrans_xlm_roberta": 0.8228710293769836,
    "llm_vs_prof_backtrans_bleu": 61.978956116711295,
    "llm_vs_prof_backtrans_chrf": 82.19710895415776,
    "llm_vs_prof_backtrans_bertscore": 0.9060972929000854,
    "llm_vs_prof_backtrans_labse": 0.9235072731971741
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 27.728983831036075,
    "same_lang_chrf": 58.11317501502867,
    "same_lang_bertscore": 0.8167511820793152,
    "same_lang_comet": 0.7708196640014648,
    "cross_lang_xlm_roberta": 0.994799017906189,
    "cross_lang_labse": 0.9905749559402466,
    "cross_lang_mbert": 0.9448952674865723,
    "cross_lang_comet_qe": -0.14863187074661255,
    "backtrans_bleu": 63.43525093596538,
    "backtrans_chrf": 82.95152934767933,
    "backtrans_bertscore": 0.9328477382659912,
    "prof_backtrans_bleu": 42.36460197924165,
    "prof_backtrans_chrf": 72.87391009341056,
    "prof_backtrans_bertscore": 0.8969045877456665,
    "prof_backtrans_labse": 0.9377480745315552,
    "prof_backtrans_xlm_roberta": 0.8623502850532532,
    "llm_vs_prof_backtrans_bleu": 46.19624585810164,
    "llm_vs_prof_backtrans_chrf": 71.73068698837774,
    "llm_vs_prof_backtrans_bertscore": 0.8953231573104858,
    "llm_vs_prof_backtrans_labse": 0.9366470575332642
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 51.76787625951119,
    "same_lang_chrf": 75.01430876380746,
    "same_lang_bertscore": 0.8272685408592224,
    "same_lang_comet": 0.8020938634872437,
    "cross_lang_xlm_roberta": 0.7048240303993225,
    "cross_lang_labse": 0.8842886686325073,
    "cross_lang_mbert": 0.7723897099494934,
    "cross_lang_comet_qe": 0.1722925305366516,
    "backtrans_bleu": 63.70812775822041,
    "backtrans_chrf": 83.4731208146208,
    "backtrans_bertscore": 0.8794248700141907,
    "prof_backtrans_bleu": 45.87023701803081,
    "prof_backtrans_chrf": 76.54884149940561,
    "prof_backtrans_bertscore": 0.8719819784164429,
    "prof_backtrans_labse": 0.8757604360580444,
    "prof_backtrans_xlm_roberta": 0.7864788770675659,
    "llm_vs_prof_backtrans_bleu": 62.58654259294391,
    "llm_vs_prof_backtrans_chrf": 78.46480989413631,
    "llm_vs_prof_backtrans_bertscore": 0.9336270093917847,
    "llm_vs_prof_backtrans_labse": 0.9045448899269104
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 19.812841136801932,
    "same_lang_chrf": 36.851243399372805,
    "same_lang_bertscore": 0.7880498766899109,
    "same_lang_comet": 0.8667169213294983,
    "cross_lang_xlm_roberta": 0.7409010529518127,
    "cross_lang_labse": 0.8287745714187622,
    "cross_lang_mbert": 0.7604564428329468,
    "cross_lang_comet_qe": 0.22123557329177856,
    "backtrans_bleu": 54.518766045681225,
    "backtrans_chrf": 81.06489382595666,
    "backtrans_bertscore": 0.8690782189369202,
    "prof_backtrans_bleu": 43.424012988537534,
    "prof_backtrans_chrf": 74.80126288602472,
    "prof_backtrans_bertscore": 0.8678818345069885,
    "prof_backtrans_labse": 0.8716493844985962,
    "prof_backtrans_xlm_roberta": 0.6575053334236145,
    "llm_vs_prof_backtrans_bleu": 49.832148533121355,
    "llm_vs_prof_backtrans_chrf": 72.36035673664298,
    "llm_vs_prof_backtrans_bertscore": 0.9266409873962402,
    "llm_vs_prof_backtrans_labse": 0.9014026522636414
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 50.829589618760984,
    "same_lang_chrf": 66.27194515527685,
    "same_lang_bertscore": 0.8060408234596252,
    "same_lang_comet": 0.8571130037307739,
    "cross_lang_xlm_roberta": 0.7029331922531128,
    "cross_lang_labse": 0.8382377028465271,
    "cross_lang_mbert": 0.7545584440231323,
    "cross_lang_comet_qe": 0.18303291499614716,
    "backtrans_bleu": 59.81244376623172,
    "backtrans_chrf": 82.99316598072913,
    "backtrans_bertscore": 0.8740152716636658,
    "prof_backtrans_bleu": 42.74908963967602,
    "prof_backtrans_chrf": 73.79541075081346,
    "prof_backtrans_bertscore": 0.8905472159385681,
    "prof_backtrans_labse": 0.9126436114311218,
    "prof_backtrans_xlm_roberta": 0.847021222114563,
    "llm_vs_prof_backtrans_bleu": 49.96353708946484,
    "llm_vs_prof_backtrans_chrf": 72.52125818419928,
    "llm_vs_prof_backtrans_bertscore": 0.8708230257034302,
    "llm_vs_prof_backtrans_labse": 0.8410487174987793
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 35.08961741947474,
    "same_lang_chrf": 68.66454095889088,
    "same_lang_bertscore": 0.791527271270752,
    "same_lang_comet": 0.8484796285629272,
    "cross_lang_xlm_roberta": 0.7116031646728516,
    "cross_lang_labse": 0.8548976182937622,
    "cross_lang_mbert": 0.7772057056427002,
    "cross_lang_comet_qe": 0.1651436686515808,
    "backtrans_bleu": 58.926802647016736,
    "backtrans_chrf": 83.59348689615484,
    "backtrans_bertscore": 0.8696460723876953,
    "prof_backtrans_bleu": 48.258135997067384,
    "prof_backtrans_chrf": 76.70844779312577,
    "prof_backtrans_bertscore": 0.8646146059036255,
    "prof_backtrans_labse": 0.8823078274726868,
    "prof_backtrans_xlm_roberta": 0.7285991907119751,
    "llm_vs_prof_backtrans_bleu": 53.88608299332403,
    "llm_vs_prof_backtrans_chrf": 74.4337620589642,
    "llm_vs_prof_backtrans_bertscore": 0.9367830157279968,
    "llm_vs_prof_backtrans_labse": 0.9485021829605103
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 33.79208165908804,
    "same_lang_chrf": 65.13770342638621,
    "same_lang_bertscore": 0.8044657707214355,
    "same_lang_comet": 0.8643649220466614,
    "cross_lang_xlm_roberta": 0.7962774634361267,
    "cross_lang_labse": 0.8698161840438843,
    "cross_lang_mbert": 0.8195861577987671,
    "cross_lang_comet_qe": 0.1606457680463791,
    "backtrans_bleu": 59.23404586023701,
    "backtrans_chrf": 85.66828772338123,
    "backtrans_bertscore": 0.8782566785812378,
    "prof_backtrans_bleu": 28.019175082464496,
    "prof_backtrans_chrf": 65.22256104501616,
    "prof_backtrans_bertscore": 0.8686283826828003,
    "prof_backtrans_labse": 0.8853466510772705,
    "prof_backtrans_xlm_roberta": 0.6849873661994934,
    "llm_vs_prof_backtrans_bleu": 30.996076427762425,
    "llm_vs_prof_backtrans_chrf": 58.88787572072848,
    "llm_vs_prof_backtrans_bertscore": 0.8427422046661377,
    "llm_vs_prof_backtrans_labse": 0.8590731620788574
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 30.643700822536555,
    "same_lang_chrf": 50.75254756121286,
    "same_lang_bertscore": 0.8107200264930725,
    "same_lang_comet": 0.8826236724853516,
    "cross_lang_xlm_roberta": 0.6313472390174866,
    "cross_lang_labse": 0.8318064212799072,
    "cross_lang_mbert": 0.7624759078025818,
    "cross_lang_comet_qe": 0.1976289004087448,
    "backtrans_bleu": 62.61092700630787,
    "backtrans_chrf": 81.45360730280309,
    "backtrans_bertscore": 0.8710834383964539,
    "prof_backtrans_bleu": 60.22860344269789,
    "prof_backtrans_chrf": 82.01392858605774,
    "prof_backtrans_bertscore": 0.8724693059921265,
    "prof_backtrans_labse": 0.8424671292304993,
    "prof_backtrans_xlm_roberta": 0.641459047794342,
    "llm_vs_prof_backtrans_bleu": 71.40270202511971,
    "llm_vs_prof_backtrans_chrf": 84.63475848560698,
    "llm_vs_prof_backtrans_bertscore": 0.962763786315918,
    "llm_vs_prof_backtrans_labse": 0.9700900912284851
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 29.580518127508586,
    "same_lang_chrf": 60.512333513614166,
    "same_lang_bertscore": 0.7896568775177002,
    "same_lang_comet": 0.8580336570739746,
    "cross_lang_xlm_roberta": 0.7298742532730103,
    "cross_lang_labse": 0.8518699407577515,
    "cross_lang_mbert": 0.7813778519630432,
    "cross_lang_comet_qe": 0.11686073988676071,
    "backtrans_bleu": 50.63179652585547,
    "backtrans_chrf": 77.03156464459745,
    "backtrans_bertscore": 0.8688435554504395,
    "prof_backtrans_bleu": 37.35905500773699,
    "prof_backtrans_chrf": 70.05534851396462,
    "prof_backtrans_bertscore": 0.8631604909896851,
    "prof_backtrans_labse": 0.878319501876831,
    "prof_backtrans_xlm_roberta": 0.7158011198043823,
    "llm_vs_prof_backtrans_bleu": 46.27693981380829,
    "llm_vs_prof_backtrans_chrf": 70.74113611763293,
    "llm_vs_prof_backtrans_bertscore": 0.9259964227676392,
    "llm_vs_prof_backtrans_labse": 0.9412712454795837
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 50.89572340982491,
    "same_lang_chrf": 73.6251126847332,
    "same_lang_bertscore": 0.8230797052383423,
    "same_lang_comet": 0.7966111898422241,
    "cross_lang_xlm_roberta": 0.7143086791038513,
    "cross_lang_labse": 0.8849253058433533,
    "cross_lang_mbert": 0.7616548538208008,
    "cross_lang_comet_qe": 0.2205536663532257,
    "backtrans_bleu": 63.25122843767673,
    "backtrans_chrf": 82.10487601193314,
    "backtrans_bertscore": 0.8791818022727966,
    "prof_backtrans_bleu": 39.42039721376876,
    "prof_backtrans_chrf": 72.28216900187684,
    "prof_backtrans_bertscore": 0.8755351305007935,
    "prof_backtrans_labse": 0.8951684832572937,
    "prof_backtrans_xlm_roberta": 0.8151616454124451,
    "llm_vs_prof_backtrans_bleu": 49.62216704768883,
    "llm_vs_prof_backtrans_chrf": 67.38537317312598,
    "llm_vs_prof_backtrans_bertscore": 0.9262793064117432,
    "llm_vs_prof_backtrans_labse": 0.8871880173683167
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 7.65156075034577,
    "same_lang_chrf": 39.17854970821637,
    "same_lang_bertscore": 0.7830907702445984,
    "same_lang_comet": 0.8522164821624756,
    "cross_lang_xlm_roberta": 0.8035008311271667,
    "cross_lang_labse": 0.8921334743499756,
    "cross_lang_mbert": 0.8199489116668701,
    "cross_lang_comet_qe": 0.1763235330581665,
    "backtrans_bleu": 50.063161116338875,
    "backtrans_chrf": 77.40711088351334,
    "backtrans_bertscore": 0.8786602020263672,
    "prof_backtrans_bleu": 40.371418386414625,
    "prof_backtrans_chrf": 72.3103243646776,
    "prof_backtrans_bertscore": 0.8584439754486084,
    "prof_backtrans_labse": 0.875203549861908,
    "prof_backtrans_xlm_roberta": 0.6555798053741455,
    "llm_vs_prof_backtrans_bleu": 55.83465370706153,
    "llm_vs_prof_backtrans_chrf": 75.01586668805176,
    "llm_vs_prof_backtrans_bertscore": 0.9075738191604614,
    "llm_vs_prof_backtrans_labse": 0.928777277469635
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 44.669342196090604,
    "same_lang_chrf": 62.07457438422828,
    "same_lang_bertscore": 0.7973377108573914,
    "same_lang_comet": 0.86213219165802,
    "cross_lang_xlm_roberta": 0.7810817360877991,
    "cross_lang_labse": 0.8966783881187439,
    "cross_lang_mbert": 0.7189594507217407,
    "cross_lang_comet_qe": 0.15889209508895874,
    "backtrans_bleu": 55.2130939428848,
    "backtrans_chrf": 79.43864250209549,
    "backtrans_bertscore": 0.8777777552604675,
    "prof_backtrans_bleu": 46.503326608351706,
    "prof_backtrans_chrf": 73.55887903959695,
    "prof_backtrans_bertscore": 0.8797958493232727,
    "prof_backtrans_labse": 0.9314413666725159,
    "prof_backtrans_xlm_roberta": 0.8578270077705383,
    "llm_vs_prof_backtrans_bleu": 61.00349484701423,
    "llm_vs_prof_backtrans_chrf": 78.25862232102875,
    "llm_vs_prof_backtrans_bertscore": 0.9134207963943481,
    "llm_vs_prof_backtrans_labse": 0.8821744918823242
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 35.86285267075382,
    "same_lang_chrf": 68.30760886013566,
    "same_lang_bertscore": 0.7923778295516968,
    "same_lang_comet": 0.8594447374343872,
    "cross_lang_xlm_roberta": 0.785297691822052,
    "cross_lang_labse": 0.8965057134628296,
    "cross_lang_mbert": 0.8747085928916931,
    "cross_lang_comet_qe": 0.1204557791352272,
    "backtrans_bleu": 63.42626498463514,
    "backtrans_chrf": 82.16505878467265,
    "backtrans_bertscore": 0.8912844657897949,
    "prof_backtrans_bleu": 52.66203020793166,
    "prof_backtrans_chrf": 79.35754203693153,
    "prof_backtrans_bertscore": 0.8771941065788269,
    "prof_backtrans_labse": 0.8968799114227295,
    "prof_backtrans_xlm_roberta": 0.7384610772132874,
    "llm_vs_prof_backtrans_bleu": 60.26875620269895,
    "llm_vs_prof_backtrans_chrf": 77.61936783151427,
    "llm_vs_prof_backtrans_bertscore": 0.9364985227584839,
    "llm_vs_prof_backtrans_labse": 0.9427984952926636
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 32.825896776569515,
    "same_lang_chrf": 61.28817909635435,
    "same_lang_bertscore": 0.7976940870285034,
    "same_lang_comet": 0.8659912943840027,
    "cross_lang_xlm_roberta": 0.7927291989326477,
    "cross_lang_labse": 0.8935455083847046,
    "cross_lang_mbert": 0.8082860112190247,
    "cross_lang_comet_qe": 0.12084560096263885,
    "backtrans_bleu": 37.84827001973191,
    "backtrans_chrf": 66.33232288094007,
    "backtrans_bertscore": 0.8785611987113953,
    "prof_backtrans_bleu": 30.82816017246442,
    "prof_backtrans_chrf": 65.53746379551119,
    "prof_backtrans_bertscore": 0.8613298535346985,
    "prof_backtrans_labse": 0.8544682860374451,
    "prof_backtrans_xlm_roberta": 0.6828276515007019,
    "llm_vs_prof_backtrans_bleu": 36.42809274753195,
    "llm_vs_prof_backtrans_chrf": 60.490446896651825,
    "llm_vs_prof_backtrans_bertscore": 0.9153561592102051,
    "llm_vs_prof_backtrans_labse": 0.9130561351776123
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 32.2118435830589,
    "same_lang_chrf": 50.36768123108889,
    "same_lang_bertscore": 0.8246220946311951,
    "same_lang_comet": 0.8898028135299683,
    "cross_lang_xlm_roberta": 0.805311381816864,
    "cross_lang_labse": 0.8857975602149963,
    "cross_lang_mbert": 0.8150275945663452,
    "cross_lang_comet_qe": 0.16030196845531464,
    "backtrans_bleu": 41.250909066025784,
    "backtrans_chrf": 72.05315451536771,
    "backtrans_bertscore": 0.8789401054382324,
    "prof_backtrans_bleu": 45.62729883387511,
    "prof_backtrans_chrf": 73.88096349524041,
    "prof_backtrans_bertscore": 0.8829919099807739,
    "prof_backtrans_labse": 0.9549359083175659,
    "prof_backtrans_xlm_roberta": 0.8615441918373108,
    "llm_vs_prof_backtrans_bleu": 52.88496467952866,
    "llm_vs_prof_backtrans_chrf": 75.25475635938396,
    "llm_vs_prof_backtrans_bertscore": 0.9217231869697571,
    "llm_vs_prof_backtrans_labse": 0.9057236909866333
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 22.289746885386,
    "same_lang_chrf": 52.337635853320805,
    "same_lang_bertscore": 0.7900709509849548,
    "same_lang_comet": 0.7519001960754395,
    "cross_lang_xlm_roberta": 0.7900139689445496,
    "cross_lang_labse": 0.8990039229393005,
    "cross_lang_mbert": 0.7763065695762634,
    "cross_lang_comet_qe": 0.15110376477241516,
    "backtrans_bleu": 59.003427644524315,
    "backtrans_chrf": 80.267204854207,
    "backtrans_bertscore": 0.8842232823371887,
    "prof_backtrans_bleu": 28.97429395630684,
    "prof_backtrans_chrf": 63.97631023264522,
    "prof_backtrans_bertscore": 0.8690000176429749,
    "prof_backtrans_labse": 0.8912271857261658,
    "prof_backtrans_xlm_roberta": 0.7023657560348511,
    "llm_vs_prof_backtrans_bleu": 32.84175717980732,
    "llm_vs_prof_backtrans_chrf": 59.74379782738215,
    "llm_vs_prof_backtrans_bertscore": 0.8838504552841187,
    "llm_vs_prof_backtrans_labse": 0.9333117008209229
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 49.65024390936853,
    "same_lang_chrf": 76.21207333884824,
    "same_lang_bertscore": 0.8160833120346069,
    "same_lang_comet": 0.8171907663345337,
    "cross_lang_xlm_roberta": 0.9273993372917175,
    "cross_lang_labse": 0.8581559658050537,
    "cross_lang_mbert": 0.879144549369812,
    "cross_lang_comet_qe": 0.18364176154136658,
    "backtrans_bleu": 63.62053741921649,
    "backtrans_chrf": 84.72059418535525,
    "backtrans_bertscore": 0.8556569218635559,
    "prof_backtrans_bleu": 46.71317180901537,
    "prof_backtrans_chrf": 78.39515963926954,
    "prof_backtrans_bertscore": 0.8954487442970276,
    "prof_backtrans_labse": 0.9553770422935486,
    "prof_backtrans_xlm_roberta": 0.907467782497406,
    "llm_vs_prof_backtrans_bleu": 57.759675044057374,
    "llm_vs_prof_backtrans_chrf": 80.68053955532072,
    "llm_vs_prof_backtrans_bertscore": 0.8522132635116577,
    "llm_vs_prof_backtrans_labse": 0.8504409790039062
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 11.3455191252756,
    "same_lang_chrf": 36.47647659837246,
    "same_lang_bertscore": 0.7710898518562317,
    "same_lang_comet": 0.8297223448753357,
    "cross_lang_xlm_roberta": 0.9262785911560059,
    "cross_lang_labse": 0.8508238196372986,
    "cross_lang_mbert": 0.9076008200645447,
    "cross_lang_comet_qe": 0.1365688145160675,
    "backtrans_bleu": 55.139914355156684,
    "backtrans_chrf": 80.84205969707386,
    "backtrans_bertscore": 0.8695271611213684,
    "prof_backtrans_bleu": 36.181624408122715,
    "prof_backtrans_chrf": 71.20971237406879,
    "prof_backtrans_bertscore": 0.860303521156311,
    "prof_backtrans_labse": 0.8826451897621155,
    "prof_backtrans_xlm_roberta": 0.6357136368751526,
    "llm_vs_prof_backtrans_bleu": 42.73353051834451,
    "llm_vs_prof_backtrans_chrf": 70.76839647000438,
    "llm_vs_prof_backtrans_bertscore": 0.8512634634971619,
    "llm_vs_prof_backtrans_labse": 0.7446315884590149
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 37.60037727715081,
    "same_lang_chrf": 56.93971681761679,
    "same_lang_bertscore": 0.7673179507255554,
    "same_lang_comet": 0.8299620151519775,
    "cross_lang_xlm_roberta": 0.9223057627677917,
    "cross_lang_labse": 0.8612913489341736,
    "cross_lang_mbert": 0.91079181432724,
    "cross_lang_comet_qe": 0.11706238240003586,
    "backtrans_bleu": 59.15789982285287,
    "backtrans_chrf": 81.96267205428934,
    "backtrans_bertscore": 0.8750835657119751,
    "prof_backtrans_bleu": 54.01450684914705,
    "prof_backtrans_chrf": 80.058184056361,
    "prof_backtrans_bertscore": 0.8864547610282898,
    "prof_backtrans_labse": 0.8699003458023071,
    "prof_backtrans_xlm_roberta": 0.6198511123657227,
    "llm_vs_prof_backtrans_bleu": 53.23018258527215,
    "llm_vs_prof_backtrans_chrf": 74.93641923149652,
    "llm_vs_prof_backtrans_bertscore": 0.848861575126648,
    "llm_vs_prof_backtrans_labse": 0.8171365857124329
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 39.45595780416396,
    "same_lang_chrf": 67.0535159559268,
    "same_lang_bertscore": 0.7800782918930054,
    "same_lang_comet": 0.8211563229560852,
    "cross_lang_xlm_roberta": 0.9331849217414856,
    "cross_lang_labse": 0.8607833385467529,
    "cross_lang_mbert": 0.9309825301170349,
    "cross_lang_comet_qe": 0.10389735549688339,
    "backtrans_bleu": 71.3966033467248,
    "backtrans_chrf": 87.47468208458949,
    "backtrans_bertscore": 0.8726723194122314,
    "prof_backtrans_bleu": 63.54479241683604,
    "prof_backtrans_chrf": 85.44496391268146,
    "prof_backtrans_bertscore": 0.8766905665397644,
    "prof_backtrans_labse": 0.8224383592605591,
    "prof_backtrans_xlm_roberta": 0.7359306216239929,
    "llm_vs_prof_backtrans_bleu": 68.85452002216984,
    "llm_vs_prof_backtrans_chrf": 85.5874588125579,
    "llm_vs_prof_backtrans_bertscore": 0.8582214713096619,
    "llm_vs_prof_backtrans_labse": 0.8025063276290894
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 18.72228531887031,
    "same_lang_chrf": 58.55852992895758,
    "same_lang_bertscore": 0.7998256087303162,
    "same_lang_comet": 0.8123643398284912,
    "cross_lang_xlm_roberta": 0.9428801536560059,
    "cross_lang_labse": 0.8659796714782715,
    "cross_lang_mbert": 0.905648946762085,
    "cross_lang_comet_qe": 0.1517075151205063,
    "backtrans_bleu": 54.82573677449132,
    "backtrans_chrf": 80.83757606848417,
    "backtrans_bertscore": 0.8729001879692078,
    "prof_backtrans_bleu": 34.95278291675753,
    "prof_backtrans_chrf": 71.75082958612526,
    "prof_backtrans_bertscore": 0.8669725656509399,
    "prof_backtrans_labse": 0.8694295883178711,
    "prof_backtrans_xlm_roberta": 0.6387337446212769,
    "llm_vs_prof_backtrans_bleu": 34.73172755777259,
    "llm_vs_prof_backtrans_chrf": 66.62451758173725,
    "llm_vs_prof_backtrans_bertscore": 0.8388147950172424,
    "llm_vs_prof_backtrans_labse": 0.8061608672142029
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 10.704468389748937,
    "same_lang_chrf": 38.94653889155707,
    "same_lang_bertscore": 0.7087603807449341,
    "same_lang_comet": 0.7892169952392578,
    "cross_lang_xlm_roberta": 0.9163199067115784,
    "cross_lang_labse": 0.8518792390823364,
    "cross_lang_mbert": 0.8895100355148315,
    "cross_lang_comet_qe": 0.17088614404201508,
    "backtrans_bleu": 60.17013138050346,
    "backtrans_chrf": 82.10489882517528,
    "backtrans_bertscore": 0.8533140420913696,
    "prof_backtrans_bleu": 40.5936476633975,
    "prof_backtrans_chrf": 73.67757468792891,
    "prof_backtrans_bertscore": 0.8611809611320496,
    "prof_backtrans_labse": 0.8673158288002014,
    "prof_backtrans_xlm_roberta": 0.6412841081619263,
    "llm_vs_prof_backtrans_bleu": 44.90081958007637,
    "llm_vs_prof_backtrans_chrf": 71.99033043028021,
    "llm_vs_prof_backtrans_bertscore": 0.862235963344574,
    "llm_vs_prof_backtrans_labse": 0.7909859418869019
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 29.7435969702565,
    "same_lang_chrf": 57.0815692684457,
    "same_lang_bertscore": 0.7968844771385193,
    "same_lang_comet": 0.7616918087005615,
    "cross_lang_xlm_roberta": 0.9927098155021667,
    "cross_lang_labse": 0.9868090152740479,
    "cross_lang_mbert": 0.9467374086380005,
    "cross_lang_comet_qe": -0.13328313827514648,
    "backtrans_bleu": 59.78470361716501,
    "backtrans_chrf": 81.96708809660798,
    "backtrans_bertscore": 0.9389617443084717,
    "prof_backtrans_bleu": 43.933951747344764,
    "prof_backtrans_chrf": 75.39108826927526,
    "prof_backtrans_bertscore": 0.8921839594841003,
    "prof_backtrans_labse": 0.9249196648597717,
    "prof_backtrans_xlm_roberta": 0.6843642592430115,
    "llm_vs_prof_backtrans_bleu": 46.51109254989308,
    "llm_vs_prof_backtrans_chrf": 72.29452076103749,
    "llm_vs_prof_backtrans_bertscore": 0.8909967541694641,
    "llm_vs_prof_backtrans_labse": 0.9221161007881165
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 41.42375215325616,
    "same_lang_chrf": 74.65347484132843,
    "same_lang_bertscore": 0.8346585035324097,
    "same_lang_comet": 0.8011807203292847,
    "cross_lang_xlm_roberta": 0.9861548542976379,
    "cross_lang_labse": 0.9908993244171143,
    "cross_lang_mbert": 0.9730910062789917,
    "cross_lang_comet_qe": -0.2010599970817566,
    "backtrans_bleu": 63.19998210507197,
    "backtrans_chrf": 84.68863659158013,
    "backtrans_bertscore": 0.9180371761322021,
    "prof_backtrans_bleu": 46.28404874734654,
    "prof_backtrans_chrf": 77.62520838092371,
    "prof_backtrans_bertscore": 0.8870695233345032,
    "prof_backtrans_labse": 0.9213722944259644,
    "prof_backtrans_xlm_roberta": 0.9086100459098816,
    "llm_vs_prof_backtrans_bleu": 49.34796439960596,
    "llm_vs_prof_backtrans_chrf": 78.25526436585488,
    "llm_vs_prof_backtrans_bertscore": 0.9001632332801819,
    "llm_vs_prof_backtrans_labse": 0.9116717576980591
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.025264812431207,
    "same_lang_chrf": 35.66635153094361,
    "same_lang_bertscore": 0.7970619201660156,
    "same_lang_comet": 0.826546311378479,
    "cross_lang_xlm_roberta": 0.9442565441131592,
    "cross_lang_labse": 0.9389095902442932,
    "cross_lang_mbert": 0.9133496880531311,
    "cross_lang_comet_qe": 0.09829334914684296,
    "backtrans_bleu": 56.21698049822657,
    "backtrans_chrf": 80.57843470231424,
    "backtrans_bertscore": 0.9010715484619141,
    "prof_backtrans_bleu": 58.67393331061992,
    "prof_backtrans_chrf": 81.11506438035362,
    "prof_backtrans_bertscore": 0.8595677614212036,
    "prof_backtrans_labse": 0.8375600576400757,
    "prof_backtrans_xlm_roberta": 0.6447117924690247,
    "llm_vs_prof_backtrans_bleu": 66.54405314148285,
    "llm_vs_prof_backtrans_chrf": 82.39317795275882,
    "llm_vs_prof_backtrans_bertscore": 0.9033747315406799,
    "llm_vs_prof_backtrans_labse": 0.9095617532730103
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 40.03867523303792,
    "same_lang_chrf": 59.8121489043281,
    "same_lang_bertscore": 0.8155825734138489,
    "same_lang_comet": 0.8129304647445679,
    "cross_lang_xlm_roberta": 0.9978437423706055,
    "cross_lang_labse": 0.9953002333641052,
    "cross_lang_mbert": 0.9887205958366394,
    "cross_lang_comet_qe": -0.16306321322917938,
    "backtrans_bleu": 63.28232529522205,
    "backtrans_chrf": 84.72724372650927,
    "backtrans_bertscore": 0.9475400447845459,
    "prof_backtrans_bleu": 51.68566030168776,
    "prof_backtrans_chrf": 78.1087641023125,
    "prof_backtrans_bertscore": 0.859583854675293,
    "prof_backtrans_labse": 0.8375510573387146,
    "prof_backtrans_xlm_roberta": 0.6851908564567566,
    "llm_vs_prof_backtrans_bleu": 57.73474556235646,
    "llm_vs_prof_backtrans_chrf": 78.45411429533206,
    "llm_vs_prof_backtrans_bertscore": 0.8653861880302429,
    "llm_vs_prof_backtrans_labse": 0.8413013219833374
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 45.345410105955715,
    "same_lang_chrf": 74.06194788740804,
    "same_lang_bertscore": 0.7970634698867798,
    "same_lang_comet": 0.8221957683563232,
    "cross_lang_xlm_roberta": 0.9938209056854248,
    "cross_lang_labse": 0.9932337403297424,
    "cross_lang_mbert": 0.9953317046165466,
    "cross_lang_comet_qe": -0.2126600295305252,
    "backtrans_bleu": 65.01064196963789,
    "backtrans_chrf": 85.64753471973935,
    "backtrans_bertscore": 0.9285157918930054,
    "prof_backtrans_bleu": 63.935387595787425,
    "prof_backtrans_chrf": 85.43388779860095,
    "prof_backtrans_bertscore": 0.8746380805969238,
    "prof_backtrans_labse": 0.7983366847038269,
    "prof_backtrans_xlm_roberta": 0.7394025325775146,
    "llm_vs_prof_backtrans_bleu": 66.50572905548327,
    "llm_vs_prof_backtrans_chrf": 84.81928691772967,
    "llm_vs_prof_backtrans_bertscore": 0.8776569962501526,
    "llm_vs_prof_backtrans_labse": 0.7946076989173889
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 23.623147004512155,
    "same_lang_chrf": 60.26030314701381,
    "same_lang_bertscore": 0.805494487285614,
    "same_lang_comet": 0.8171842098236084,
    "cross_lang_xlm_roberta": 0.997189998626709,
    "cross_lang_labse": 0.9970499277114868,
    "cross_lang_mbert": 0.9768643975257874,
    "cross_lang_comet_qe": -0.16433320939540863,
    "backtrans_bleu": 67.78164451269205,
    "backtrans_chrf": 86.41262304227808,
    "backtrans_bertscore": 0.9497547149658203,
    "prof_backtrans_bleu": 33.230389609145135,
    "prof_backtrans_chrf": 70.97599916749138,
    "prof_backtrans_bertscore": 0.8650734424591064,
    "prof_backtrans_labse": 0.875630795955658,
    "prof_backtrans_xlm_roberta": 0.6299024820327759,
    "llm_vs_prof_backtrans_bleu": 33.134138216481205,
    "llm_vs_prof_backtrans_chrf": 65.06413144351264,
    "llm_vs_prof_backtrans_bertscore": 0.8740102052688599,
    "llm_vs_prof_backtrans_labse": 0.8749481439590454
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 9.982587676393651,
    "same_lang_chrf": 37.94103426440116,
    "same_lang_bertscore": 0.7480217218399048,
    "same_lang_comet": 0.7776855230331421,
    "cross_lang_xlm_roberta": 0.9796786308288574,
    "cross_lang_labse": 0.9822795391082764,
    "cross_lang_mbert": 0.9487124681472778,
    "cross_lang_comet_qe": -0.1208670437335968,
    "backtrans_bleu": 61.25436918179163,
    "backtrans_chrf": 83.24481159983395,
    "backtrans_bertscore": 0.9104534983634949,
    "prof_backtrans_bleu": 38.40592680636474,
    "prof_backtrans_chrf": 73.54127457346341,
    "prof_backtrans_bertscore": 0.8740933537483215,
    "prof_backtrans_labse": 0.8868451118469238,
    "prof_backtrans_xlm_roberta": 0.6461531519889832,
    "llm_vs_prof_backtrans_bleu": 41.48773633512377,
    "llm_vs_prof_backtrans_chrf": 74.28268726566985,
    "llm_vs_prof_backtrans_bertscore": 0.8795847296714783,
    "llm_vs_prof_backtrans_labse": 0.8826454281806946
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 26.852165323668352,
    "same_lang_chrf": 56.57954899274884,
    "same_lang_bertscore": 0.8124459385871887,
    "same_lang_comet": 0.7614738941192627,
    "cross_lang_xlm_roberta": 0.9949511885643005,
    "cross_lang_labse": 0.993010401725769,
    "cross_lang_mbert": 0.9642056226730347,
    "cross_lang_comet_qe": -0.19210702180862427,
    "backtrans_bleu": 63.618379425205994,
    "backtrans_chrf": 83.98374247955009,
    "backtrans_bertscore": 0.952636182308197,
    "prof_backtrans_bleu": 34.40489158540527,
    "prof_backtrans_chrf": 70.71957312061333,
    "prof_backtrans_bertscore": 0.882240355014801,
    "prof_backtrans_labse": 0.9256448149681091,
    "prof_backtrans_xlm_roberta": 0.6874673962593079,
    "llm_vs_prof_backtrans_bleu": 36.57845213627423,
    "llm_vs_prof_backtrans_chrf": 65.3505542110722,
    "llm_vs_prof_backtrans_bertscore": 0.8838079571723938,
    "llm_vs_prof_backtrans_labse": 0.9153168797492981
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 48.28105677514045,
    "same_lang_chrf": 76.45289708160547,
    "same_lang_bertscore": 0.8171814680099487,
    "same_lang_comet": 0.8214025497436523,
    "cross_lang_xlm_roberta": 0.931169331073761,
    "cross_lang_labse": 0.8567324876785278,
    "cross_lang_mbert": 0.8987104892730713,
    "cross_lang_comet_qe": 0.16331911087036133,
    "backtrans_bleu": 64.18666102575983,
    "backtrans_chrf": 85.48617976086885,
    "backtrans_bertscore": 0.8669864535331726,
    "prof_backtrans_bleu": 45.20974188627211,
    "prof_backtrans_chrf": 77.78099807058312,
    "prof_backtrans_bertscore": 0.8652228713035583,
    "prof_backtrans_labse": 0.8549461960792542,
    "prof_backtrans_xlm_roberta": 0.9212013483047485,
    "llm_vs_prof_backtrans_bleu": 58.40125767719723,
    "llm_vs_prof_backtrans_chrf": 78.83026538069961,
    "llm_vs_prof_backtrans_bertscore": 0.9453376531600952,
    "llm_vs_prof_backtrans_labse": 0.9303245544433594
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 15.663358071957147,
    "same_lang_chrf": 36.49953332846139,
    "same_lang_bertscore": 0.7605375647544861,
    "same_lang_comet": 0.8248577117919922,
    "cross_lang_xlm_roberta": 0.9214370250701904,
    "cross_lang_labse": 0.817038357257843,
    "cross_lang_mbert": 0.8807103633880615,
    "cross_lang_comet_qe": 0.22211122512817383,
    "backtrans_bleu": 61.29805347922598,
    "backtrans_chrf": 83.29048737825656,
    "backtrans_bertscore": 0.8555845618247986,
    "prof_backtrans_bleu": 40.88210909446735,
    "prof_backtrans_chrf": 73.41673062043932,
    "prof_backtrans_bertscore": 0.8754429817199707,
    "prof_backtrans_labse": 0.8860772848129272,
    "prof_backtrans_xlm_roberta": 0.6424371004104614,
    "llm_vs_prof_backtrans_bleu": 42.99388233253416,
    "llm_vs_prof_backtrans_chrf": 68.57404667008574,
    "llm_vs_prof_backtrans_bertscore": 0.8766303658485413,
    "llm_vs_prof_backtrans_labse": 0.7256197333335876
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 44.21655383220208,
    "same_lang_chrf": 63.424856127090976,
    "same_lang_bertscore": 0.7876785397529602,
    "same_lang_comet": 0.8334784507751465,
    "cross_lang_xlm_roberta": 0.9194726347923279,
    "cross_lang_labse": 0.8129461407661438,
    "cross_lang_mbert": 0.891551673412323,
    "cross_lang_comet_qe": 0.22744354605674744,
    "backtrans_bleu": 66.274376215159,
    "backtrans_chrf": 88.39191091756115,
    "backtrans_bertscore": 0.8632825613021851,
    "prof_backtrans_bleu": 38.021822184484506,
    "prof_backtrans_chrf": 72.94272446997412,
    "prof_backtrans_bertscore": 0.8734045028686523,
    "prof_backtrans_labse": 0.8517122268676758,
    "prof_backtrans_xlm_roberta": 0.6183128952980042,
    "llm_vs_prof_backtrans_bleu": 38.37614832010298,
    "llm_vs_prof_backtrans_chrf": 67.3355905065621,
    "llm_vs_prof_backtrans_bertscore": 0.8443747758865356,
    "llm_vs_prof_backtrans_labse": 0.7891572713851929
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 47.10743777961217,
    "same_lang_chrf": 75.13805268924077,
    "same_lang_bertscore": 0.7872883677482605,
    "same_lang_comet": 0.8319340944290161,
    "cross_lang_xlm_roberta": 0.9208506345748901,
    "cross_lang_labse": 0.801590621471405,
    "cross_lang_mbert": 0.89063960313797,
    "cross_lang_comet_qe": 0.2430417537689209,
    "backtrans_bleu": 60.83943820932771,
    "backtrans_chrf": 85.45011406122576,
    "backtrans_bertscore": 0.8552943468093872,
    "prof_backtrans_bleu": 63.45155227083934,
    "prof_backtrans_chrf": 85.11873240105766,
    "prof_backtrans_bertscore": 0.8615942001342773,
    "prof_backtrans_labse": 0.8239051699638367,
    "prof_backtrans_xlm_roberta": 0.7960517406463623,
    "llm_vs_prof_backtrans_bleu": 68.20092661517245,
    "llm_vs_prof_backtrans_chrf": 86.12165478912182,
    "llm_vs_prof_backtrans_bertscore": 0.9406430721282959,
    "llm_vs_prof_backtrans_labse": 0.9378843307495117
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 26.99439595944455,
    "same_lang_chrf": 62.77393529562112,
    "same_lang_bertscore": 0.7797695994377136,
    "same_lang_comet": 0.8507671356201172,
    "cross_lang_xlm_roberta": 0.912407398223877,
    "cross_lang_labse": 0.824763298034668,
    "cross_lang_mbert": 0.8567323684692383,
    "cross_lang_comet_qe": 0.1650909185409546,
    "backtrans_bleu": 54.602682882916525,
    "backtrans_chrf": 83.7794325809132,
    "backtrans_bertscore": 0.8675588369369507,
    "prof_backtrans_bleu": 24.231173084842776,
    "prof_backtrans_chrf": 64.91730051137526,
    "prof_backtrans_bertscore": 0.8605622053146362,
    "prof_backtrans_labse": 0.868340790271759,
    "prof_backtrans_xlm_roberta": 0.6384285092353821,
    "llm_vs_prof_backtrans_bleu": 26.14732122851599,
    "llm_vs_prof_backtrans_chrf": 56.91650695908744,
    "llm_vs_prof_backtrans_bertscore": 0.8358083963394165,
    "llm_vs_prof_backtrans_labse": 0.7846639752388
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 9.022344953493118,
    "same_lang_chrf": 40.28766613340857,
    "same_lang_bertscore": 0.715532660484314,
    "same_lang_comet": 0.7849768996238708,
    "cross_lang_xlm_roberta": 0.9080592393875122,
    "cross_lang_labse": 0.8539538383483887,
    "cross_lang_mbert": 0.8844339847564697,
    "cross_lang_comet_qe": 0.2238505631685257,
    "backtrans_bleu": 63.89070519758886,
    "backtrans_chrf": 83.7123186352817,
    "backtrans_bertscore": 0.8610807657241821,
    "prof_backtrans_bleu": 13.892523224043295,
    "prof_backtrans_chrf": 58.10501431877482,
    "prof_backtrans_bertscore": 0.8286694884300232,
    "prof_backtrans_labse": 0.8407056331634521,
    "prof_backtrans_xlm_roberta": 0.6199514269828796,
    "llm_vs_prof_backtrans_bleu": 13.981522020452855,
    "llm_vs_prof_backtrans_chrf": 56.26902550052834,
    "llm_vs_prof_backtrans_bertscore": 0.813241183757782,
    "llm_vs_prof_backtrans_labse": 0.7448157668113708
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 30.30252554163875,
    "same_lang_chrf": 60.29002344402253,
    "same_lang_bertscore": 0.7893656492233276,
    "same_lang_comet": 0.7742010354995728,
    "cross_lang_xlm_roberta": 0.9294357299804688,
    "cross_lang_labse": 0.8457335829734802,
    "cross_lang_mbert": 0.9123530387878418,
    "cross_lang_comet_qe": 0.16676576435565948,
    "backtrans_bleu": 59.51266226598193,
    "backtrans_chrf": 82.3027094669344,
    "backtrans_bertscore": 0.8707237839698792,
    "prof_backtrans_bleu": 30.66538685282064,
    "prof_backtrans_chrf": 69.0538341318755,
    "prof_backtrans_bertscore": 0.8510808944702148,
    "prof_backtrans_labse": 0.8616167306900024,
    "prof_backtrans_xlm_roberta": 0.6829803586006165,
    "llm_vs_prof_backtrans_bleu": 33.57386194199581,
    "llm_vs_prof_backtrans_chrf": 65.61064158523733,
    "llm_vs_prof_backtrans_bertscore": 0.9004628658294678,
    "llm_vs_prof_backtrans_labse": 0.8421230316162109
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 48.664160519869235,
    "same_lang_chrf": 75.59812536071522,
    "same_lang_bertscore": 0.8156079649925232,
    "same_lang_comet": 0.8181328773498535,
    "cross_lang_xlm_roberta": 0.9312620162963867,
    "cross_lang_labse": 0.843273937702179,
    "cross_lang_mbert": 0.9097126126289368,
    "cross_lang_comet_qe": 0.16413776576519012,
    "backtrans_bleu": 64.46965612876716,
    "backtrans_chrf": 83.59753703657974,
    "backtrans_bertscore": 0.872309148311615,
    "prof_backtrans_bleu": 47.12611180607801,
    "prof_backtrans_chrf": 77.38567063609815,
    "prof_backtrans_bertscore": 0.868746817111969,
    "prof_backtrans_labse": 0.8963900208473206,
    "prof_backtrans_xlm_roberta": 0.8959223628044128,
    "llm_vs_prof_backtrans_bleu": 63.09946825037253,
    "llm_vs_prof_backtrans_chrf": 79.42604229779097,
    "llm_vs_prof_backtrans_bertscore": 0.9517667293548584,
    "llm_vs_prof_backtrans_labse": 0.9318719506263733
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 6.784635578791777,
    "same_lang_chrf": 37.43692942988155,
    "same_lang_bertscore": 0.7799612283706665,
    "same_lang_comet": 0.8259936571121216,
    "cross_lang_xlm_roberta": 0.8298628926277161,
    "cross_lang_labse": 0.8319680690765381,
    "cross_lang_mbert": 0.8495941758155823,
    "cross_lang_comet_qe": 0.10846525430679321,
    "backtrans_bleu": 53.649582411925905,
    "backtrans_chrf": 79.81591660615969,
    "backtrans_bertscore": 0.8705071210861206,
    "prof_backtrans_bleu": 28.397438211043394,
    "prof_backtrans_chrf": 67.24711373458513,
    "prof_backtrans_bertscore": 0.8560276031494141,
    "prof_backtrans_labse": 0.8782057762145996,
    "prof_backtrans_xlm_roberta": 0.6287891268730164,
    "llm_vs_prof_backtrans_bleu": 40.91606444840802,
    "llm_vs_prof_backtrans_chrf": 70.27211682166798,
    "llm_vs_prof_backtrans_bertscore": 0.8364924192428589,
    "llm_vs_prof_backtrans_labse": 0.7057948112487793
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 38.39179195688009,
    "same_lang_chrf": 58.32252299743025,
    "same_lang_bertscore": 0.7734048366546631,
    "same_lang_comet": 0.8211671710014343,
    "cross_lang_xlm_roberta": 0.927010715007782,
    "cross_lang_labse": 0.8608484268188477,
    "cross_lang_mbert": 0.8749467730522156,
    "cross_lang_comet_qe": 0.15583433210849762,
    "backtrans_bleu": 58.51045462197961,
    "backtrans_chrf": 81.66509109118482,
    "backtrans_bertscore": 0.8755213618278503,
    "prof_backtrans_bleu": 50.518956866800636,
    "prof_backtrans_chrf": 77.27606490570423,
    "prof_backtrans_bertscore": 0.8570650815963745,
    "prof_backtrans_labse": 0.8372527956962585,
    "prof_backtrans_xlm_roberta": 0.6696820259094238,
    "llm_vs_prof_backtrans_bleu": 62.82142927955358,
    "llm_vs_prof_backtrans_chrf": 80.93279845440206,
    "llm_vs_prof_backtrans_bertscore": 0.9171909689903259,
    "llm_vs_prof_backtrans_labse": 0.9249799847602844
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 44.490245533968164,
    "same_lang_chrf": 71.83804289923384,
    "same_lang_bertscore": 0.7998062372207642,
    "same_lang_comet": 0.8283935189247131,
    "cross_lang_xlm_roberta": 0.9133445024490356,
    "cross_lang_labse": 0.8520021438598633,
    "cross_lang_mbert": 0.8970831036567688,
    "cross_lang_comet_qe": 0.18037408590316772,
    "backtrans_bleu": 60.43007345408173,
    "backtrans_chrf": 82.82140807975019,
    "backtrans_bertscore": 0.8685488104820251,
    "prof_backtrans_bleu": 57.23341416982659,
    "prof_backtrans_chrf": 82.19917330765072,
    "prof_backtrans_bertscore": 0.8747392296791077,
    "prof_backtrans_labse": 0.8232169151306152,
    "prof_backtrans_xlm_roberta": 0.7333045601844788,
    "llm_vs_prof_backtrans_bleu": 61.395301291611325,
    "llm_vs_prof_backtrans_chrf": 82.19480833818614,
    "llm_vs_prof_backtrans_bertscore": 0.8614786863327026,
    "llm_vs_prof_backtrans_labse": 0.8118007183074951
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 23.98344255039852,
    "same_lang_chrf": 59.26868519897257,
    "same_lang_bertscore": 0.7739705443382263,
    "same_lang_comet": 0.8488730788230896,
    "cross_lang_xlm_roberta": 0.9216111302375793,
    "cross_lang_labse": 0.8597332239151001,
    "cross_lang_mbert": 0.8662430644035339,
    "cross_lang_comet_qe": 0.15699270367622375,
    "backtrans_bleu": 39.83726308781849,
    "backtrans_chrf": 71.0399130387976,
    "backtrans_bertscore": 0.8711826205253601,
    "prof_backtrans_bleu": 28.140132169771896,
    "prof_backtrans_chrf": 63.789309846028985,
    "prof_backtrans_bertscore": 0.8569695353507996,
    "prof_backtrans_labse": 0.8669406771659851,
    "prof_backtrans_xlm_roberta": 0.6329866647720337,
    "llm_vs_prof_backtrans_bleu": 32.768444079790285,
    "llm_vs_prof_backtrans_chrf": 64.51352305578605,
    "llm_vs_prof_backtrans_bertscore": 0.8408582806587219,
    "llm_vs_prof_backtrans_labse": 0.8159366250038147
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 10.676553751705079,
    "same_lang_chrf": 38.77419484639521,
    "same_lang_bertscore": 0.7318152189254761,
    "same_lang_comet": 0.7878162860870361,
    "cross_lang_xlm_roberta": 0.9435192942619324,
    "cross_lang_labse": 0.8606154918670654,
    "cross_lang_mbert": 0.8843886852264404,
    "cross_lang_comet_qe": 0.13317281007766724,
    "backtrans_bleu": 44.15621950544394,
    "backtrans_chrf": 75.74488709200644,
    "backtrans_bertscore": 0.8752622604370117,
    "prof_backtrans_bleu": 32.19977423395364,
    "prof_backtrans_chrf": 68.55728452848494,
    "prof_backtrans_bertscore": 0.8707126379013062,
    "prof_backtrans_labse": 0.8664281368255615,
    "prof_backtrans_xlm_roberta": 0.6234229207038879,
    "llm_vs_prof_backtrans_bleu": 40.2027746804617,
    "llm_vs_prof_backtrans_chrf": 73.86899593677614,
    "llm_vs_prof_backtrans_bertscore": 0.8522516489028931,
    "llm_vs_prof_backtrans_labse": 0.7402969002723694
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 25.5162927041177,
    "same_lang_chrf": 54.723484875454396,
    "same_lang_bertscore": 0.7678122520446777,
    "same_lang_comet": 0.7494990825653076,
    "cross_lang_xlm_roberta": 0.9156817197799683,
    "cross_lang_labse": 0.8506500720977783,
    "cross_lang_mbert": 0.8953738212585449,
    "cross_lang_comet_qe": 0.0632181242108345,
    "backtrans_bleu": 50.62946551139101,
    "backtrans_chrf": 77.69032367063033,
    "backtrans_bertscore": 0.8658420443534851,
    "prof_backtrans_bleu": 32.67654165390782,
    "prof_backtrans_chrf": 63.439917614127104,
    "prof_backtrans_bertscore": 0.8496212959289551,
    "prof_backtrans_labse": 0.8972901105880737,
    "prof_backtrans_xlm_roberta": 0.6867298483848572,
    "llm_vs_prof_backtrans_bleu": 38.6667655377196,
    "llm_vs_prof_backtrans_chrf": 67.05585213697792,
    "llm_vs_prof_backtrans_bertscore": 0.8916699290275574,
    "llm_vs_prof_backtrans_labse": 0.8640933036804199
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 49.88924659134645,
    "same_lang_chrf": 75.73573641412219,
    "same_lang_bertscore": 0.8012788891792297,
    "same_lang_comet": 0.7943054437637329,
    "cross_lang_xlm_roberta": 0.9364974498748779,
    "cross_lang_labse": 0.8941047787666321,
    "cross_lang_mbert": 0.8440717458724976,
    "cross_lang_comet_qe": 0.09779489785432816,
    "backtrans_bleu": 62.573398767003134,
    "backtrans_chrf": 82.15303318757613,
    "backtrans_bertscore": 0.8515017628669739,
    "prof_backtrans_bleu": 53.86509535151934,
    "prof_backtrans_chrf": 79.4791958897056,
    "prof_backtrans_bertscore": 0.8565829992294312,
    "prof_backtrans_labse": 0.897623598575592,
    "prof_backtrans_xlm_roberta": 0.9496527314186096,
    "llm_vs_prof_backtrans_bleu": 64.12978171352583,
    "llm_vs_prof_backtrans_chrf": 81.10162138068283,
    "llm_vs_prof_backtrans_bertscore": 0.9150978326797485,
    "llm_vs_prof_backtrans_labse": 0.9479581117630005
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 11.754733340847867,
    "same_lang_chrf": 34.994173739105186,
    "same_lang_bertscore": 0.7689257860183716,
    "same_lang_comet": 0.825402557849884,
    "cross_lang_xlm_roberta": 0.9777119159698486,
    "cross_lang_labse": 0.8797505497932434,
    "cross_lang_mbert": 0.9154388308525085,
    "cross_lang_comet_qe": 0.1074167937040329,
    "backtrans_bleu": 68.98069950837215,
    "backtrans_chrf": 85.9020219175101,
    "backtrans_bertscore": 0.868353009223938,
    "prof_backtrans_bleu": 51.65089874768896,
    "prof_backtrans_chrf": 79.19458774712189,
    "prof_backtrans_bertscore": 0.8692540526390076,
    "prof_backtrans_labse": 0.9130020141601562,
    "prof_backtrans_xlm_roberta": 0.7255380749702454,
    "llm_vs_prof_backtrans_bleu": 57.926151576456114,
    "llm_vs_prof_backtrans_chrf": 76.13700025484479,
    "llm_vs_prof_backtrans_bertscore": 0.8827248215675354,
    "llm_vs_prof_backtrans_labse": 0.8143068552017212
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 39.9342156148034,
    "same_lang_chrf": 57.82303921154078,
    "same_lang_bertscore": 0.7866103649139404,
    "same_lang_comet": 0.8291919231414795,
    "cross_lang_xlm_roberta": 0.9646021127700806,
    "cross_lang_labse": 0.8979945182800293,
    "cross_lang_mbert": 0.8953185677528381,
    "cross_lang_comet_qe": 0.008115485310554504,
    "backtrans_bleu": 62.4938206555972,
    "backtrans_chrf": 82.30442964445558,
    "backtrans_bertscore": 0.8861039280891418,
    "prof_backtrans_bleu": 55.89205970673254,
    "prof_backtrans_chrf": 81.14932968190921,
    "prof_backtrans_bertscore": 0.8901324272155762,
    "prof_backtrans_labse": 0.9107612371444702,
    "prof_backtrans_xlm_roberta": 0.6868468523025513,
    "llm_vs_prof_backtrans_bleu": 54.286977655927004,
    "llm_vs_prof_backtrans_chrf": 76.27590793899213,
    "llm_vs_prof_backtrans_bertscore": 0.8600443005561829,
    "llm_vs_prof_backtrans_labse": 0.7895570397377014
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 32.211563307078116,
    "same_lang_chrf": 65.00631858889919,
    "same_lang_bertscore": 0.7847752571105957,
    "same_lang_comet": 0.826480507850647,
    "cross_lang_xlm_roberta": 0.9773707985877991,
    "cross_lang_labse": 0.9924017190933228,
    "cross_lang_mbert": 0.9541385173797607,
    "cross_lang_comet_qe": -0.20489606261253357,
    "backtrans_bleu": 64.81522623182047,
    "backtrans_chrf": 83.71762994615251,
    "backtrans_bertscore": 0.9021033048629761,
    "prof_backtrans_bleu": 59.881489892700266,
    "prof_backtrans_chrf": 82.98541206734154,
    "prof_backtrans_bertscore": 0.8844306468963623,
    "prof_backtrans_labse": 0.9127773642539978,
    "prof_backtrans_xlm_roberta": 0.8431838750839233,
    "llm_vs_prof_backtrans_bleu": 59.30030935603004,
    "llm_vs_prof_backtrans_chrf": 79.98374583723373,
    "llm_vs_prof_backtrans_bertscore": 0.8794858455657959,
    "llm_vs_prof_backtrans_labse": 0.912944495677948
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 31.08830547472444,
    "same_lang_chrf": 61.418780234131795,
    "same_lang_bertscore": 0.7826271653175354,
    "same_lang_comet": 0.8202453851699829,
    "cross_lang_xlm_roberta": 0.9792324900627136,
    "cross_lang_labse": 0.896468460559845,
    "cross_lang_mbert": 0.8877906203269958,
    "cross_lang_comet_qe": 0.01924656331539154,
    "backtrans_bleu": 57.19821962684155,
    "backtrans_chrf": 80.36947794784248,
    "backtrans_bertscore": 0.8690388202667236,
    "prof_backtrans_bleu": 40.923985615382314,
    "prof_backtrans_chrf": 74.13718674626915,
    "prof_backtrans_bertscore": 0.8823525309562683,
    "prof_backtrans_labse": 0.9036243557929993,
    "prof_backtrans_xlm_roberta": 0.7492551803588867,
    "llm_vs_prof_backtrans_bleu": 46.80641975655721,
    "llm_vs_prof_backtrans_chrf": 71.20966356392961,
    "llm_vs_prof_backtrans_bertscore": 0.8477078080177307,
    "llm_vs_prof_backtrans_labse": 0.8477487564086914
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 26.724348808990715,
    "same_lang_chrf": 45.090669881050374,
    "same_lang_bertscore": 0.7933300137519836,
    "same_lang_comet": 0.8634209632873535,
    "cross_lang_xlm_roberta": 0.96864914894104,
    "cross_lang_labse": 0.8949003219604492,
    "cross_lang_mbert": 0.9260953068733215,
    "cross_lang_comet_qe": 0.05424961447715759,
    "backtrans_bleu": 57.552146555870486,
    "backtrans_chrf": 79.96183758594542,
    "backtrans_bertscore": 0.8667359948158264,
    "prof_backtrans_bleu": 42.31060760263043,
    "prof_backtrans_chrf": 74.20924540204172,
    "prof_backtrans_bertscore": 0.8679013252258301,
    "prof_backtrans_labse": 0.8992491960525513,
    "prof_backtrans_xlm_roberta": 0.7570332288742065,
    "llm_vs_prof_backtrans_bleu": 50.83333650930799,
    "llm_vs_prof_backtrans_chrf": 74.51322514917513,
    "llm_vs_prof_backtrans_bertscore": 0.9027727246284485,
    "llm_vs_prof_backtrans_labse": 0.8811832666397095
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 26.99654144518125,
    "same_lang_chrf": 56.30354047828705,
    "same_lang_bertscore": 0.7827919125556946,
    "same_lang_comet": 0.791972279548645,
    "cross_lang_xlm_roberta": 0.8650625944137573,
    "cross_lang_labse": 0.8939884901046753,
    "cross_lang_mbert": 0.8723230957984924,
    "cross_lang_comet_qe": 0.011971011757850647,
    "backtrans_bleu": 58.06736535898381,
    "backtrans_chrf": 79.8037505214396,
    "backtrans_bertscore": 0.8560615181922913,
    "prof_backtrans_bleu": 42.38318963688328,
    "prof_backtrans_chrf": 73.62991124053023,
    "prof_backtrans_bertscore": 0.880476713180542,
    "prof_backtrans_labse": 0.9024685621261597,
    "prof_backtrans_xlm_roberta": 0.737993061542511,
    "llm_vs_prof_backtrans_bleu": 44.629781480533076,
    "llm_vs_prof_backtrans_chrf": 70.04423844177823,
    "llm_vs_prof_backtrans_bertscore": 0.8496608138084412,
    "llm_vs_prof_backtrans_labse": 0.842155396938324
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 44.24625336400099,
    "same_lang_chrf": 74.40217320227535,
    "same_lang_bertscore": 0.8504578471183777,
    "same_lang_comet": 0.8087459206581116,
    "cross_lang_xlm_roberta": 0.9812023639678955,
    "cross_lang_labse": 0.9911002516746521,
    "cross_lang_mbert": 0.9154478311538696,
    "cross_lang_comet_qe": -0.19116747379302979,
    "backtrans_bleu": 61.96406767406395,
    "backtrans_chrf": 83.90811263345567,
    "backtrans_bertscore": 0.9081611633300781,
    "prof_backtrans_bleu": 46.70033885954262,
    "prof_backtrans_chrf": 76.74010741530924,
    "prof_backtrans_bertscore": 0.8850273489952087,
    "prof_backtrans_labse": 0.9429569840431213,
    "prof_backtrans_xlm_roberta": 0.9252718091011047,
    "llm_vs_prof_backtrans_bleu": 51.65649231946295,
    "llm_vs_prof_backtrans_chrf": 77.51164245565005,
    "llm_vs_prof_backtrans_bertscore": 0.891211986541748,
    "llm_vs_prof_backtrans_labse": 0.9355400800704956
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.347646481981966,
    "same_lang_chrf": 36.088204685245096,
    "same_lang_bertscore": 0.7943810224533081,
    "same_lang_comet": 0.813448429107666,
    "cross_lang_xlm_roberta": 0.9827560186386108,
    "cross_lang_labse": 0.9902054667472839,
    "cross_lang_mbert": 0.9216482043266296,
    "cross_lang_comet_qe": -0.16214074194431305,
    "backtrans_bleu": 68.32345425882907,
    "backtrans_chrf": 85.54186922151212,
    "backtrans_bertscore": 0.9145231246948242,
    "prof_backtrans_bleu": 40.25323635094634,
    "prof_backtrans_chrf": 73.90216172157508,
    "prof_backtrans_bertscore": 0.8701547980308533,
    "prof_backtrans_labse": 0.9210962653160095,
    "prof_backtrans_xlm_roberta": 0.7192999720573425,
    "llm_vs_prof_backtrans_bleu": 42.57644069804471,
    "llm_vs_prof_backtrans_chrf": 70.26439957748309,
    "llm_vs_prof_backtrans_bertscore": 0.8755267858505249,
    "llm_vs_prof_backtrans_labse": 0.9209335446357727
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 43.581879870773236,
    "same_lang_chrf": 63.30254760045059,
    "same_lang_bertscore": 0.833552360534668,
    "same_lang_comet": 0.8266545534133911,
    "cross_lang_xlm_roberta": 0.9909253120422363,
    "cross_lang_labse": 0.9925073981285095,
    "cross_lang_mbert": 0.9765625596046448,
    "cross_lang_comet_qe": -0.14736123383045197,
    "backtrans_bleu": 64.06027828479692,
    "backtrans_chrf": 83.95182547225254,
    "backtrans_bertscore": 0.9216569066047668,
    "prof_backtrans_bleu": 50.62148991182,
    "prof_backtrans_chrf": 77.9574684781243,
    "prof_backtrans_bertscore": 0.8870760798454285,
    "prof_backtrans_labse": 0.9197995066642761,
    "prof_backtrans_xlm_roberta": 0.6871708035469055,
    "llm_vs_prof_backtrans_bleu": 53.2123945738081,
    "llm_vs_prof_backtrans_chrf": 77.30079619659213,
    "llm_vs_prof_backtrans_bertscore": 0.8853265047073364,
    "llm_vs_prof_backtrans_labse": 0.9104002118110657
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 37.2384627558762,
    "same_lang_chrf": 72.67108111012061,
    "same_lang_bertscore": 0.8141212463378906,
    "same_lang_comet": 0.8248482942581177,
    "cross_lang_xlm_roberta": 0.9971643686294556,
    "cross_lang_labse": 0.9961376786231995,
    "cross_lang_mbert": 0.9904881119728088,
    "cross_lang_comet_qe": -0.18693183362483978,
    "backtrans_bleu": 65.59611730248763,
    "backtrans_chrf": 85.98291540893969,
    "backtrans_bertscore": 0.936002254486084,
    "prof_backtrans_bleu": 53.00495665641608,
    "prof_backtrans_chrf": 79.77694665317294,
    "prof_backtrans_bertscore": 0.8847383856773376,
    "prof_backtrans_labse": 0.9176148772239685,
    "prof_backtrans_xlm_roberta": 0.8132210373878479,
    "llm_vs_prof_backtrans_bleu": 54.662782453126646,
    "llm_vs_prof_backtrans_chrf": 77.94907642440096,
    "llm_vs_prof_backtrans_bertscore": 0.886820375919342,
    "llm_vs_prof_backtrans_labse": 0.9156956076622009
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 30.21286779888157,
    "same_lang_chrf": 63.30064176182913,
    "same_lang_bertscore": 0.8270841836929321,
    "same_lang_comet": 0.83265620470047,
    "cross_lang_xlm_roberta": 0.9856936931610107,
    "cross_lang_labse": 0.9893721342086792,
    "cross_lang_mbert": 0.9327079057693481,
    "cross_lang_comet_qe": 0.07297040522098541,
    "backtrans_bleu": 71.39631447099318,
    "backtrans_chrf": 87.48037277182694,
    "backtrans_bertscore": 0.9095534682273865,
    "prof_backtrans_bleu": 36.07694730102524,
    "prof_backtrans_chrf": 72.04232771329457,
    "prof_backtrans_bertscore": 0.8791907429695129,
    "prof_backtrans_labse": 0.905752956867218,
    "prof_backtrans_xlm_roberta": 0.7430227994918823,
    "llm_vs_prof_backtrans_bleu": 40.94806282414875,
    "llm_vs_prof_backtrans_chrf": 68.1678424350526,
    "llm_vs_prof_backtrans_bertscore": 0.8663376569747925,
    "llm_vs_prof_backtrans_labse": 0.9064500331878662
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 26.209778517891092,
    "same_lang_chrf": 43.77367946387011,
    "same_lang_bertscore": 0.8205658793449402,
    "same_lang_comet": 0.8787970542907715,
    "cross_lang_xlm_roberta": 0.9590702652931213,
    "cross_lang_labse": 0.9758254885673523,
    "cross_lang_mbert": 0.9201889634132385,
    "cross_lang_comet_qe": 0.11921761929988861,
    "backtrans_bleu": 68.11032925729322,
    "backtrans_chrf": 85.14845266213989,
    "backtrans_bertscore": 0.893336296081543,
    "prof_backtrans_bleu": 47.05565724658802,
    "prof_backtrans_chrf": 76.28339563323513,
    "prof_backtrans_bertscore": 0.8830704689025879,
    "prof_backtrans_labse": 0.909390389919281,
    "prof_backtrans_xlm_roberta": 0.7634440064430237,
    "llm_vs_prof_backtrans_bleu": 50.55853706792861,
    "llm_vs_prof_backtrans_chrf": 74.08779900331105,
    "llm_vs_prof_backtrans_bertscore": 0.9037619233131409,
    "llm_vs_prof_backtrans_labse": 0.9072766900062561
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 25.03330098351363,
    "same_lang_chrf": 57.13536873271655,
    "same_lang_bertscore": 0.8178572058677673,
    "same_lang_comet": 0.8116048574447632,
    "cross_lang_xlm_roberta": 0.9835647344589233,
    "cross_lang_labse": 0.9931134581565857,
    "cross_lang_mbert": 0.9313936233520508,
    "cross_lang_comet_qe": -0.16765731573104858,
    "backtrans_bleu": 68.88333392354897,
    "backtrans_chrf": 84.9744186959552,
    "backtrans_bertscore": 0.9230366349220276,
    "prof_backtrans_bleu": 44.67925178352583,
    "prof_backtrans_chrf": 74.71485926458813,
    "prof_backtrans_bertscore": 0.8815349340438843,
    "prof_backtrans_labse": 0.9129603505134583,
    "prof_backtrans_xlm_roberta": 0.7643045783042908,
    "llm_vs_prof_backtrans_bleu": 47.47820289946989,
    "llm_vs_prof_backtrans_chrf": 74.0656461770613,
    "llm_vs_prof_backtrans_bertscore": 0.8832760453224182,
    "llm_vs_prof_backtrans_labse": 0.9112073183059692
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 47.77803398610293,
    "same_lang_chrf": 75.31715702515446,
    "same_lang_bertscore": 0.7982573509216309,
    "same_lang_comet": 0.7951086759567261,
    "cross_lang_xlm_roberta": 0.9295017719268799,
    "cross_lang_labse": 0.8480745553970337,
    "cross_lang_mbert": 0.8473758697509766,
    "cross_lang_comet_qe": 0.10169383138418198,
    "backtrans_bleu": 64.87611395089255,
    "backtrans_chrf": 84.11444901654556,
    "backtrans_bertscore": 0.8590162396430969,
    "prof_backtrans_bleu": 47.73860565467447,
    "prof_backtrans_chrf": 77.90006867263224,
    "prof_backtrans_bertscore": 0.866296648979187,
    "prof_backtrans_labse": 0.9049142599105835,
    "prof_backtrans_xlm_roberta": 0.959650993347168,
    "llm_vs_prof_backtrans_bleu": 60.88704068314412,
    "llm_vs_prof_backtrans_chrf": 79.04980987552041,
    "llm_vs_prof_backtrans_bertscore": 0.9302788376808167,
    "llm_vs_prof_backtrans_labse": 0.9139589071273804
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 16.921492632312624,
    "same_lang_chrf": 37.90003199756135,
    "same_lang_bertscore": 0.7635958790779114,
    "same_lang_comet": 0.8283286094665527,
    "cross_lang_xlm_roberta": 0.95098876953125,
    "cross_lang_labse": 0.851365864276886,
    "cross_lang_mbert": 0.8383862376213074,
    "cross_lang_comet_qe": 0.11657784879207611,
    "backtrans_bleu": 60.88616762375377,
    "backtrans_chrf": 85.71047243039247,
    "backtrans_bertscore": 0.8528201580047607,
    "prof_backtrans_bleu": 32.63819442667098,
    "prof_backtrans_chrf": 69.97726863722052,
    "prof_backtrans_bertscore": 0.8621664643287659,
    "prof_backtrans_labse": 0.9127206802368164,
    "prof_backtrans_xlm_roberta": 0.7210360765457153,
    "llm_vs_prof_backtrans_bleu": 32.30923310836557,
    "llm_vs_prof_backtrans_chrf": 63.81323579786458,
    "llm_vs_prof_backtrans_bertscore": 0.8399019241333008,
    "llm_vs_prof_backtrans_labse": 0.8012593984603882
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 46.49985330079883,
    "same_lang_chrf": 64.47676422810996,
    "same_lang_bertscore": 0.7829336524009705,
    "same_lang_comet": 0.8376184701919556,
    "cross_lang_xlm_roberta": 0.944139838218689,
    "cross_lang_labse": 0.8407639861106873,
    "cross_lang_mbert": 0.849830150604248,
    "cross_lang_comet_qe": 0.10779999196529388,
    "backtrans_bleu": 75.04191589839508,
    "backtrans_chrf": 89.72187319126375,
    "backtrans_bertscore": 0.8619208335876465,
    "prof_backtrans_bleu": 34.768739181099896,
    "prof_backtrans_chrf": 69.93020340065397,
    "prof_backtrans_bertscore": 0.8770768046379089,
    "prof_backtrans_labse": 0.9149373769760132,
    "prof_backtrans_xlm_roberta": 0.688829779624939,
    "llm_vs_prof_backtrans_bleu": 35.986732645251166,
    "llm_vs_prof_backtrans_chrf": 64.48264896577234,
    "llm_vs_prof_backtrans_bertscore": 0.8480892777442932,
    "llm_vs_prof_backtrans_labse": 0.7651353478431702
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 39.298725964669664,
    "same_lang_chrf": 73.32029731351793,
    "same_lang_bertscore": 0.7789570093154907,
    "same_lang_comet": 0.8084886074066162,
    "cross_lang_xlm_roberta": 0.9637359976768494,
    "cross_lang_labse": 0.867010235786438,
    "cross_lang_mbert": 0.9120064973831177,
    "cross_lang_comet_qe": 0.1434689164161682,
    "backtrans_bleu": 68.53313672685661,
    "backtrans_chrf": 87.0112064593305,
    "backtrans_bertscore": 0.8654409646987915,
    "prof_backtrans_bleu": 53.51076706398107,
    "prof_backtrans_chrf": 79.03128262012011,
    "prof_backtrans_bertscore": 0.8541737198829651,
    "prof_backtrans_labse": 0.885860025882721,
    "prof_backtrans_xlm_roberta": 0.7978602647781372,
    "llm_vs_prof_backtrans_bleu": 62.1125173214175,
    "llm_vs_prof_backtrans_chrf": 79.28829707556221,
    "llm_vs_prof_backtrans_bertscore": 0.9131203293800354,
    "llm_vs_prof_backtrans_labse": 0.892181396484375
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 32.05385072492287,
    "same_lang_chrf": 62.997986629119886,
    "same_lang_bertscore": 0.7606168985366821,
    "same_lang_comet": 0.8239610195159912,
    "cross_lang_xlm_roberta": 0.9519931674003601,
    "cross_lang_labse": 0.8553521633148193,
    "cross_lang_mbert": 0.848029613494873,
    "cross_lang_comet_qe": 0.14044791460037231,
    "backtrans_bleu": 58.89781274031645,
    "backtrans_chrf": 82.19507436313015,
    "backtrans_bertscore": 0.8552818298339844,
    "prof_backtrans_bleu": 33.70720796719254,
    "prof_backtrans_chrf": 68.92557136644672,
    "prof_backtrans_bertscore": 0.8509793877601624,
    "prof_backtrans_labse": 0.8142406344413757,
    "prof_backtrans_xlm_roberta": 0.7473722696304321,
    "llm_vs_prof_backtrans_bleu": 44.64859592997284,
    "llm_vs_prof_backtrans_chrf": 66.30216368605191,
    "llm_vs_prof_backtrans_bertscore": 0.9194839000701904,
    "llm_vs_prof_backtrans_labse": 0.8991188406944275
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 24.829818867399506,
    "same_lang_chrf": 45.91776270659303,
    "same_lang_bertscore": 0.7856173515319824,
    "same_lang_comet": 0.8635098934173584,
    "cross_lang_xlm_roberta": 0.9550556540489197,
    "cross_lang_labse": 0.8538482189178467,
    "cross_lang_mbert": 0.8792991042137146,
    "cross_lang_comet_qe": 0.08309340476989746,
    "backtrans_bleu": 50.02211202951224,
    "backtrans_chrf": 77.27093398252426,
    "backtrans_bertscore": 0.853107750415802,
    "prof_backtrans_bleu": 39.186664942991044,
    "prof_backtrans_chrf": 72.52102248806041,
    "prof_backtrans_bertscore": 0.8635032773017883,
    "prof_backtrans_labse": 0.8682721257209778,
    "prof_backtrans_xlm_roberta": 0.7634233236312866,
    "llm_vs_prof_backtrans_bleu": 42.57806589071241,
    "llm_vs_prof_backtrans_chrf": 70.52198821498735,
    "llm_vs_prof_backtrans_bertscore": 0.8976029753684998,
    "llm_vs_prof_backtrans_labse": 0.8873177766799927
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 27.218053450983998,
    "same_lang_chrf": 59.19914709449827,
    "same_lang_bertscore": 0.7815136313438416,
    "same_lang_comet": 0.8186569809913635,
    "cross_lang_xlm_roberta": 0.9353783130645752,
    "cross_lang_labse": 0.8363722562789917,
    "cross_lang_mbert": 0.8452121019363403,
    "cross_lang_comet_qe": 0.13168811798095703,
    "backtrans_bleu": 56.57027120879134,
    "backtrans_chrf": 80.97538979652066,
    "backtrans_bertscore": 0.8579214215278625,
    "prof_backtrans_bleu": 36.49206017955571,
    "prof_backtrans_chrf": 70.3777527320712,
    "prof_backtrans_bertscore": 0.8589789867401123,
    "prof_backtrans_labse": 0.8945446610450745,
    "prof_backtrans_xlm_roberta": 0.7579749226570129,
    "llm_vs_prof_backtrans_bleu": 42.13722833173474,
    "llm_vs_prof_backtrans_chrf": 67.44657623037294,
    "llm_vs_prof_backtrans_bertscore": 0.9049666523933411,
    "llm_vs_prof_backtrans_labse": 0.8916969299316406
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 46.3090028579014,
    "same_lang_chrf": 74.3222152914785,
    "same_lang_bertscore": 0.7999644875526428,
    "same_lang_comet": 0.7969940900802612,
    "cross_lang_xlm_roberta": 0.9179539680480957,
    "cross_lang_labse": 0.8810508251190186,
    "cross_lang_mbert": 0.8616352081298828,
    "cross_lang_comet_qe": 0.13255536556243896,
    "backtrans_bleu": 61.23594413352397,
    "backtrans_chrf": 82.4296373162715,
    "backtrans_bertscore": 0.8591994643211365,
    "prof_backtrans_bleu": 41.66429753050573,
    "prof_backtrans_chrf": 75.34723144197002,
    "prof_backtrans_bertscore": 0.8904892802238464,
    "prof_backtrans_labse": 0.9419347643852234,
    "prof_backtrans_xlm_roberta": 0.9377548098564148,
    "llm_vs_prof_backtrans_bleu": 49.08596649725326,
    "llm_vs_prof_backtrans_chrf": 71.90044604436672,
    "llm_vs_prof_backtrans_bertscore": 0.8621960878372192,
    "llm_vs_prof_backtrans_labse": 0.8690462708473206
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 4.7402252912435285,
    "same_lang_chrf": 36.76189789287267,
    "same_lang_bertscore": 0.7669978141784668,
    "same_lang_comet": 0.8212598562240601,
    "cross_lang_xlm_roberta": 0.9306062459945679,
    "cross_lang_labse": 0.8869507908821106,
    "cross_lang_mbert": 0.8663987517356873,
    "cross_lang_comet_qe": 0.15796460211277008,
    "backtrans_bleu": 47.017506693498056,
    "backtrans_chrf": 75.62304116438433,
    "backtrans_bertscore": 0.8556704521179199,
    "prof_backtrans_bleu": 35.10738165949516,
    "prof_backtrans_chrf": 68.3519088301828,
    "prof_backtrans_bertscore": 0.8584127426147461,
    "prof_backtrans_labse": 0.90375816822052,
    "prof_backtrans_xlm_roberta": 0.7241500616073608,
    "llm_vs_prof_backtrans_bleu": 40.79050706553233,
    "llm_vs_prof_backtrans_chrf": 67.76679518682836,
    "llm_vs_prof_backtrans_bertscore": 0.8658573031425476,
    "llm_vs_prof_backtrans_labse": 0.8276204466819763
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 43.33367266096246,
    "same_lang_chrf": 61.498694085375426,
    "same_lang_bertscore": 0.7794739007949829,
    "same_lang_comet": 0.8302255868911743,
    "cross_lang_xlm_roberta": 0.9135969281196594,
    "cross_lang_labse": 0.8942880630493164,
    "cross_lang_mbert": 0.7329210638999939,
    "cross_lang_comet_qe": 0.1368412971496582,
    "backtrans_bleu": 45.20229666496216,
    "backtrans_chrf": 73.74619415197746,
    "backtrans_bertscore": 0.8537576198577881,
    "prof_backtrans_bleu": 36.22008973284819,
    "prof_backtrans_chrf": 67.28211836235741,
    "prof_backtrans_bertscore": 0.8642703294754028,
    "prof_backtrans_labse": 0.8988398909568787,
    "prof_backtrans_xlm_roberta": 0.6851505637168884,
    "llm_vs_prof_backtrans_bleu": 43.58011505498725,
    "llm_vs_prof_backtrans_chrf": 71.67230923393339,
    "llm_vs_prof_backtrans_bertscore": 0.859967052936554,
    "llm_vs_prof_backtrans_labse": 0.8775688409805298
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 36.005618315970054,
    "same_lang_chrf": 66.7567663476421,
    "same_lang_bertscore": 0.7795616388320923,
    "same_lang_comet": 0.8179280757904053,
    "cross_lang_xlm_roberta": 0.9804683923721313,
    "cross_lang_labse": 0.8366577625274658,
    "cross_lang_mbert": 0.8965215086936951,
    "cross_lang_comet_qe": 0.00902758538722992,
    "backtrans_bleu": 60.43015038708866,
    "backtrans_chrf": 82.59216699982306,
    "backtrans_bertscore": 0.869158923625946,
    "prof_backtrans_bleu": 38.95965527465479,
    "prof_backtrans_chrf": 67.31877138315572,
    "prof_backtrans_bertscore": 0.8355085253715515,
    "prof_backtrans_labse": 0.8652490377426147,
    "prof_backtrans_xlm_roberta": 0.809029757976532,
    "llm_vs_prof_backtrans_bleu": 43.20406340660054,
    "llm_vs_prof_backtrans_chrf": 72.88857771967807,
    "llm_vs_prof_backtrans_bertscore": 0.8428975343704224,
    "llm_vs_prof_backtrans_labse": 0.7061769366264343
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 29.822938613564602,
    "same_lang_chrf": 61.4646495082785,
    "same_lang_bertscore": 0.7770906686782837,
    "same_lang_comet": 0.8197638392448425,
    "cross_lang_xlm_roberta": 0.9663816690444946,
    "cross_lang_labse": 0.8810803294181824,
    "cross_lang_mbert": 0.8668785095214844,
    "cross_lang_comet_qe": 0.10057748854160309,
    "backtrans_bleu": 48.09583323303825,
    "backtrans_chrf": 75.14424762438284,
    "backtrans_bertscore": 0.8674687147140503,
    "prof_backtrans_bleu": 36.052027422602414,
    "prof_backtrans_chrf": 68.20670585717419,
    "prof_backtrans_bertscore": 0.8761252760887146,
    "prof_backtrans_labse": 0.8983238339424133,
    "prof_backtrans_xlm_roberta": 0.744448721408844,
    "llm_vs_prof_backtrans_bleu": 41.35460852302288,
    "llm_vs_prof_backtrans_chrf": 65.95159230458876,
    "llm_vs_prof_backtrans_bertscore": 0.8778897523880005,
    "llm_vs_prof_backtrans_labse": 0.8946879506111145
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 27.18381478653543,
    "same_lang_chrf": 46.30600645576172,
    "same_lang_bertscore": 0.8080382943153381,
    "same_lang_comet": 0.8658221960067749,
    "cross_lang_xlm_roberta": 0.979107677936554,
    "cross_lang_labse": 0.8728334903717041,
    "cross_lang_mbert": 0.9029633402824402,
    "cross_lang_comet_qe": 0.13624031841754913,
    "backtrans_bleu": 45.69722065858757,
    "backtrans_chrf": 74.64597806166373,
    "backtrans_bertscore": 0.8631940484046936,
    "prof_backtrans_bleu": 39.88434525282499,
    "prof_backtrans_chrf": 73.10786452487332,
    "prof_backtrans_bertscore": 0.8691098093986511,
    "prof_backtrans_labse": 0.9085375070571899,
    "prof_backtrans_xlm_roberta": 0.758581280708313,
    "llm_vs_prof_backtrans_bleu": 51.09580278500426,
    "llm_vs_prof_backtrans_chrf": 75.91799766885045,
    "llm_vs_prof_backtrans_bertscore": 0.9122966527938843,
    "llm_vs_prof_backtrans_labse": 0.8898429870605469
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 22.348351446960535,
    "same_lang_chrf": 53.33622220882791,
    "same_lang_bertscore": 0.7797307372093201,
    "same_lang_comet": 0.7744028568267822,
    "cross_lang_xlm_roberta": 0.9708451628684998,
    "cross_lang_labse": 0.8857939839363098,
    "cross_lang_mbert": 0.9159516096115112,
    "cross_lang_comet_qe": -0.04741796851158142,
    "backtrans_bleu": 52.966943337174506,
    "backtrans_chrf": 76.90550867870918,
    "backtrans_bertscore": 0.8656324148178101,
    "prof_backtrans_bleu": 33.979325859780495,
    "prof_backtrans_chrf": 68.07733405834306,
    "prof_backtrans_bertscore": 0.8630181550979614,
    "prof_backtrans_labse": 0.9041385054588318,
    "prof_backtrans_xlm_roberta": 0.7402777075767517,
    "llm_vs_prof_backtrans_bleu": 40.302215919231784,
    "llm_vs_prof_backtrans_chrf": 65.24292127443661,
    "llm_vs_prof_backtrans_bertscore": 0.8937573432922363,
    "llm_vs_prof_backtrans_labse": 0.9154241681098938
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 48.39932804225933,
    "same_lang_chrf": 74.21580881597437,
    "same_lang_bertscore": 0.8251650333404541,
    "same_lang_comet": 0.8021961450576782,
    "cross_lang_xlm_roberta": 0.9864015579223633,
    "cross_lang_labse": 0.9912424683570862,
    "cross_lang_mbert": 0.9443342685699463,
    "cross_lang_comet_qe": -0.06939716637134552,
    "backtrans_bleu": 70.17245411756738,
    "backtrans_chrf": 86.91246786263565,
    "backtrans_bertscore": 0.9251436591148376,
    "prof_backtrans_bleu": 58.49896104224447,
    "prof_backtrans_chrf": 82.69565915984862,
    "prof_backtrans_bertscore": 0.858029842376709,
    "prof_backtrans_labse": 0.8629913926124573,
    "prof_backtrans_xlm_roberta": 0.9065971374511719,
    "llm_vs_prof_backtrans_bleu": 63.158499610615515,
    "llm_vs_prof_backtrans_chrf": 82.63473758606555,
    "llm_vs_prof_backtrans_bertscore": 0.8572577238082886,
    "llm_vs_prof_backtrans_labse": 0.8690763115882874
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.985858646050957,
    "same_lang_chrf": 39.488920551710855,
    "same_lang_bertscore": 0.8284128308296204,
    "same_lang_comet": 0.8816002607345581,
    "cross_lang_xlm_roberta": 0.8924651145935059,
    "cross_lang_labse": 0.8694385290145874,
    "cross_lang_mbert": 0.8242846131324768,
    "cross_lang_comet_qe": 0.18762698769569397,
    "backtrans_bleu": 68.98792253651982,
    "backtrans_chrf": 86.43439523514033,
    "backtrans_bertscore": 0.86557936668396,
    "prof_backtrans_bleu": 48.14412180251901,
    "prof_backtrans_chrf": 78.8775383709237,
    "prof_backtrans_bertscore": 0.8608311414718628,
    "prof_backtrans_labse": 0.8336805701255798,
    "prof_backtrans_xlm_roberta": 0.7354798913002014,
    "llm_vs_prof_backtrans_bleu": 51.97834526699007,
    "llm_vs_prof_backtrans_chrf": 75.47942948376627,
    "llm_vs_prof_backtrans_bertscore": 0.8706992864608765,
    "llm_vs_prof_backtrans_labse": 0.827341616153717
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 44.331644755821465,
    "same_lang_chrf": 60.42095940256331,
    "same_lang_bertscore": 0.8250077962875366,
    "same_lang_comet": 0.8580245971679688,
    "cross_lang_xlm_roberta": 0.9881734848022461,
    "cross_lang_labse": 0.9933762550354004,
    "cross_lang_mbert": 0.9380428194999695,
    "cross_lang_comet_qe": -0.07142695039510727,
    "backtrans_bleu": 63.336810088188734,
    "backtrans_chrf": 84.03979348452168,
    "backtrans_bertscore": 0.9350377321243286,
    "prof_backtrans_bleu": 59.24753810359969,
    "prof_backtrans_chrf": 81.74263002779753,
    "prof_backtrans_bertscore": 0.903699517250061,
    "prof_backtrans_labse": 0.9338914155960083,
    "prof_backtrans_xlm_roberta": 0.7654762268066406,
    "llm_vs_prof_backtrans_bleu": 59.98485239877586,
    "llm_vs_prof_backtrans_chrf": 80.06254430641147,
    "llm_vs_prof_backtrans_bertscore": 0.9003651142120361,
    "llm_vs_prof_backtrans_labse": 0.9347380995750427
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 44.2750263800148,
    "same_lang_chrf": 68.54778944480726,
    "same_lang_bertscore": 0.77464759349823,
    "same_lang_comet": 0.8078992366790771,
    "cross_lang_xlm_roberta": 0.9254311323165894,
    "cross_lang_labse": 0.8967940211296082,
    "cross_lang_mbert": 0.8599328994750977,
    "cross_lang_comet_qe": 0.12745551764965057,
    "backtrans_bleu": 70.1883843857474,
    "backtrans_chrf": 87.33049420372686,
    "backtrans_bertscore": 0.8670598864555359,
    "prof_backtrans_bleu": 72.27096825371804,
    "prof_backtrans_chrf": 88.1073459190372,
    "prof_backtrans_bertscore": 0.8638822436332703,
    "prof_backtrans_labse": 0.8590075969696045,
    "prof_backtrans_xlm_roberta": 0.8498566746711731,
    "llm_vs_prof_backtrans_bleu": 78.76902404881868,
    "llm_vs_prof_backtrans_chrf": 88.30305672415378,
    "llm_vs_prof_backtrans_bertscore": 0.9279437065124512,
    "llm_vs_prof_backtrans_labse": 0.9439331293106079
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 34.1858895248864,
    "same_lang_chrf": 65.26683082562434,
    "same_lang_bertscore": 0.7902224659919739,
    "same_lang_comet": 0.8645893931388855,
    "cross_lang_xlm_roberta": 0.8906329870223999,
    "cross_lang_labse": 0.8663865327835083,
    "cross_lang_mbert": 0.8557312488555908,
    "cross_lang_comet_qe": 0.21088075637817383,
    "backtrans_bleu": 68.10123814240579,
    "backtrans_chrf": 85.09249769660173,
    "backtrans_bertscore": 0.8696826100349426,
    "prof_backtrans_bleu": 43.52191790385435,
    "prof_backtrans_chrf": 76.75451213687398,
    "prof_backtrans_bertscore": 0.8703411817550659,
    "prof_backtrans_labse": 0.9100569486618042,
    "prof_backtrans_xlm_roberta": 0.8009294271469116,
    "llm_vs_prof_backtrans_bleu": 50.07644279795548,
    "llm_vs_prof_backtrans_chrf": 73.04547296357946,
    "llm_vs_prof_backtrans_bertscore": 0.8774551153182983,
    "llm_vs_prof_backtrans_labse": 0.8980676531791687
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 23.61932270553342,
    "same_lang_chrf": 45.21237110374325,
    "same_lang_bertscore": 0.7603281736373901,
    "same_lang_comet": 0.8351999521255493,
    "cross_lang_xlm_roberta": 0.8885834813117981,
    "cross_lang_labse": 0.8746845722198486,
    "cross_lang_mbert": 0.8427846431732178,
    "cross_lang_comet_qe": 0.1427532583475113,
    "backtrans_bleu": 57.30093878934494,
    "backtrans_chrf": 82.16963779889187,
    "backtrans_bertscore": 0.8617111444473267,
    "prof_backtrans_bleu": 47.743849540984826,
    "prof_backtrans_chrf": 77.35018256083973,
    "prof_backtrans_bertscore": 0.8642699718475342,
    "prof_backtrans_labse": 0.9169715642929077,
    "prof_backtrans_xlm_roberta": 0.7735559344291687,
    "llm_vs_prof_backtrans_bleu": 53.48783475638917,
    "llm_vs_prof_backtrans_chrf": 77.25967200685824,
    "llm_vs_prof_backtrans_bertscore": 0.8921542763710022,
    "llm_vs_prof_backtrans_labse": 0.8801840543746948
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 39.88787820230773,
    "same_lang_chrf": 64.68181695774155,
    "same_lang_bertscore": 0.8307067155838013,
    "same_lang_comet": 0.7691845893859863,
    "cross_lang_xlm_roberta": 0.9576783180236816,
    "cross_lang_labse": 0.9742425680160522,
    "cross_lang_mbert": 0.9088651537895203,
    "cross_lang_comet_qe": -0.1490071713924408,
    "backtrans_bleu": 56.27793797961409,
    "backtrans_chrf": 80.31403643015926,
    "backtrans_bertscore": 0.9116082787513733,
    "prof_backtrans_bleu": 56.75496431759201,
    "prof_backtrans_chrf": 81.43286558191309,
    "prof_backtrans_bertscore": 0.8958200812339783,
    "prof_backtrans_labse": 0.937952995300293,
    "prof_backtrans_xlm_roberta": 0.736467719078064,
    "llm_vs_prof_backtrans_bleu": 52.82145578833001,
    "llm_vs_prof_backtrans_chrf": 76.38806384910815,
    "llm_vs_prof_backtrans_bertscore": 0.8993337750434875,
    "llm_vs_prof_backtrans_labse": 0.9482566118240356
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 44.0594248237939,
    "same_lang_chrf": 73.4157171161794,
    "same_lang_bertscore": 0.8202621340751648,
    "same_lang_comet": 0.8032960891723633,
    "cross_lang_xlm_roberta": 0.9816811084747314,
    "cross_lang_labse": 0.9904260635375977,
    "cross_lang_mbert": 0.937127947807312,
    "cross_lang_comet_qe": -0.04468563199043274,
    "backtrans_bleu": 67.58380861763938,
    "backtrans_chrf": 85.4503593973358,
    "backtrans_bertscore": 0.9125998616218567,
    "prof_backtrans_bleu": 52.939720767801205,
    "prof_backtrans_chrf": 80.66458966380631,
    "prof_backtrans_bertscore": 0.883693277835846,
    "prof_backtrans_labse": 0.8765150308609009,
    "prof_backtrans_xlm_roberta": 0.8574463725090027,
    "llm_vs_prof_backtrans_bleu": 56.1521326285911,
    "llm_vs_prof_backtrans_chrf": 78.85689136473978,
    "llm_vs_prof_backtrans_bertscore": 0.8854309916496277,
    "llm_vs_prof_backtrans_labse": 0.8704707622528076
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 12.587137384930898,
    "same_lang_chrf": 37.91564143587301,
    "same_lang_bertscore": 0.7909961938858032,
    "same_lang_comet": 0.8655552864074707,
    "cross_lang_xlm_roberta": 0.950066089630127,
    "cross_lang_labse": 0.9828503727912903,
    "cross_lang_mbert": 0.9261653423309326,
    "cross_lang_comet_qe": 0.16738447546958923,
    "backtrans_bleu": 66.06591928885186,
    "backtrans_chrf": 84.91996261168234,
    "backtrans_bertscore": 0.8932532668113708,
    "prof_backtrans_bleu": 55.8245326517274,
    "prof_backtrans_chrf": 80.87004816983593,
    "prof_backtrans_bertscore": 0.8572517037391663,
    "prof_backtrans_labse": 0.8562955856323242,
    "prof_backtrans_xlm_roberta": 0.6985117793083191,
    "llm_vs_prof_backtrans_bleu": 61.68255603467862,
    "llm_vs_prof_backtrans_chrf": 84.24230788457187,
    "llm_vs_prof_backtrans_bertscore": 0.8920990824699402,
    "llm_vs_prof_backtrans_labse": 0.871806263923645
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 47.311535652935135,
    "same_lang_chrf": 65.17888964514378,
    "same_lang_bertscore": 0.8404247164726257,
    "same_lang_comet": 0.8719667196273804,
    "cross_lang_xlm_roberta": 0.9856701493263245,
    "cross_lang_labse": 0.9926292300224304,
    "cross_lang_mbert": 0.9555280208587646,
    "cross_lang_comet_qe": -0.0093793123960495,
    "backtrans_bleu": 67.98336869454522,
    "backtrans_chrf": 85.55784842846124,
    "backtrans_bertscore": 0.9304311275482178,
    "prof_backtrans_bleu": 60.05767087342766,
    "prof_backtrans_chrf": 81.94976628907497,
    "prof_backtrans_bertscore": 0.9073584675788879,
    "prof_backtrans_labse": 0.9312927722930908,
    "prof_backtrans_xlm_roberta": 0.7686148881912231,
    "llm_vs_prof_backtrans_bleu": 62.41850497133673,
    "llm_vs_prof_backtrans_chrf": 82.1129351525762,
    "llm_vs_prof_backtrans_bertscore": 0.9043487310409546,
    "llm_vs_prof_backtrans_labse": 0.9221408367156982
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 49.000062420422495,
    "same_lang_chrf": 75.48646600243654,
    "same_lang_bertscore": 0.8326678276062012,
    "same_lang_comet": 0.8533591032028198,
    "cross_lang_xlm_roberta": 0.988566517829895,
    "cross_lang_labse": 0.9944589734077454,
    "cross_lang_mbert": 0.9901329278945923,
    "cross_lang_comet_qe": -0.10337674617767334,
    "backtrans_bleu": 70.88536342065761,
    "backtrans_chrf": 87.89304571279474,
    "backtrans_bertscore": 0.920819103717804,
    "prof_backtrans_bleu": 63.357088991479785,
    "prof_backtrans_chrf": 84.36116282142186,
    "prof_backtrans_bertscore": 0.8883069157600403,
    "prof_backtrans_labse": 0.941480815410614,
    "prof_backtrans_xlm_roberta": 0.8578218221664429,
    "llm_vs_prof_backtrans_bleu": 65.21629451733563,
    "llm_vs_prof_backtrans_chrf": 83.50576731625543,
    "llm_vs_prof_backtrans_bertscore": 0.8881245851516724,
    "llm_vs_prof_backtrans_labse": 0.9387238025665283
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 32.86006006378432,
    "same_lang_chrf": 66.53493177895275,
    "same_lang_bertscore": 0.8490828275680542,
    "same_lang_comet": 0.8465818762779236,
    "cross_lang_xlm_roberta": 0.9913687109947205,
    "cross_lang_labse": 0.994525134563446,
    "cross_lang_mbert": 0.9930325746536255,
    "cross_lang_comet_qe": -0.10878036171197891,
    "backtrans_bleu": 69.8504655075821,
    "backtrans_chrf": 87.02534485281844,
    "backtrans_bertscore": 0.9318032264709473,
    "prof_backtrans_bleu": 46.13503782573028,
    "prof_backtrans_chrf": 78.32723961513538,
    "prof_backtrans_bertscore": 0.8519833087921143,
    "prof_backtrans_labse": 0.8329002857208252,
    "prof_backtrans_xlm_roberta": 0.7603528499603271,
    "llm_vs_prof_backtrans_bleu": 46.862923660044686,
    "llm_vs_prof_backtrans_chrf": 74.90149075983712,
    "llm_vs_prof_backtrans_bertscore": 0.8536522388458252,
    "llm_vs_prof_backtrans_labse": 0.8317989706993103
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 23.406027323041975,
    "same_lang_chrf": 42.17905376569893,
    "same_lang_bertscore": 0.8056063055992126,
    "same_lang_comet": 0.846210241317749,
    "cross_lang_xlm_roberta": 0.971231997013092,
    "cross_lang_labse": 0.9891092777252197,
    "cross_lang_mbert": 0.9343723654747009,
    "cross_lang_comet_qe": -0.028115957975387573,
    "backtrans_bleu": 64.52248536275367,
    "backtrans_chrf": 83.92450525600336,
    "backtrans_bertscore": 0.9114684462547302,
    "prof_backtrans_bleu": 55.85432925209639,
    "prof_backtrans_chrf": 81.15287746275231,
    "prof_backtrans_bertscore": 0.8567885756492615,
    "prof_backtrans_labse": 0.8541128635406494,
    "prof_backtrans_xlm_roberta": 0.7158846855163574,
    "llm_vs_prof_backtrans_bleu": 58.336361556391275,
    "llm_vs_prof_backtrans_chrf": 82.49909358565995,
    "llm_vs_prof_backtrans_bertscore": 0.8656576871871948,
    "llm_vs_prof_backtrans_labse": 0.8598514199256897
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 40.51388007528723,
    "same_lang_chrf": 64.96432559152304,
    "same_lang_bertscore": 0.8385865092277527,
    "same_lang_comet": 0.7543357610702515,
    "cross_lang_xlm_roberta": 0.9739847779273987,
    "cross_lang_labse": 0.992864727973938,
    "cross_lang_mbert": 0.9226725697517395,
    "cross_lang_comet_qe": 0.036262765526771545,
    "backtrans_bleu": 68.36578843955883,
    "backtrans_chrf": 85.15491792481106,
    "backtrans_bertscore": 0.9258854985237122,
    "prof_backtrans_bleu": 55.171085203486086,
    "prof_backtrans_chrf": 80.88427420272409,
    "prof_backtrans_bertscore": 0.9038000702857971,
    "prof_backtrans_labse": 0.9350111484527588,
    "prof_backtrans_xlm_roberta": 0.7636949419975281,
    "llm_vs_prof_backtrans_bleu": 55.88984726390856,
    "llm_vs_prof_backtrans_chrf": 78.39432195598616,
    "llm_vs_prof_backtrans_bertscore": 0.9083818197250366,
    "llm_vs_prof_backtrans_labse": 0.9352901577949524
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 47.94728244887286,
    "same_lang_chrf": 74.56910587831338,
    "same_lang_bertscore": 0.8001797199249268,
    "same_lang_comet": 0.8137359619140625,
    "cross_lang_xlm_roberta": 0.844914436340332,
    "cross_lang_labse": 0.7981173992156982,
    "cross_lang_mbert": 0.7983789443969727,
    "cross_lang_comet_qe": 0.25212612748146057,
    "backtrans_bleu": 68.35113597909827,
    "backtrans_chrf": 86.68096247578058,
    "backtrans_bertscore": 0.8549721837043762,
    "prof_backtrans_bleu": 54.559957889896204,
    "prof_backtrans_chrf": 82.09380160232709,
    "prof_backtrans_bertscore": 0.8626797795295715,
    "prof_backtrans_labse": 0.868051290512085,
    "prof_backtrans_xlm_roberta": 0.9061906933784485,
    "llm_vs_prof_backtrans_bleu": 62.46856157590798,
    "llm_vs_prof_backtrans_chrf": 78.62492872960634,
    "llm_vs_prof_backtrans_bertscore": 0.9220268726348877,
    "llm_vs_prof_backtrans_labse": 0.8858465552330017
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 18.047502681197127,
    "same_lang_chrf": 38.489576479308084,
    "same_lang_bertscore": 0.8186591863632202,
    "same_lang_comet": 0.8748494386672974,
    "cross_lang_xlm_roberta": 0.8223078846931458,
    "cross_lang_labse": 0.8069441914558411,
    "cross_lang_mbert": 0.8076803684234619,
    "cross_lang_comet_qe": 0.2636505365371704,
    "backtrans_bleu": 68.0106966365585,
    "backtrans_chrf": 86.35682266462119,
    "backtrans_bertscore": 0.8535434603691101,
    "prof_backtrans_bleu": 50.315902565976494,
    "prof_backtrans_chrf": 79.6638915967633,
    "prof_backtrans_bertscore": 0.8603057265281677,
    "prof_backtrans_labse": 0.843083381652832,
    "prof_backtrans_xlm_roberta": 0.7522953152656555,
    "llm_vs_prof_backtrans_bleu": 53.199986686951135,
    "llm_vs_prof_backtrans_chrf": 75.55726161428422,
    "llm_vs_prof_backtrans_bertscore": 0.9273764491081238,
    "llm_vs_prof_backtrans_labse": 0.9099740982055664
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 49.03786282364673,
    "same_lang_chrf": 66.47885365950579,
    "same_lang_bertscore": 0.7915629148483276,
    "same_lang_comet": 0.8658583164215088,
    "cross_lang_xlm_roberta": 0.8387383222579956,
    "cross_lang_labse": 0.8083822727203369,
    "cross_lang_mbert": 0.8107008337974548,
    "cross_lang_comet_qe": 0.28045371174812317,
    "backtrans_bleu": 68.23943541558928,
    "backtrans_chrf": 88.2903987831871,
    "backtrans_bertscore": 0.8527617454528809,
    "prof_backtrans_bleu": 63.81882469912758,
    "prof_backtrans_chrf": 84.3752466702597,
    "prof_backtrans_bertscore": 0.8579277992248535,
    "prof_backtrans_labse": 0.8467238545417786,
    "prof_backtrans_xlm_roberta": 0.8099029064178467,
    "llm_vs_prof_backtrans_bleu": 65.98386235880201,
    "llm_vs_prof_backtrans_chrf": 83.23545800282017,
    "llm_vs_prof_backtrans_bertscore": 0.9312825798988342,
    "llm_vs_prof_backtrans_labse": 0.9062609672546387
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 49.168818629221164,
    "same_lang_chrf": 78.17642056279978,
    "same_lang_bertscore": 0.7841718792915344,
    "same_lang_comet": 0.8354566097259521,
    "cross_lang_xlm_roberta": 0.9033211469650269,
    "cross_lang_labse": 0.8151328563690186,
    "cross_lang_mbert": 0.832156240940094,
    "cross_lang_comet_qe": 0.1954830288887024,
    "backtrans_bleu": 68.65884822709916,
    "backtrans_chrf": 89.7348093321041,
    "backtrans_bertscore": 0.8636811375617981,
    "prof_backtrans_bleu": 77.21080780329432,
    "prof_backtrans_chrf": 90.36435170428227,
    "prof_backtrans_bertscore": 0.8572887182235718,
    "prof_backtrans_labse": 0.8383300304412842,
    "prof_backtrans_xlm_roberta": 0.8453758358955383,
    "llm_vs_prof_backtrans_bleu": 74.01480290732137,
    "llm_vs_prof_backtrans_chrf": 90.14998874113076,
    "llm_vs_prof_backtrans_bertscore": 0.9484959244728088,
    "llm_vs_prof_backtrans_labse": 0.9085804224014282
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 30.375530465875176,
    "same_lang_chrf": 64.2980781195029,
    "same_lang_bertscore": 0.7789501547813416,
    "same_lang_comet": 0.8482959270477295,
    "cross_lang_xlm_roberta": 0.8572033047676086,
    "cross_lang_labse": 0.8087465763092041,
    "cross_lang_mbert": 0.8141371011734009,
    "cross_lang_comet_qe": 0.2667255103588104,
    "backtrans_bleu": 64.95445164626268,
    "backtrans_chrf": 86.07745825418948,
    "backtrans_bertscore": 0.8512611389160156,
    "prof_backtrans_bleu": 34.208584057185035,
    "prof_backtrans_chrf": 70.50678268081111,
    "prof_backtrans_bertscore": 0.85127854347229,
    "prof_backtrans_labse": 0.8325014710426331,
    "prof_backtrans_xlm_roberta": 0.7752063274383545,
    "llm_vs_prof_backtrans_bleu": 36.14729751511044,
    "llm_vs_prof_backtrans_chrf": 62.98732201463605,
    "llm_vs_prof_backtrans_bertscore": 0.9064353704452515,
    "llm_vs_prof_backtrans_labse": 0.9044083952903748
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 24.38737492060282,
    "same_lang_chrf": 40.86587379710445,
    "same_lang_bertscore": 0.7385173439979553,
    "same_lang_comet": 0.839946985244751,
    "cross_lang_xlm_roberta": 0.8497633337974548,
    "cross_lang_labse": 0.8031883239746094,
    "cross_lang_mbert": 0.7550439834594727,
    "cross_lang_comet_qe": 0.24245084822177887,
    "backtrans_bleu": 57.16909011235239,
    "backtrans_chrf": 82.65378632551253,
    "backtrans_bertscore": 0.8465479016304016,
    "prof_backtrans_bleu": 26.807255429186558,
    "prof_backtrans_chrf": 68.05566303186461,
    "prof_backtrans_bertscore": 0.8662933111190796,
    "prof_backtrans_labse": 0.9139975309371948,
    "prof_backtrans_xlm_roberta": 0.7396175265312195,
    "llm_vs_prof_backtrans_bleu": 24.82807841474152,
    "llm_vs_prof_backtrans_chrf": 66.06418000801918,
    "llm_vs_prof_backtrans_bertscore": 0.8344369530677795,
    "llm_vs_prof_backtrans_labse": 0.7884737253189087
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 45.97466903384851,
    "same_lang_chrf": 70.01139927130487,
    "same_lang_bertscore": 0.7951644062995911,
    "same_lang_comet": 0.7536221742630005,
    "cross_lang_xlm_roberta": 0.8982527852058411,
    "cross_lang_labse": 0.8722943067550659,
    "cross_lang_mbert": 0.8770141005516052,
    "cross_lang_comet_qe": 0.15073204040527344,
    "backtrans_bleu": 62.25421378127609,
    "backtrans_chrf": 83.91061379521146,
    "backtrans_bertscore": 0.8751638531684875,
    "prof_backtrans_bleu": 54.52067282791589,
    "prof_backtrans_chrf": 80.50913072721015,
    "prof_backtrans_bertscore": 0.856801450252533,
    "prof_backtrans_labse": 0.8383363485336304,
    "prof_backtrans_xlm_roberta": 0.7510136961936951,
    "llm_vs_prof_backtrans_bleu": 65.62686303833799,
    "llm_vs_prof_backtrans_chrf": 82.26344423897608,
    "llm_vs_prof_backtrans_bertscore": 0.954334557056427,
    "llm_vs_prof_backtrans_labse": 0.9563085436820984
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 47.670395347925286,
    "same_lang_chrf": 72.76708218182873,
    "same_lang_bertscore": 0.814592719078064,
    "same_lang_comet": 0.8197499513626099,
    "cross_lang_xlm_roberta": 0.8692342638969421,
    "cross_lang_labse": 0.8637696504592896,
    "cross_lang_mbert": 0.873428225517273,
    "cross_lang_comet_qe": 0.21481384336948395,
    "backtrans_bleu": 68.71365172239334,
    "backtrans_chrf": 84.59614339252677,
    "backtrans_bertscore": 0.8688446283340454,
    "prof_backtrans_bleu": 50.1224650717458,
    "prof_backtrans_chrf": 80.01578200141311,
    "prof_backtrans_bertscore": 0.8605882525444031,
    "prof_backtrans_labse": 0.8746728301048279,
    "prof_backtrans_xlm_roberta": 0.9051346778869629,
    "llm_vs_prof_backtrans_bleu": 61.53381709288962,
    "llm_vs_prof_backtrans_chrf": 78.42013331652838,
    "llm_vs_prof_backtrans_bertscore": 0.9398377537727356,
    "llm_vs_prof_backtrans_labse": 0.9384632110595703
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 7.098880404564493,
    "same_lang_chrf": 39.122141026437355,
    "same_lang_bertscore": 0.8320719599723816,
    "same_lang_comet": 0.863949179649353,
    "cross_lang_xlm_roberta": 0.8373183012008667,
    "cross_lang_labse": 0.8705610036849976,
    "cross_lang_mbert": 0.8135502934455872,
    "cross_lang_comet_qe": 0.22548319399356842,
    "backtrans_bleu": 49.44420350868886,
    "backtrans_chrf": 78.14387737120178,
    "backtrans_bertscore": 0.8563551902770996,
    "prof_backtrans_bleu": 40.10754370654465,
    "prof_backtrans_chrf": 71.79186304965698,
    "prof_backtrans_bertscore": 0.853008508682251,
    "prof_backtrans_labse": 0.7678884267807007,
    "prof_backtrans_xlm_roberta": 0.7517739534378052,
    "llm_vs_prof_backtrans_bleu": 50.452915562715155,
    "llm_vs_prof_backtrans_chrf": 75.60133122231584,
    "llm_vs_prof_backtrans_bertscore": 0.8748235702514648,
    "llm_vs_prof_backtrans_labse": 0.771519660949707
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 47.51089908864002,
    "same_lang_chrf": 64.47452770383691,
    "same_lang_bertscore": 0.8021091818809509,
    "same_lang_comet": 0.8631736636161804,
    "cross_lang_xlm_roberta": 0.8675469756126404,
    "cross_lang_labse": 0.8616639375686646,
    "cross_lang_mbert": 0.7612506151199341,
    "cross_lang_comet_qe": 0.23766931891441345,
    "backtrans_bleu": 62.083741743251345,
    "backtrans_chrf": 80.66988055133135,
    "backtrans_bertscore": 0.8528174161911011,
    "prof_backtrans_bleu": 42.81497289191639,
    "prof_backtrans_chrf": 75.38883896874175,
    "prof_backtrans_bertscore": 0.8787688612937927,
    "prof_backtrans_labse": 0.9219375848770142,
    "prof_backtrans_xlm_roberta": 0.7826208472251892,
    "llm_vs_prof_backtrans_bleu": 47.236960743851014,
    "llm_vs_prof_backtrans_chrf": 69.77561282709537,
    "llm_vs_prof_backtrans_bertscore": 0.8903365731239319,
    "llm_vs_prof_backtrans_labse": 0.8933709263801575
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 43.40411768975942,
    "same_lang_chrf": 68.706745762095,
    "same_lang_bertscore": 0.7832096815109253,
    "same_lang_comet": 0.8352885246276855,
    "cross_lang_xlm_roberta": 0.8144944906234741,
    "cross_lang_labse": 0.8549938201904297,
    "cross_lang_mbert": 0.8209455609321594,
    "cross_lang_comet_qe": 0.203852117061615,
    "backtrans_bleu": 67.33871204031018,
    "backtrans_chrf": 84.11348999363744,
    "backtrans_bertscore": 0.8548837900161743,
    "prof_backtrans_bleu": 60.121765546079075,
    "prof_backtrans_chrf": 83.2251008686338,
    "prof_backtrans_bertscore": 0.8636091947555542,
    "prof_backtrans_labse": 0.8579812049865723,
    "prof_backtrans_xlm_roberta": 0.8706499934196472,
    "llm_vs_prof_backtrans_bleu": 62.25763687865012,
    "llm_vs_prof_backtrans_chrf": 79.56389563229726,
    "llm_vs_prof_backtrans_bertscore": 0.9278930425643921,
    "llm_vs_prof_backtrans_labse": 0.9499384760856628
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 30.50521295332583,
    "same_lang_chrf": 62.50945499203917,
    "same_lang_bertscore": 0.7784419059753418,
    "same_lang_comet": 0.857349693775177,
    "cross_lang_xlm_roberta": 0.8509623408317566,
    "cross_lang_labse": 0.8627057075500488,
    "cross_lang_mbert": 0.7852869629859924,
    "cross_lang_comet_qe": 0.253531277179718,
    "backtrans_bleu": 52.70065575978791,
    "backtrans_chrf": 76.4021363326806,
    "backtrans_bertscore": 0.8521860241889954,
    "prof_backtrans_bleu": null,
    "prof_backtrans_chrf": null,
    "prof_backtrans_bertscore": null,
    "prof_backtrans_labse": null,
    "prof_backtrans_xlm_roberta": null,
    "llm_vs_prof_backtrans_bleu": null,
    "llm_vs_prof_backtrans_chrf": null,
    "llm_vs_prof_backtrans_bertscore": null,
    "llm_vs_prof_backtrans_labse": null
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 24.40095524850028,
    "same_lang_chrf": 43.03181370549036,
    "same_lang_bertscore": 0.7640703916549683,
    "same_lang_comet": 0.8457744121551514,
    "cross_lang_xlm_roberta": 0.9185795187950134,
    "cross_lang_labse": 0.8624476194381714,
    "cross_lang_mbert": 0.8459916114807129,
    "cross_lang_comet_qe": 0.22062142193317413,
    "backtrans_bleu": 50.61478690423554,
    "backtrans_chrf": 78.43345357810149,
    "backtrans_bertscore": 0.8624276518821716,
    "prof_backtrans_bleu": 32.98172313340094,
    "prof_backtrans_chrf": 71.38986269307986,
    "prof_backtrans_bertscore": 0.8728598356246948,
    "prof_backtrans_labse": 0.907712459564209,
    "prof_backtrans_xlm_roberta": 0.7525290846824646,
    "llm_vs_prof_backtrans_bleu": 38.750512640393964,
    "llm_vs_prof_backtrans_chrf": 69.64374907242977,
    "llm_vs_prof_backtrans_bertscore": 0.8497942090034485,
    "llm_vs_prof_backtrans_labse": 0.8073697686195374
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 34.55064576791041,
    "same_lang_chrf": 61.43024441599404,
    "same_lang_bertscore": 0.7832807302474976,
    "same_lang_comet": 0.7991362810134888,
    "cross_lang_xlm_roberta": 0.8757116198539734,
    "cross_lang_labse": 0.864471435546875,
    "cross_lang_mbert": 0.8079148530960083,
    "cross_lang_comet_qe": 0.05974839627742767,
    "backtrans_bleu": 51.1166281526131,
    "backtrans_chrf": 76.7357282776789,
    "backtrans_bertscore": 0.8585625290870667,
    "prof_backtrans_bleu": 43.974880895948615,
    "prof_backtrans_chrf": 75.31072217056577,
    "prof_backtrans_bertscore": 0.8688254952430725,
    "prof_backtrans_labse": 0.9265669584274292,
    "prof_backtrans_xlm_roberta": 0.7632054686546326,
    "llm_vs_prof_backtrans_bleu": 48.459672995155344,
    "llm_vs_prof_backtrans_chrf": 72.86990731319496,
    "llm_vs_prof_backtrans_bertscore": 0.8859556913375854,
    "llm_vs_prof_backtrans_labse": 0.8943557739257812
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 45.96226602778697,
    "same_lang_chrf": 72.97751410973193,
    "same_lang_bertscore": 0.8251146674156189,
    "same_lang_comet": 0.8132388591766357,
    "cross_lang_xlm_roberta": 0.9886739253997803,
    "cross_lang_labse": 0.9514142870903015,
    "cross_lang_mbert": 0.9630820155143738,
    "cross_lang_comet_qe": 0.14662973582744598,
    "backtrans_bleu": 62.76241424453981,
    "backtrans_chrf": 83.72378457000939,
    "backtrans_bertscore": 0.8900065422058105,
    "prof_backtrans_bleu": 57.893425678792156,
    "prof_backtrans_chrf": 80.05546553579791,
    "prof_backtrans_bertscore": 0.8583896160125732,
    "prof_backtrans_labse": 0.8321253061294556,
    "prof_backtrans_xlm_roberta": 0.7422714233398438,
    "llm_vs_prof_backtrans_bleu": 65.08560446804856,
    "llm_vs_prof_backtrans_chrf": 84.12578028080792,
    "llm_vs_prof_backtrans_bertscore": 0.9096105694770813,
    "llm_vs_prof_backtrans_labse": 0.8819509744644165
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.215368255430304,
    "same_lang_chrf": 40.8528096336458,
    "same_lang_bertscore": 0.7811818718910217,
    "same_lang_comet": 0.8469651937484741,
    "cross_lang_xlm_roberta": 0.8518186211585999,
    "cross_lang_labse": 0.8576542139053345,
    "cross_lang_mbert": 0.809360146522522,
    "cross_lang_comet_qe": 0.1143515482544899,
    "backtrans_bleu": 62.22447825154266,
    "backtrans_chrf": 83.06178212925658,
    "backtrans_bertscore": 0.8749314546585083,
    "prof_backtrans_bleu": 51.67565271658754,
    "prof_backtrans_chrf": 78.81775477090022,
    "prof_backtrans_bertscore": 0.873021125793457,
    "prof_backtrans_labse": 0.8607971668243408,
    "prof_backtrans_xlm_roberta": 0.6310197114944458,
    "llm_vs_prof_backtrans_bleu": 56.18722456586719,
    "llm_vs_prof_backtrans_chrf": 77.03004596370145,
    "llm_vs_prof_backtrans_bertscore": 0.8570840358734131,
    "llm_vs_prof_backtrans_labse": 0.794223427772522
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 39.82482006306816,
    "same_lang_chrf": 60.19250837430272,
    "same_lang_bertscore": 0.814950168132782,
    "same_lang_comet": 0.8269200325012207,
    "cross_lang_xlm_roberta": 0.993559718132019,
    "cross_lang_labse": 0.991496741771698,
    "cross_lang_mbert": 0.985055148601532,
    "cross_lang_comet_qe": -0.11650965362787247,
    "backtrans_bleu": 61.65525359980331,
    "backtrans_chrf": 83.42083501819734,
    "backtrans_bertscore": 0.936873197555542,
    "prof_backtrans_bleu": 53.17597356628748,
    "prof_backtrans_chrf": 79.39641888398953,
    "prof_backtrans_bertscore": 0.8893500566482544,
    "prof_backtrans_labse": 0.8629763722419739,
    "prof_backtrans_xlm_roberta": 0.5885258913040161,
    "llm_vs_prof_backtrans_bleu": 53.941011510743934,
    "llm_vs_prof_backtrans_chrf": 77.27841160556427,
    "llm_vs_prof_backtrans_bertscore": 0.8938300013542175,
    "llm_vs_prof_backtrans_labse": 0.8679019212722778
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 45.347314112092775,
    "same_lang_chrf": 70.59332376975811,
    "same_lang_bertscore": 0.8063026070594788,
    "same_lang_comet": 0.8582736253738403,
    "cross_lang_xlm_roberta": 0.997009813785553,
    "cross_lang_labse": 0.9414342045783997,
    "cross_lang_mbert": 0.9741069674491882,
    "cross_lang_comet_qe": 0.09382449835538864,
    "backtrans_bleu": 69.0481335510818,
    "backtrans_chrf": 85.76717837259253,
    "backtrans_bertscore": 0.8987216949462891,
    "prof_backtrans_bleu": 57.887807330728656,
    "prof_backtrans_chrf": 76.23668579104329,
    "prof_backtrans_bertscore": 0.9104617834091187,
    "prof_backtrans_labse": 0.927606463432312,
    "prof_backtrans_xlm_roberta": 0.7256135940551758,
    "llm_vs_prof_backtrans_bleu": 52.23271764392345,
    "llm_vs_prof_backtrans_chrf": 78.95987701583668,
    "llm_vs_prof_backtrans_bertscore": 0.8709741234779358,
    "llm_vs_prof_backtrans_labse": 0.874318540096283
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 28.402749620697993,
    "same_lang_chrf": 62.878536875544356,
    "same_lang_bertscore": 0.8398529291152954,
    "same_lang_comet": 0.8567781448364258,
    "cross_lang_xlm_roberta": 0.9983863234519958,
    "cross_lang_labse": 0.9782084822654724,
    "cross_lang_mbert": 0.9959174394607544,
    "cross_lang_comet_qe": 0.01758052408695221,
    "backtrans_bleu": 63.788428296683435,
    "backtrans_chrf": 82.94081882006518,
    "backtrans_bertscore": 0.8997293710708618,
    "prof_backtrans_bleu": 47.08844161199113,
    "prof_backtrans_chrf": 76.52268231064646,
    "prof_backtrans_bertscore": 0.8664728999137878,
    "prof_backtrans_labse": 0.9056897759437561,
    "prof_backtrans_xlm_roberta": 0.6179249286651611,
    "llm_vs_prof_backtrans_bleu": 49.64305172952781,
    "llm_vs_prof_backtrans_chrf": 71.5662281306359,
    "llm_vs_prof_backtrans_bertscore": 0.8669676184654236,
    "llm_vs_prof_backtrans_labse": 0.9235824346542358
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 10.810524412203854,
    "same_lang_chrf": 42.751959319803575,
    "same_lang_bertscore": 0.7530913949012756,
    "same_lang_comet": 0.8282012343406677,
    "cross_lang_xlm_roberta": 0.9949047565460205,
    "cross_lang_labse": 0.9376596808433533,
    "cross_lang_mbert": 0.9807350635528564,
    "cross_lang_comet_qe": 0.1072361171245575,
    "backtrans_bleu": 56.95217949424218,
    "backtrans_chrf": 80.77686685064636,
    "backtrans_bertscore": 0.8821128010749817,
    "prof_backtrans_bleu": 34.16924144613027,
    "prof_backtrans_chrf": 73.09176466832261,
    "prof_backtrans_bertscore": 0.8625440001487732,
    "prof_backtrans_labse": 0.890615701675415,
    "prof_backtrans_xlm_roberta": 0.6352952718734741,
    "llm_vs_prof_backtrans_bleu": 42.458411173551674,
    "llm_vs_prof_backtrans_chrf": 72.46285162543025,
    "llm_vs_prof_backtrans_bertscore": 0.8570649027824402,
    "llm_vs_prof_backtrans_labse": 0.8460708260536194
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 31.918078907181634,
    "same_lang_chrf": 60.688480394622424,
    "same_lang_bertscore": 0.8186421990394592,
    "same_lang_comet": 0.783477783203125,
    "cross_lang_xlm_roberta": 0.993226945400238,
    "cross_lang_labse": 0.9851459860801697,
    "cross_lang_mbert": 0.9761860966682434,
    "cross_lang_comet_qe": -0.0412135124206543,
    "backtrans_bleu": 62.39373041434365,
    "backtrans_chrf": 83.59088680448582,
    "backtrans_bertscore": 0.8970118165016174,
    "prof_backtrans_bleu": 44.493857483769034,
    "prof_backtrans_chrf": 75.35183369395666,
    "prof_backtrans_bertscore": 0.8899055123329163,
    "prof_backtrans_labse": 0.8829813003540039,
    "prof_backtrans_xlm_roberta": 0.6839887499809265,
    "llm_vs_prof_backtrans_bleu": 46.958902402319694,
    "llm_vs_prof_backtrans_chrf": 71.87857127109889,
    "llm_vs_prof_backtrans_bertscore": 0.8894692659378052,
    "llm_vs_prof_backtrans_labse": 0.9016468524932861
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 42.59748016776281,
    "same_lang_chrf": 71.21589836664734,
    "same_lang_bertscore": 0.8486217260360718,
    "same_lang_comet": 0.8084203004837036,
    "cross_lang_xlm_roberta": 0.9906066656112671,
    "cross_lang_labse": 0.989976704120636,
    "cross_lang_mbert": 0.9706366062164307,
    "cross_lang_comet_qe": -0.11041563749313354,
    "backtrans_bleu": 64.84160931158277,
    "backtrans_chrf": 84.78254723782398,
    "backtrans_bertscore": 0.9057389497756958,
    "prof_backtrans_bleu": 43.174783389819474,
    "prof_backtrans_chrf": 74.16557440450939,
    "prof_backtrans_bertscore": 0.8815851807594299,
    "prof_backtrans_labse": 0.9203807711601257,
    "prof_backtrans_xlm_roberta": 0.8817050457000732,
    "llm_vs_prof_backtrans_bleu": 50.532126844362224,
    "llm_vs_prof_backtrans_chrf": 74.44059741186658,
    "llm_vs_prof_backtrans_bertscore": 0.8922193050384521,
    "llm_vs_prof_backtrans_labse": 0.9175611734390259
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 15.10825822241089,
    "same_lang_chrf": 40.616934195188,
    "same_lang_bertscore": 0.8284866809844971,
    "same_lang_comet": 0.8462535738945007,
    "cross_lang_xlm_roberta": 0.9980459809303284,
    "cross_lang_labse": 0.9901308417320251,
    "cross_lang_mbert": 0.9858027696609497,
    "cross_lang_comet_qe": -0.0802261009812355,
    "backtrans_bleu": 66.36179755401356,
    "backtrans_chrf": 85.91739645668015,
    "backtrans_bertscore": 0.9227296710014343,
    "prof_backtrans_bleu": 49.70682710298095,
    "prof_backtrans_chrf": 78.632170154696,
    "prof_backtrans_bertscore": 0.8851479291915894,
    "prof_backtrans_labse": 0.8784164190292358,
    "prof_backtrans_xlm_roberta": 0.6361213326454163,
    "llm_vs_prof_backtrans_bleu": 55.97354946011647,
    "llm_vs_prof_backtrans_chrf": 77.31333660849668,
    "llm_vs_prof_backtrans_bertscore": 0.8952932953834534,
    "llm_vs_prof_backtrans_labse": 0.8830483555793762
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 41.44067103670159,
    "same_lang_chrf": 62.099745604165236,
    "same_lang_bertscore": 0.8207495212554932,
    "same_lang_comet": 0.8333872556686401,
    "cross_lang_xlm_roberta": 0.9985129833221436,
    "cross_lang_labse": 0.9934273362159729,
    "cross_lang_mbert": 0.986278772354126,
    "cross_lang_comet_qe": -0.16817820072174072,
    "backtrans_bleu": 67.79463105261826,
    "backtrans_chrf": 86.44775371969556,
    "backtrans_bertscore": 0.9336673021316528,
    "prof_backtrans_bleu": 58.3069192242035,
    "prof_backtrans_chrf": 81.86108263606476,
    "prof_backtrans_bertscore": 0.8648359179496765,
    "prof_backtrans_labse": 0.8469745516777039,
    "prof_backtrans_xlm_roberta": 0.580298125743866,
    "llm_vs_prof_backtrans_bleu": 59.082573120410146,
    "llm_vs_prof_backtrans_chrf": 80.31230618915185,
    "llm_vs_prof_backtrans_bertscore": 0.8706796765327454,
    "llm_vs_prof_backtrans_labse": 0.8411164283752441
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 42.41371126985519,
    "same_lang_chrf": 72.44077943546698,
    "same_lang_bertscore": 0.8269157409667969,
    "same_lang_comet": 0.8420829772949219,
    "cross_lang_xlm_roberta": 0.991889238357544,
    "cross_lang_labse": 0.9950410723686218,
    "cross_lang_mbert": 0.9855522513389587,
    "cross_lang_comet_qe": -0.11475983262062073,
    "backtrans_bleu": 64.08298962797338,
    "backtrans_chrf": 84.54932245465987,
    "backtrans_bertscore": 0.912606418132782,
    "prof_backtrans_bleu": 59.062787174968406,
    "prof_backtrans_chrf": 83.58381703683744,
    "prof_backtrans_bertscore": 0.9034655094146729,
    "prof_backtrans_labse": 0.9273145794868469,
    "prof_backtrans_xlm_roberta": 0.7334553003311157,
    "llm_vs_prof_backtrans_bleu": 58.40050082398483,
    "llm_vs_prof_backtrans_chrf": 81.84091983558844,
    "llm_vs_prof_backtrans_bertscore": 0.907599925994873,
    "llm_vs_prof_backtrans_labse": 0.9272370934486389
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 29.0977075455473,
    "same_lang_chrf": 63.872174780635206,
    "same_lang_bertscore": 0.8430969715118408,
    "same_lang_comet": 0.856816291809082,
    "cross_lang_xlm_roberta": 0.9983269572257996,
    "cross_lang_labse": 0.9928473234176636,
    "cross_lang_mbert": 0.9958456754684448,
    "cross_lang_comet_qe": 0.03947751224040985,
    "backtrans_bleu": 80.25664984667894,
    "backtrans_chrf": 92.21488401549804,
    "backtrans_bertscore": 0.9246627688407898,
    "prof_backtrans_bleu": 35.116311261977835,
    "prof_backtrans_chrf": 72.2181431602983,
    "prof_backtrans_bertscore": 0.8765909075737,
    "prof_backtrans_labse": 0.8824967741966248,
    "prof_backtrans_xlm_roberta": 0.6177590489387512,
    "llm_vs_prof_backtrans_bleu": 38.28633476122405,
    "llm_vs_prof_backtrans_chrf": 66.91052563878,
    "llm_vs_prof_backtrans_bertscore": 0.8684419393539429,
    "llm_vs_prof_backtrans_labse": 0.8936976194381714
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 9.514344406883597,
    "same_lang_chrf": 37.332542093761155,
    "same_lang_bertscore": 0.766939640045166,
    "same_lang_comet": 0.7959797382354736,
    "cross_lang_xlm_roberta": 0.9958529472351074,
    "cross_lang_labse": 0.9809939861297607,
    "cross_lang_mbert": 0.985879123210907,
    "cross_lang_comet_qe": -0.09409785270690918,
    "backtrans_bleu": 63.741425223972925,
    "backtrans_chrf": 83.68580736373153,
    "backtrans_bertscore": 0.9214083552360535,
    "prof_backtrans_bleu": 46.96542821274578,
    "prof_backtrans_chrf": 76.52861792070314,
    "prof_backtrans_bertscore": 0.8874059319496155,
    "prof_backtrans_labse": 0.9000110626220703,
    "prof_backtrans_xlm_roberta": 0.6848554611206055,
    "llm_vs_prof_backtrans_bleu": 50.82338491782793,
    "llm_vs_prof_backtrans_chrf": 75.29550697676383,
    "llm_vs_prof_backtrans_bertscore": 0.8940841555595398,
    "llm_vs_prof_backtrans_labse": 0.8915079236030579
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 25.912881571444043,
    "same_lang_chrf": 56.14992817741773,
    "same_lang_bertscore": 0.8162757754325867,
    "same_lang_comet": 0.7314097881317139,
    "cross_lang_xlm_roberta": 0.9971298575401306,
    "cross_lang_labse": 0.9825646281242371,
    "cross_lang_mbert": 0.9842984080314636,
    "cross_lang_comet_qe": -0.13035064935684204,
    "backtrans_bleu": 68.97132269889033,
    "backtrans_chrf": 86.34339344303609,
    "backtrans_bertscore": 0.9342784881591797,
    "prof_backtrans_bleu": 42.2541406916389,
    "prof_backtrans_chrf": 74.16389726584057,
    "prof_backtrans_bertscore": 0.8874077200889587,
    "prof_backtrans_labse": 0.8825831413269043,
    "prof_backtrans_xlm_roberta": 0.6811599135398865,
    "llm_vs_prof_backtrans_bleu": 44.94342880081688,
    "llm_vs_prof_backtrans_chrf": 70.83129052028855,
    "llm_vs_prof_backtrans_bertscore": 0.8887236714363098,
    "llm_vs_prof_backtrans_labse": 0.9064579606056213
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 46.75825852055568,
    "same_lang_chrf": 72.8991933153122,
    "same_lang_bertscore": 0.8002985119819641,
    "same_lang_comet": 0.8056639432907104,
    "cross_lang_xlm_roberta": 0.7986679673194885,
    "cross_lang_labse": 0.8614867329597473,
    "cross_lang_mbert": 0.7715224027633667,
    "cross_lang_comet_qe": 0.21608901023864746,
    "backtrans_bleu": 64.83361848878074,
    "backtrans_chrf": 84.27251749744251,
    "backtrans_bertscore": 0.8765262365341187,
    "prof_backtrans_bleu": 44.588132617584314,
    "prof_backtrans_chrf": 74.64459328195454,
    "prof_backtrans_bertscore": 0.8697257041931152,
    "prof_backtrans_labse": 0.8642071485519409,
    "prof_backtrans_xlm_roberta": 0.8504754304885864,
    "llm_vs_prof_backtrans_bleu": 56.529723624802685,
    "llm_vs_prof_backtrans_chrf": 74.77655344789284,
    "llm_vs_prof_backtrans_bertscore": 0.919418215751648,
    "llm_vs_prof_backtrans_labse": 0.9156093597412109
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 17.690842223785854,
    "same_lang_chrf": 41.13847896929685,
    "same_lang_bertscore": 0.7621747255325317,
    "same_lang_comet": 0.8403180837631226,
    "cross_lang_xlm_roberta": 0.8170780539512634,
    "cross_lang_labse": 0.7996143102645874,
    "cross_lang_mbert": 0.7521017789840698,
    "cross_lang_comet_qe": 0.19174198806285858,
    "backtrans_bleu": 61.49687891860235,
    "backtrans_chrf": 83.90005456523171,
    "backtrans_bertscore": 0.8624580502510071,
    "prof_backtrans_bleu": 40.703844006969646,
    "prof_backtrans_chrf": 74.65741469291912,
    "prof_backtrans_bertscore": 0.8704458475112915,
    "prof_backtrans_labse": 0.8590883016586304,
    "prof_backtrans_xlm_roberta": 0.6297829151153564,
    "llm_vs_prof_backtrans_bleu": 45.31901185567876,
    "llm_vs_prof_backtrans_chrf": 69.30448977877344,
    "llm_vs_prof_backtrans_bertscore": 0.874079167842865,
    "llm_vs_prof_backtrans_labse": 0.7330929636955261
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 50.547616900411136,
    "same_lang_chrf": 67.73420542533785,
    "same_lang_bertscore": 0.7856855392456055,
    "same_lang_comet": 0.8459839224815369,
    "cross_lang_xlm_roberta": 0.7850838303565979,
    "cross_lang_labse": 0.8611310124397278,
    "cross_lang_mbert": 0.7852236032485962,
    "cross_lang_comet_qe": 0.20878605544567108,
    "backtrans_bleu": 72.74915840332119,
    "backtrans_chrf": 87.99299479304854,
    "backtrans_bertscore": 0.8785633444786072,
    "prof_backtrans_bleu": 39.078741711487226,
    "prof_backtrans_chrf": 66.18785516289577,
    "prof_backtrans_bertscore": 0.8466834425926208,
    "prof_backtrans_labse": 0.8306623697280884,
    "prof_backtrans_xlm_roberta": 0.5885258913040161,
    "llm_vs_prof_backtrans_bleu": 42.46204520428166,
    "llm_vs_prof_backtrans_chrf": 71.8100920754059,
    "llm_vs_prof_backtrans_bertscore": 0.8437636494636536,
    "llm_vs_prof_backtrans_labse": 0.7675578594207764
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 47.45806370498822,
    "same_lang_chrf": 75.8385525196985,
    "same_lang_bertscore": 0.7728878855705261,
    "same_lang_comet": 0.8562949895858765,
    "cross_lang_xlm_roberta": 0.794946014881134,
    "cross_lang_labse": 0.8467068076133728,
    "cross_lang_mbert": 0.7789151668548584,
    "cross_lang_comet_qe": 0.24554331600666046,
    "backtrans_bleu": 67.74855725666306,
    "backtrans_chrf": 87.94982715158422,
    "backtrans_bertscore": 0.8740323781967163,
    "prof_backtrans_bleu": 55.78474826598579,
    "prof_backtrans_chrf": 82.23396300164644,
    "prof_backtrans_bertscore": 0.8986164331436157,
    "prof_backtrans_labse": 0.9233195781707764,
    "prof_backtrans_xlm_roberta": 0.7178633213043213,
    "llm_vs_prof_backtrans_bleu": 56.49757057302684,
    "llm_vs_prof_backtrans_chrf": 78.5371756558751,
    "llm_vs_prof_backtrans_bertscore": 0.8608307838439941,
    "llm_vs_prof_backtrans_labse": 0.8232957124710083
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 29.72108848755814,
    "same_lang_chrf": 64.43332709148457,
    "same_lang_bertscore": 0.7753232717514038,
    "same_lang_comet": 0.8545756936073303,
    "cross_lang_xlm_roberta": 0.8501085638999939,
    "cross_lang_labse": 0.8210846781730652,
    "cross_lang_mbert": 0.7147468328475952,
    "cross_lang_comet_qe": 0.2146908938884735,
    "backtrans_bleu": 38.7652750043648,
    "backtrans_chrf": 71.72917166220468,
    "backtrans_bertscore": 0.8554337620735168,
    "prof_backtrans_bleu": 30.154441452921198,
    "prof_backtrans_chrf": 66.87498164046943,
    "prof_backtrans_bertscore": 0.8597235083580017,
    "prof_backtrans_labse": 0.849312424659729,
    "prof_backtrans_xlm_roberta": 0.6238574981689453,
    "llm_vs_prof_backtrans_bleu": 38.5591007665121,
    "llm_vs_prof_backtrans_chrf": 65.38673754305768,
    "llm_vs_prof_backtrans_bertscore": 0.9042621850967407,
    "llm_vs_prof_backtrans_labse": 0.8624517917633057
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 7.2615115718965875,
    "same_lang_chrf": 36.42241263075315,
    "same_lang_bertscore": 0.7097381949424744,
    "same_lang_comet": 0.8239680528640747,
    "cross_lang_xlm_roberta": 0.8030678629875183,
    "cross_lang_labse": 0.8244653940200806,
    "cross_lang_mbert": 0.7740790247917175,
    "cross_lang_comet_qe": 0.25047767162323,
    "backtrans_bleu": 58.76130696579514,
    "backtrans_chrf": 83.05837879659953,
    "backtrans_bertscore": 0.8659060001373291,
    "prof_backtrans_bleu": 30.251190515098465,
    "prof_backtrans_chrf": 69.83136717170977,
    "prof_backtrans_bertscore": 0.8632056713104248,
    "prof_backtrans_labse": 0.8739567995071411,
    "prof_backtrans_xlm_roberta": 0.6239644289016724,
    "llm_vs_prof_backtrans_bleu": 29.282353165753506,
    "llm_vs_prof_backtrans_chrf": 63.403840497467726,
    "llm_vs_prof_backtrans_bertscore": 0.8511145710945129,
    "llm_vs_prof_backtrans_labse": 0.7628209590911865
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 32.01346772984331,
    "same_lang_chrf": 61.728272644633094,
    "same_lang_bertscore": 0.7764731049537659,
    "same_lang_comet": 0.7134748697280884,
    "cross_lang_xlm_roberta": 0.8127129077911377,
    "cross_lang_labse": 0.8730593323707581,
    "cross_lang_mbert": 0.7931832075119019,
    "cross_lang_comet_qe": 0.14190427958965302,
    "backtrans_bleu": 49.68185816001586,
    "backtrans_chrf": 77.4729885928684,
    "backtrans_bertscore": 0.8751975297927856,
    "prof_backtrans_bleu": 32.34190037042744,
    "prof_backtrans_chrf": 68.91724555668873,
    "prof_backtrans_bertscore": 0.8649819493293762,
    "prof_backtrans_labse": 0.8773207664489746,
    "prof_backtrans_xlm_roberta": 0.6853395104408264,
    "llm_vs_prof_backtrans_bleu": 42.907079436787086,
    "llm_vs_prof_backtrans_chrf": 68.75098925795857,
    "llm_vs_prof_backtrans_bertscore": 0.9173654317855835,
    "llm_vs_prof_backtrans_labse": 0.9195272326469421
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 46.128767615224604,
    "same_lang_chrf": 72.88244987042656,
    "same_lang_bertscore": 0.8544393181800842,
    "same_lang_comet": 0.832181453704834,
    "cross_lang_xlm_roberta": 0.9924080967903137,
    "cross_lang_labse": 0.9826159477233887,
    "cross_lang_mbert": 0.9859861135482788,
    "cross_lang_comet_qe": 0.12479333579540253,
    "backtrans_bleu": 63.433641748819966,
    "backtrans_chrf": 84.18646520637354,
    "backtrans_bertscore": 0.9016588926315308,
    "prof_backtrans_bleu": 42.238111476420755,
    "prof_backtrans_chrf": 69.23101470089307,
    "prof_backtrans_bertscore": 0.8640549778938293,
    "prof_backtrans_labse": 0.9249376654624939,
    "prof_backtrans_xlm_roberta": 0.8463131785392761,
    "llm_vs_prof_backtrans_bleu": 50.61498009569488,
    "llm_vs_prof_backtrans_chrf": 75.94154571399456,
    "llm_vs_prof_backtrans_bertscore": 0.8801653385162354,
    "llm_vs_prof_backtrans_labse": 0.9287230968475342
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.391186406385218,
    "same_lang_chrf": 40.235136508501,
    "same_lang_bertscore": 0.788500189781189,
    "same_lang_comet": 0.8432570695877075,
    "cross_lang_xlm_roberta": 0.8575798869132996,
    "cross_lang_labse": 0.8682201504707336,
    "cross_lang_mbert": 0.7905935049057007,
    "cross_lang_comet_qe": 0.17829462885856628,
    "backtrans_bleu": 59.26966309888256,
    "backtrans_chrf": 81.562187976971,
    "backtrans_bertscore": 0.8780969977378845,
    "prof_backtrans_bleu": 34.957678338666064,
    "prof_backtrans_chrf": 64.18556224382864,
    "prof_backtrans_bertscore": 0.8508649468421936,
    "prof_backtrans_labse": 0.8569695353507996,
    "prof_backtrans_xlm_roberta": 0.6282214522361755,
    "llm_vs_prof_backtrans_bleu": 46.41876149030547,
    "llm_vs_prof_backtrans_chrf": 74.10728768908453,
    "llm_vs_prof_backtrans_bertscore": 0.882821798324585,
    "llm_vs_prof_backtrans_labse": 0.7960257530212402
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 42.7863039382533,
    "same_lang_chrf": 62.177194865640914,
    "same_lang_bertscore": 0.7857959270477295,
    "same_lang_comet": 0.837429404258728,
    "cross_lang_xlm_roberta": 0.6535190343856812,
    "cross_lang_labse": 0.8460361361503601,
    "cross_lang_mbert": 0.6254333257675171,
    "cross_lang_comet_qe": 0.17649398744106293,
    "backtrans_bleu": 52.364329469541545,
    "backtrans_chrf": 77.31831803476983,
    "backtrans_bertscore": 0.8672881126403809,
    "prof_backtrans_bleu": 35.641759836533026,
    "prof_backtrans_chrf": 66.43940702149737,
    "prof_backtrans_bertscore": 0.8840931057929993,
    "prof_backtrans_labse": 0.8639266490936279,
    "prof_backtrans_xlm_roberta": 0.5957648158073425,
    "llm_vs_prof_backtrans_bleu": 39.991549077754506,
    "llm_vs_prof_backtrans_chrf": 70.200458910358,
    "llm_vs_prof_backtrans_bertscore": 0.8568810224533081,
    "llm_vs_prof_backtrans_labse": 0.8295993804931641
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 43.4808618624198,
    "same_lang_chrf": 68.7677005567037,
    "same_lang_bertscore": 0.7854083180427551,
    "same_lang_comet": 0.855635404586792,
    "cross_lang_xlm_roberta": 0.8494827151298523,
    "cross_lang_labse": 0.8709264397621155,
    "cross_lang_mbert": 0.8106611967086792,
    "cross_lang_comet_qe": 0.12287469953298569,
    "backtrans_bleu": 62.74042758316269,
    "backtrans_chrf": 82.92750166852126,
    "backtrans_bertscore": 0.8900285959243774,
    "prof_backtrans_bleu": 55.74297039459076,
    "prof_backtrans_chrf": 81.02339077156864,
    "prof_backtrans_bertscore": 0.9049093723297119,
    "prof_backtrans_labse": 0.9245855212211609,
    "prof_backtrans_xlm_roberta": 0.7172247767448425,
    "llm_vs_prof_backtrans_bleu": 55.10712736862761,
    "llm_vs_prof_backtrans_chrf": 75.45702119875325,
    "llm_vs_prof_backtrans_bertscore": 0.8597164154052734,
    "llm_vs_prof_backtrans_labse": 0.8322357535362244
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 29.676929164620176,
    "same_lang_chrf": 63.205813467534355,
    "same_lang_bertscore": 0.7775270938873291,
    "same_lang_comet": 0.8539477586746216,
    "cross_lang_xlm_roberta": 0.8441933393478394,
    "cross_lang_labse": 0.8648243546485901,
    "cross_lang_mbert": 0.7699231505393982,
    "cross_lang_comet_qe": 0.18629960715770721,
    "backtrans_bleu": 54.9018902599169,
    "backtrans_chrf": 79.95941987369658,
    "backtrans_bertscore": 0.8714081645011902,
    "prof_backtrans_bleu": 29.122654794715235,
    "prof_backtrans_chrf": 67.78068921740353,
    "prof_backtrans_bertscore": 0.868410050868988,
    "prof_backtrans_labse": 0.8573198914527893,
    "prof_backtrans_xlm_roberta": 0.6161773204803467,
    "llm_vs_prof_backtrans_bleu": 33.297527195775466,
    "llm_vs_prof_backtrans_chrf": 62.80430640145745,
    "llm_vs_prof_backtrans_bertscore": 0.8540415167808533,
    "llm_vs_prof_backtrans_labse": 0.7824974656105042
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 8.693339323258963,
    "same_lang_chrf": 40.643729536580224,
    "same_lang_bertscore": 0.7500189542770386,
    "same_lang_comet": 0.8274008631706238,
    "cross_lang_xlm_roberta": 0.8453018665313721,
    "cross_lang_labse": 0.8608168363571167,
    "cross_lang_mbert": 0.8028925657272339,
    "cross_lang_comet_qe": 0.1630009412765503,
    "backtrans_bleu": 49.895622659540386,
    "backtrans_chrf": 77.21787150537645,
    "backtrans_bertscore": 0.8781482577323914,
    "prof_backtrans_bleu": 25.33791861097176,
    "prof_backtrans_chrf": 67.07141325068761,
    "prof_backtrans_bertscore": 0.8660798668861389,
    "prof_backtrans_labse": 0.8942360877990723,
    "prof_backtrans_xlm_roberta": 0.6575025916099548,
    "llm_vs_prof_backtrans_bleu": 31.76709952396964,
    "llm_vs_prof_backtrans_chrf": 65.59212652866037,
    "llm_vs_prof_backtrans_bertscore": 0.8533786535263062,
    "llm_vs_prof_backtrans_labse": 0.7979429364204407
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 23.897928003095124,
    "same_lang_chrf": 54.269068350452756,
    "same_lang_bertscore": 0.7630035281181335,
    "same_lang_comet": 0.7804097533226013,
    "cross_lang_xlm_roberta": 0.8208011388778687,
    "cross_lang_labse": 0.8689387440681458,
    "cross_lang_mbert": 0.7873910665512085,
    "cross_lang_comet_qe": 0.11214343458414078,
    "backtrans_bleu": 54.18538553844743,
    "backtrans_chrf": 79.29193468556628,
    "backtrans_bertscore": 0.8730323314666748,
    "prof_backtrans_bleu": 27.53857606076181,
    "prof_backtrans_chrf": 62.027852996069136,
    "prof_backtrans_bertscore": 0.85054612159729,
    "prof_backtrans_labse": 0.8798571825027466,
    "prof_backtrans_xlm_roberta": 0.6902165412902832,
    "llm_vs_prof_backtrans_bleu": 30.641285623871727,
    "llm_vs_prof_backtrans_chrf": 61.54257631976995,
    "llm_vs_prof_backtrans_bertscore": 0.8485565185546875,
    "llm_vs_prof_backtrans_labse": 0.7890009880065918
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 47.77542910724133,
    "same_lang_chrf": 72.62371304526293,
    "same_lang_bertscore": 0.8452867269515991,
    "same_lang_comet": 0.8280556201934814,
    "cross_lang_xlm_roberta": 0.9730382561683655,
    "cross_lang_labse": 0.9868242740631104,
    "cross_lang_mbert": 0.9342415928840637,
    "cross_lang_comet_qe": -0.05613965541124344,
    "backtrans_bleu": 70.00543632775738,
    "backtrans_chrf": 85.22596675305304,
    "backtrans_bertscore": 0.9094059467315674,
    "prof_backtrans_bleu": 52.69950395450503,
    "prof_backtrans_chrf": 79.59276474092225,
    "prof_backtrans_bertscore": 0.9099752902984619,
    "prof_backtrans_labse": 0.9567682147026062,
    "prof_backtrans_xlm_roberta": 0.9432811141014099,
    "llm_vs_prof_backtrans_bleu": 56.499674271887216,
    "llm_vs_prof_backtrans_chrf": 76.70118558856093,
    "llm_vs_prof_backtrans_bertscore": 0.900218665599823,
    "llm_vs_prof_backtrans_labse": 0.9447062611579895
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.867028955169866,
    "same_lang_chrf": 40.95079470823876,
    "same_lang_bertscore": 0.8393688201904297,
    "same_lang_comet": 0.8685050010681152,
    "cross_lang_xlm_roberta": 0.9711271524429321,
    "cross_lang_labse": 0.9274935722351074,
    "cross_lang_mbert": 0.9505078196525574,
    "cross_lang_comet_qe": 0.2035231590270996,
    "backtrans_bleu": 68.2508694201439,
    "backtrans_chrf": 86.11964788907443,
    "backtrans_bertscore": 0.8887225389480591,
    "prof_backtrans_bleu": 50.20102677033178,
    "prof_backtrans_chrf": 78.14793673029766,
    "prof_backtrans_bertscore": 0.8899868130683899,
    "prof_backtrans_labse": 0.9196198582649231,
    "prof_backtrans_xlm_roberta": 0.7571013569831848,
    "llm_vs_prof_backtrans_bleu": 50.42593715268547,
    "llm_vs_prof_backtrans_chrf": 73.92656634526872,
    "llm_vs_prof_backtrans_bertscore": 0.8738903999328613,
    "llm_vs_prof_backtrans_labse": 0.8691751956939697
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 45.01830731138621,
    "same_lang_chrf": 62.8571226601269,
    "same_lang_bertscore": 0.8523843288421631,
    "same_lang_comet": 0.84813392162323,
    "cross_lang_xlm_roberta": 0.956117570400238,
    "cross_lang_labse": 0.9781473875045776,
    "cross_lang_mbert": 0.9381465911865234,
    "cross_lang_comet_qe": -0.14812889695167542,
    "backtrans_bleu": 65.70646292313116,
    "backtrans_chrf": 85.80642867667203,
    "backtrans_bertscore": 0.9281139373779297,
    "prof_backtrans_bleu": 67.52212512190727,
    "prof_backtrans_chrf": 85.34948185210315,
    "prof_backtrans_bertscore": 0.893994152545929,
    "prof_backtrans_labse": 0.901843249797821,
    "prof_backtrans_xlm_roberta": 0.7815969586372375,
    "llm_vs_prof_backtrans_bleu": 65.67857880196121,
    "llm_vs_prof_backtrans_chrf": 82.97881703825203,
    "llm_vs_prof_backtrans_bertscore": 0.8921878933906555,
    "llm_vs_prof_backtrans_labse": 0.8987517356872559
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 39.94240417781973,
    "same_lang_chrf": 65.03964600504447,
    "same_lang_bertscore": 0.8357101082801819,
    "same_lang_comet": 0.8546090126037598,
    "cross_lang_xlm_roberta": 0.9856665134429932,
    "cross_lang_labse": 0.9868496656417847,
    "cross_lang_mbert": 0.973750650882721,
    "cross_lang_comet_qe": 0.0015171095728874207,
    "backtrans_bleu": 75.42445843889062,
    "backtrans_chrf": 87.93946197477061,
    "backtrans_bertscore": 0.9502012729644775,
    "prof_backtrans_bleu": 72.16679889250584,
    "prof_backtrans_chrf": 87.91783558440703,
    "prof_backtrans_bertscore": 0.9215579628944397,
    "prof_backtrans_labse": 0.9501140117645264,
    "prof_backtrans_xlm_roberta": 0.8808329701423645,
    "llm_vs_prof_backtrans_bleu": 68.26374145486977,
    "llm_vs_prof_backtrans_chrf": 83.56636238636068,
    "llm_vs_prof_backtrans_bertscore": 0.9143606424331665,
    "llm_vs_prof_backtrans_labse": 0.9549935460090637
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 35.0968738116867,
    "same_lang_chrf": 66.02178890075993,
    "same_lang_bertscore": 0.8213591575622559,
    "same_lang_comet": 0.8721317052841187,
    "cross_lang_xlm_roberta": 0.9720616936683655,
    "cross_lang_labse": 0.9321141242980957,
    "cross_lang_mbert": 0.9443086385726929,
    "cross_lang_comet_qe": 0.232185959815979,
    "backtrans_bleu": 64.62705473306697,
    "backtrans_chrf": 83.141932907762,
    "backtrans_bertscore": 0.8878461718559265,
    "prof_backtrans_bleu": 39.98782118659861,
    "prof_backtrans_chrf": 73.08710896466654,
    "prof_backtrans_bertscore": 0.8919953107833862,
    "prof_backtrans_labse": 0.9369223117828369,
    "prof_backtrans_xlm_roberta": 0.7976193428039551,
    "llm_vs_prof_backtrans_bleu": 43.20085038293859,
    "llm_vs_prof_backtrans_chrf": 68.13405239838994,
    "llm_vs_prof_backtrans_bertscore": 0.8587144613265991,
    "llm_vs_prof_backtrans_labse": 0.8829942941665649
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 11.069574719556265,
    "same_lang_chrf": 28.47957966659333,
    "same_lang_bertscore": 0.7255271673202515,
    "same_lang_comet": 0.8437180519104004,
    "cross_lang_xlm_roberta": 0.9573566913604736,
    "cross_lang_labse": 0.927318811416626,
    "cross_lang_mbert": 0.9443066120147705,
    "cross_lang_comet_qe": 0.24799932539463043,
    "backtrans_bleu": 63.76727209689229,
    "backtrans_chrf": 84.03571716046899,
    "backtrans_bertscore": 0.874809980392456,
    "prof_backtrans_bleu": 23.377855249842817,
    "prof_backtrans_chrf": 64.22972421215796,
    "prof_backtrans_bertscore": 0.8540012836456299,
    "prof_backtrans_labse": 0.8533751964569092,
    "prof_backtrans_xlm_roberta": 0.6911548376083374,
    "llm_vs_prof_backtrans_bleu": 24.975728497337325,
    "llm_vs_prof_backtrans_chrf": 56.98640369565455,
    "llm_vs_prof_backtrans_bertscore": 0.847903847694397,
    "llm_vs_prof_backtrans_labse": 0.7918850183486938
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 30.083465024519047,
    "same_lang_chrf": 55.364983984420945,
    "same_lang_bertscore": 0.8119831085205078,
    "same_lang_comet": 0.7851831912994385,
    "cross_lang_xlm_roberta": 0.9796098470687866,
    "cross_lang_labse": 0.983158528804779,
    "cross_lang_mbert": 0.9602924585342407,
    "cross_lang_comet_qe": -0.019894123077392578,
    "backtrans_bleu": 65.25820567334632,
    "backtrans_chrf": 84.34302052615877,
    "backtrans_bertscore": 0.948348343372345,
    "prof_backtrans_bleu": 34.056013726541835,
    "prof_backtrans_chrf": 67.5742814999236,
    "prof_backtrans_bertscore": 0.8695071339607239,
    "prof_backtrans_labse": 0.8608694076538086,
    "prof_backtrans_xlm_roberta": 0.6784571409225464,
    "llm_vs_prof_backtrans_bleu": 36.99015488804201,
    "llm_vs_prof_backtrans_chrf": 62.11516247785036,
    "llm_vs_prof_backtrans_bertscore": 0.8726353049278259,
    "llm_vs_prof_backtrans_labse": 0.8401772975921631
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 47.3638945665803,
    "same_lang_chrf": 72.6900498446238,
    "same_lang_bertscore": 0.8505929112434387,
    "same_lang_comet": 0.8194177150726318,
    "cross_lang_xlm_roberta": 0.9836414456367493,
    "cross_lang_labse": 0.9963052868843079,
    "cross_lang_mbert": 0.9758144617080688,
    "cross_lang_comet_qe": 0.03827275335788727,
    "backtrans_bleu": 73.71563083768083,
    "backtrans_chrf": 88.15406311599904,
    "backtrans_bertscore": 0.9203038215637207,
    "prof_backtrans_bleu": 56.573712226618845,
    "prof_backtrans_chrf": 81.21142317179488,
    "prof_backtrans_bertscore": 0.90836101770401,
    "prof_backtrans_labse": 0.9605319499969482,
    "prof_backtrans_xlm_roberta": 0.9326208233833313,
    "llm_vs_prof_backtrans_bleu": 60.665962566960786,
    "llm_vs_prof_backtrans_chrf": 80.82919308850279,
    "llm_vs_prof_backtrans_bertscore": 0.9090859293937683,
    "llm_vs_prof_backtrans_labse": 0.9611308574676514
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 18.57699055139287,
    "same_lang_chrf": 41.0044741586468,
    "same_lang_bertscore": 0.8672386407852173,
    "same_lang_comet": 0.8833630084991455,
    "cross_lang_xlm_roberta": 0.9707704186439514,
    "cross_lang_labse": 0.9893499612808228,
    "cross_lang_mbert": 0.9430726170539856,
    "cross_lang_comet_qe": 0.0025890693068504333,
    "backtrans_bleu": 65.21254800997097,
    "backtrans_chrf": 84.61070603754276,
    "backtrans_bertscore": 0.901634693145752,
    "prof_backtrans_bleu": 45.9426854081914,
    "prof_backtrans_chrf": 77.41787229228471,
    "prof_backtrans_bertscore": 0.8916217088699341,
    "prof_backtrans_labse": 0.931860089302063,
    "prof_backtrans_xlm_roberta": 0.7570006251335144,
    "llm_vs_prof_backtrans_bleu": 48.422115302753554,
    "llm_vs_prof_backtrans_chrf": 73.28750102207226,
    "llm_vs_prof_backtrans_bertscore": 0.8899250626564026,
    "llm_vs_prof_backtrans_labse": 0.9302743077278137
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 43.68419962292855,
    "same_lang_chrf": 61.42646636724272,
    "same_lang_bertscore": 0.8508885502815247,
    "same_lang_comet": 0.8542337417602539,
    "cross_lang_xlm_roberta": 0.9950756430625916,
    "cross_lang_labse": 0.9956547021865845,
    "cross_lang_mbert": 0.9861477613449097,
    "cross_lang_comet_qe": 0.029213160276412964,
    "backtrans_bleu": 78.10930835002033,
    "backtrans_chrf": 89.85853990834376,
    "backtrans_bertscore": 0.9471613764762878,
    "prof_backtrans_bleu": 69.54112762308269,
    "prof_backtrans_chrf": 86.64764965735955,
    "prof_backtrans_bertscore": 0.8888847827911377,
    "prof_backtrans_labse": 0.8898193836212158,
    "prof_backtrans_xlm_roberta": 0.7708402872085571,
    "llm_vs_prof_backtrans_bleu": 71.89292575445684,
    "llm_vs_prof_backtrans_chrf": 84.71322632603253,
    "llm_vs_prof_backtrans_bertscore": 0.8911536931991577,
    "llm_vs_prof_backtrans_labse": 0.8910609483718872
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 42.86676586928352,
    "same_lang_chrf": 66.30026820693257,
    "same_lang_bertscore": 0.8558191657066345,
    "same_lang_comet": 0.8570455312728882,
    "cross_lang_xlm_roberta": 0.994937002658844,
    "cross_lang_labse": 0.997344434261322,
    "cross_lang_mbert": 0.998208224773407,
    "cross_lang_comet_qe": -0.09080860018730164,
    "backtrans_bleu": 72.92807559171901,
    "backtrans_chrf": 87.82573483612816,
    "backtrans_bertscore": 0.930016040802002,
    "prof_backtrans_bleu": 72.65580508535866,
    "prof_backtrans_chrf": 87.68223511038585,
    "prof_backtrans_bertscore": 0.8895550966262817,
    "prof_backtrans_labse": 0.9147871136665344,
    "prof_backtrans_xlm_roberta": 0.9008239507675171,
    "llm_vs_prof_backtrans_bleu": 72.64437153988567,
    "llm_vs_prof_backtrans_chrf": 86.69739156282516,
    "llm_vs_prof_backtrans_bertscore": 0.8821988105773926,
    "llm_vs_prof_backtrans_labse": 0.9144557118415833
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 26.964034889921557,
    "same_lang_chrf": 60.79921368594683,
    "same_lang_bertscore": 0.8350313305854797,
    "same_lang_comet": 0.8635880947113037,
    "cross_lang_xlm_roberta": 0.9996296167373657,
    "cross_lang_labse": 0.9948844313621521,
    "cross_lang_mbert": 0.9971191883087158,
    "cross_lang_comet_qe": -0.023386269807815552,
    "backtrans_bleu": 75.64508717532325,
    "backtrans_chrf": 89.32103636938787,
    "backtrans_bertscore": 0.9347573518753052,
    "prof_backtrans_bleu": 39.67217325920591,
    "prof_backtrans_chrf": 74.28017188285952,
    "prof_backtrans_bertscore": 0.8946728706359863,
    "prof_backtrans_labse": 0.93930983543396,
    "prof_backtrans_xlm_roberta": 0.7906380891799927,
    "llm_vs_prof_backtrans_bleu": 39.32070886725339,
    "llm_vs_prof_backtrans_chrf": 68.98242572344078,
    "llm_vs_prof_backtrans_bertscore": 0.8976842761039734,
    "llm_vs_prof_backtrans_labse": 0.9388501644134521
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 10.526385208624522,
    "same_lang_chrf": 28.392959065137845,
    "same_lang_bertscore": 0.7316934466362,
    "same_lang_comet": 0.811805784702301,
    "cross_lang_xlm_roberta": 0.9819284081459045,
    "cross_lang_labse": 0.9899699687957764,
    "cross_lang_mbert": 0.9430999755859375,
    "cross_lang_comet_qe": -0.0010354220867156982,
    "backtrans_bleu": 73.10463893097898,
    "backtrans_chrf": 86.56496981712105,
    "backtrans_bertscore": 0.9176052808761597,
    "prof_backtrans_bleu": 50.58466264978855,
    "prof_backtrans_chrf": 76.8422617341941,
    "prof_backtrans_bertscore": 0.862939178943634,
    "prof_backtrans_labse": 0.8524923324584961,
    "prof_backtrans_xlm_roberta": 0.663809597492218,
    "llm_vs_prof_backtrans_bleu": 50.74821885800853,
    "llm_vs_prof_backtrans_chrf": 71.08581328661752,
    "llm_vs_prof_backtrans_bertscore": 0.8672909736633301,
    "llm_vs_prof_backtrans_labse": 0.8609400987625122
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 27.564269420539787,
    "same_lang_chrf": 52.80872490319272,
    "same_lang_bertscore": 0.787708044052124,
    "same_lang_comet": 0.745313286781311,
    "cross_lang_xlm_roberta": 0.972728431224823,
    "cross_lang_labse": 0.9961903095245361,
    "cross_lang_mbert": 0.9646227359771729,
    "cross_lang_comet_qe": 0.033810898661613464,
    "backtrans_bleu": 74.56474951913708,
    "backtrans_chrf": 87.07118500098345,
    "backtrans_bertscore": 0.939805805683136,
    "prof_backtrans_bleu": 35.184285126329314,
    "prof_backtrans_chrf": 69.07282493942381,
    "prof_backtrans_bertscore": 0.8681575655937195,
    "prof_backtrans_labse": 0.8649221658706665,
    "prof_backtrans_xlm_roberta": 0.6818556189537048,
    "llm_vs_prof_backtrans_bleu": 37.40283312607024,
    "llm_vs_prof_backtrans_chrf": 62.52472065807952,
    "llm_vs_prof_backtrans_bertscore": 0.8740071654319763,
    "llm_vs_prof_backtrans_labse": 0.8683934807777405
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 50.031767309271125,
    "same_lang_chrf": 73.39881696830687,
    "same_lang_bertscore": 0.8429989218711853,
    "same_lang_comet": 0.8371515274047852,
    "cross_lang_xlm_roberta": 0.9216591119766235,
    "cross_lang_labse": 0.9218003153800964,
    "cross_lang_mbert": 0.8929759860038757,
    "cross_lang_comet_qe": 0.28380557894706726,
    "backtrans_bleu": 73.03086218775081,
    "backtrans_chrf": 87.10676354023899,
    "backtrans_bertscore": 0.9031533002853394,
    "prof_backtrans_bleu": 46.749403960404806,
    "prof_backtrans_chrf": 78.12798427071783,
    "prof_backtrans_bertscore": 0.8902981281280518,
    "prof_backtrans_labse": 0.9479467868804932,
    "prof_backtrans_xlm_roberta": 0.9340266585350037,
    "llm_vs_prof_backtrans_bleu": 51.14208786357827,
    "llm_vs_prof_backtrans_chrf": 73.44436510867732,
    "llm_vs_prof_backtrans_bertscore": 0.8735954761505127,
    "llm_vs_prof_backtrans_labse": 0.8996437191963196
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 21.34155645477518,
    "same_lang_chrf": 41.86773859378937,
    "same_lang_bertscore": 0.8216022849082947,
    "same_lang_comet": 0.8668918609619141,
    "cross_lang_xlm_roberta": 0.9337779879570007,
    "cross_lang_labse": 0.8696450591087341,
    "cross_lang_mbert": 0.8827642202377319,
    "cross_lang_comet_qe": 0.2583148777484894,
    "backtrans_bleu": 60.8609672912482,
    "backtrans_chrf": 84.21328172930706,
    "backtrans_bertscore": 0.882108747959137,
    "prof_backtrans_bleu": 39.61336444505307,
    "prof_backtrans_chrf": 74.08039538718765,
    "prof_backtrans_bertscore": 0.8823897242546082,
    "prof_backtrans_labse": 0.8750954270362854,
    "prof_backtrans_xlm_roberta": 0.7492380142211914,
    "llm_vs_prof_backtrans_bleu": 43.85084106392551,
    "llm_vs_prof_backtrans_chrf": 69.55551095091364,
    "llm_vs_prof_backtrans_bertscore": 0.9304384589195251,
    "llm_vs_prof_backtrans_labse": 0.9339376091957092
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 53.46471223415591,
    "same_lang_chrf": 68.73004182848007,
    "same_lang_bertscore": 0.8485792279243469,
    "same_lang_comet": 0.8758459091186523,
    "cross_lang_xlm_roberta": 0.9023296236991882,
    "cross_lang_labse": 0.9281949996948242,
    "cross_lang_mbert": 0.8970223069190979,
    "cross_lang_comet_qe": 0.2831043004989624,
    "backtrans_bleu": 75.93287277363446,
    "backtrans_chrf": 88.86793137031947,
    "backtrans_bertscore": 0.8851301670074463,
    "prof_backtrans_bleu": 50.46495297663124,
    "prof_backtrans_chrf": 78.8483864856096,
    "prof_backtrans_bertscore": 0.9183284640312195,
    "prof_backtrans_labse": 0.9449689388275146,
    "prof_backtrans_xlm_roberta": 0.7806548476219177,
    "llm_vs_prof_backtrans_bleu": 51.16979496576119,
    "llm_vs_prof_backtrans_chrf": 74.06404151845057,
    "llm_vs_prof_backtrans_bertscore": 0.8750066757202148,
    "llm_vs_prof_backtrans_labse": 0.9065101146697998
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 52.480266939041186,
    "same_lang_chrf": 75.71884890768611,
    "same_lang_bertscore": 0.8251201510429382,
    "same_lang_comet": 0.8739067316055298,
    "cross_lang_xlm_roberta": 0.9048469662666321,
    "cross_lang_labse": 0.8707970380783081,
    "cross_lang_mbert": 0.912739098072052,
    "cross_lang_comet_qe": 0.30587634444236755,
    "backtrans_bleu": 69.18279283765702,
    "backtrans_chrf": 86.49506651282246,
    "backtrans_bertscore": 0.886311411857605,
    "prof_backtrans_bleu": 65.35485455542852,
    "prof_backtrans_chrf": 85.18826978419365,
    "prof_backtrans_bertscore": 0.8919525742530823,
    "prof_backtrans_labse": 0.8924142718315125,
    "prof_backtrans_xlm_roberta": 0.8814374804496765,
    "llm_vs_prof_backtrans_bleu": 71.77701805506629,
    "llm_vs_prof_backtrans_chrf": 83.8126837604243,
    "llm_vs_prof_backtrans_bertscore": 0.9321908950805664,
    "llm_vs_prof_backtrans_labse": 0.9230868220329285
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 31.84621129848625,
    "same_lang_chrf": 64.23689671850565,
    "same_lang_bertscore": 0.8261532783508301,
    "same_lang_comet": 0.8697662353515625,
    "cross_lang_xlm_roberta": 0.8969348669052124,
    "cross_lang_labse": 0.922472357749939,
    "cross_lang_mbert": 0.8878235816955566,
    "cross_lang_comet_qe": 0.3007923364639282,
    "backtrans_bleu": 57.79665663607917,
    "backtrans_chrf": 80.46994282770743,
    "backtrans_bertscore": 0.8856286406517029,
    "prof_backtrans_bleu": 33.34266399464895,
    "prof_backtrans_chrf": 67.98063146352926,
    "prof_backtrans_bertscore": 0.8779526352882385,
    "prof_backtrans_labse": 0.8939875960350037,
    "prof_backtrans_xlm_roberta": 0.7960735559463501,
    "llm_vs_prof_backtrans_bleu": 40.4978505307437,
    "llm_vs_prof_backtrans_chrf": 63.56177791855342,
    "llm_vs_prof_backtrans_bertscore": 0.9057382941246033,
    "llm_vs_prof_backtrans_labse": 0.9390409588813782
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 9.909739774299778,
    "same_lang_chrf": 27.638686866213234,
    "same_lang_bertscore": 0.7263541221618652,
    "same_lang_comet": 0.8374332189559937,
    "cross_lang_xlm_roberta": 0.9276207089424133,
    "cross_lang_labse": 0.8787704706192017,
    "cross_lang_mbert": 0.9165352582931519,
    "cross_lang_comet_qe": 0.2888624966144562,
    "backtrans_bleu": 67.11531598486842,
    "backtrans_chrf": 85.21581873786067,
    "backtrans_bertscore": 0.8857877850532532,
    "prof_backtrans_bleu": 14.524954611565638,
    "prof_backtrans_chrf": 55.12834663128212,
    "prof_backtrans_bertscore": 0.8357766270637512,
    "prof_backtrans_labse": 0.8260661363601685,
    "prof_backtrans_xlm_roberta": 0.699259877204895,
    "llm_vs_prof_backtrans_bleu": 14.254327050553687,
    "llm_vs_prof_backtrans_chrf": 46.95825267854421,
    "llm_vs_prof_backtrans_bertscore": 0.8256375789642334,
    "llm_vs_prof_backtrans_labse": 0.7658678293228149
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 30.66870359958256,
    "same_lang_chrf": 58.16940336210872,
    "same_lang_bertscore": 0.8060110807418823,
    "same_lang_comet": 0.7881152629852295,
    "cross_lang_xlm_roberta": 0.9535111784934998,
    "cross_lang_labse": 0.9106085300445557,
    "cross_lang_mbert": 0.913383960723877,
    "cross_lang_comet_qe": 0.26456624269485474,
    "backtrans_bleu": 55.945080673845936,
    "backtrans_chrf": 78.39411835417638,
    "backtrans_bertscore": 0.888434648513794,
    "prof_backtrans_bleu": 28.976391830972165,
    "prof_backtrans_chrf": 66.20868632987252,
    "prof_backtrans_bertscore": 0.8612502813339233,
    "prof_backtrans_labse": 0.8579159379005432,
    "prof_backtrans_xlm_roberta": 0.6919851899147034,
    "llm_vs_prof_backtrans_bleu": 35.187900085180154,
    "llm_vs_prof_backtrans_chrf": 61.95535098558695,
    "llm_vs_prof_backtrans_bertscore": 0.8975563049316406,
    "llm_vs_prof_backtrans_labse": 0.8669415712356567
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 44.92454758860946,
    "same_lang_chrf": 69.48192747675242,
    "same_lang_bertscore": 0.8379918932914734,
    "same_lang_comet": 0.8396893739700317,
    "cross_lang_xlm_roberta": 0.9227319359779358,
    "cross_lang_labse": 0.9191779494285583,
    "cross_lang_mbert": 0.9079201817512512,
    "cross_lang_comet_qe": 0.3028790354728699,
    "backtrans_bleu": 65.48894293599491,
    "backtrans_chrf": 83.08725007994478,
    "backtrans_bertscore": 0.8842207789421082,
    "prof_backtrans_bleu": 48.99617920433222,
    "prof_backtrans_chrf": 75.038967026109,
    "prof_backtrans_bertscore": 0.8984673619270325,
    "prof_backtrans_labse": 0.9455118775367737,
    "prof_backtrans_xlm_roberta": 0.9383783340454102,
    "llm_vs_prof_backtrans_bleu": 58.178195984829834,
    "llm_vs_prof_backtrans_chrf": 76.25902163068365,
    "llm_vs_prof_backtrans_bertscore": 0.8934802412986755,
    "llm_vs_prof_backtrans_labse": 0.8901763558387756
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 20.82743847279537,
    "same_lang_chrf": 41.30890413437555,
    "same_lang_bertscore": 0.8549056053161621,
    "same_lang_comet": 0.8706996440887451,
    "cross_lang_xlm_roberta": 0.9775985479354858,
    "cross_lang_labse": 0.9856840372085571,
    "cross_lang_mbert": 0.9491397738456726,
    "cross_lang_comet_qe": 0.14115332067012787,
    "backtrans_bleu": 51.49969478050373,
    "backtrans_chrf": 79.25124929392545,
    "backtrans_bertscore": 0.9028792977333069,
    "prof_backtrans_bleu": 43.29432998939322,
    "prof_backtrans_chrf": 75.77359738728643,
    "prof_backtrans_bertscore": 0.8937182426452637,
    "prof_backtrans_labse": 0.9235764741897583,
    "prof_backtrans_xlm_roberta": 0.7712130546569824,
    "llm_vs_prof_backtrans_bleu": 51.432124806953006,
    "llm_vs_prof_backtrans_chrf": 76.17840379028911,
    "llm_vs_prof_backtrans_bertscore": 0.8950656056404114,
    "llm_vs_prof_backtrans_labse": 0.9302605390548706
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 47.00100521138113,
    "same_lang_chrf": 62.58772828766378,
    "same_lang_bertscore": 0.8311575055122375,
    "same_lang_comet": 0.872417688369751,
    "cross_lang_xlm_roberta": 0.9068834185600281,
    "cross_lang_labse": 0.9160254001617432,
    "cross_lang_mbert": 0.8760941028594971,
    "cross_lang_comet_qe": 0.29969730973243713,
    "backtrans_bleu": 59.76576615507276,
    "backtrans_chrf": 78.80366544041945,
    "backtrans_bertscore": 0.8766562938690186,
    "prof_backtrans_bleu": 59.283329667687546,
    "prof_backtrans_chrf": 80.51929358258501,
    "prof_backtrans_bertscore": 0.9039437770843506,
    "prof_backtrans_labse": 0.9155372977256775,
    "prof_backtrans_xlm_roberta": 0.7772224545478821,
    "llm_vs_prof_backtrans_bleu": 59.49675486426614,
    "llm_vs_prof_backtrans_chrf": 76.03359559132535,
    "llm_vs_prof_backtrans_bertscore": 0.8933025598526001,
    "llm_vs_prof_backtrans_labse": 0.9456971883773804
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 42.61833389148107,
    "same_lang_chrf": 66.67594442742111,
    "same_lang_bertscore": 0.8291262984275818,
    "same_lang_comet": 0.8688472509384155,
    "cross_lang_xlm_roberta": 0.953264057636261,
    "cross_lang_labse": 0.9171953797340393,
    "cross_lang_mbert": 0.9248378872871399,
    "cross_lang_comet_qe": 0.2282252013683319,
    "backtrans_bleu": 64.62827869043731,
    "backtrans_chrf": 83.46806853484432,
    "backtrans_bertscore": 0.8893285393714905,
    "prof_backtrans_bleu": 61.33116459736083,
    "prof_backtrans_chrf": 83.2060199935787,
    "prof_backtrans_bertscore": 0.8925575613975525,
    "prof_backtrans_labse": 0.8970524668693542,
    "prof_backtrans_xlm_roberta": 0.8840277194976807,
    "llm_vs_prof_backtrans_bleu": 67.51090273167284,
    "llm_vs_prof_backtrans_chrf": 81.33058491278055,
    "llm_vs_prof_backtrans_bertscore": 0.9422584772109985,
    "llm_vs_prof_backtrans_labse": 0.9583254456520081
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 28.996498464574493,
    "same_lang_chrf": 60.209769802293366,
    "same_lang_bertscore": 0.8090987205505371,
    "same_lang_comet": 0.8651365041732788,
    "cross_lang_xlm_roberta": 0.9443339109420776,
    "cross_lang_labse": 0.9131231307983398,
    "cross_lang_mbert": 0.9178774952888489,
    "cross_lang_comet_qe": 0.26790860295295715,
    "backtrans_bleu": 56.80126699419882,
    "backtrans_chrf": 77.7710754477808,
    "backtrans_bertscore": 0.8859697580337524,
    "prof_backtrans_bleu": 30.48187680735326,
    "prof_backtrans_chrf": 66.61819283969044,
    "prof_backtrans_bertscore": 0.889682948589325,
    "prof_backtrans_labse": 0.9347479343414307,
    "prof_backtrans_xlm_roberta": 0.785839855670929,
    "llm_vs_prof_backtrans_bleu": 38.60140154785403,
    "llm_vs_prof_backtrans_chrf": 63.42390977303714,
    "llm_vs_prof_backtrans_bertscore": 0.8788355588912964,
    "llm_vs_prof_backtrans_labse": 0.8798723220825195
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 10.728601823308798,
    "same_lang_chrf": 29.308052063527928,
    "same_lang_bertscore": 0.7386425733566284,
    "same_lang_comet": 0.8399507999420166,
    "cross_lang_xlm_roberta": 0.9726705551147461,
    "cross_lang_labse": 0.940929651260376,
    "cross_lang_mbert": 0.9447433352470398,
    "cross_lang_comet_qe": 0.2673588693141937,
    "backtrans_bleu": 54.338331038791594,
    "backtrans_chrf": 79.46983994371844,
    "backtrans_bertscore": 0.8966261744499207,
    "prof_backtrans_bleu": 26.47996030812535,
    "prof_backtrans_chrf": 64.63096373284316,
    "prof_backtrans_bertscore": 0.8640876412391663,
    "prof_backtrans_labse": 0.821840763092041,
    "prof_backtrans_xlm_roberta": 0.6732886433601379,
    "llm_vs_prof_backtrans_bleu": 29.732994799382332,
    "llm_vs_prof_backtrans_chrf": 61.45176720289829,
    "llm_vs_prof_backtrans_bertscore": 0.871104896068573,
    "llm_vs_prof_backtrans_labse": 0.7968915700912476
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 25.75586238446983,
    "same_lang_chrf": 51.56387522851637,
    "same_lang_bertscore": 0.7798609137535095,
    "same_lang_comet": 0.8135080337524414,
    "cross_lang_xlm_roberta": 0.9400310516357422,
    "cross_lang_labse": 0.9200513362884521,
    "cross_lang_mbert": 0.915161669254303,
    "cross_lang_comet_qe": 0.22601740062236786,
    "backtrans_bleu": 56.87374131325288,
    "backtrans_chrf": 79.23052512867301,
    "backtrans_bertscore": 0.8935391902923584,
    "prof_backtrans_bleu": 34.08849229974469,
    "prof_backtrans_chrf": 63.956334223876596,
    "prof_backtrans_bertscore": 0.8669067025184631,
    "prof_backtrans_labse": 0.8433675765991211,
    "prof_backtrans_xlm_roberta": 0.6863886713981628,
    "llm_vs_prof_backtrans_bleu": 39.46179661678853,
    "llm_vs_prof_backtrans_chrf": 63.99874008259484,
    "llm_vs_prof_backtrans_bertscore": 0.8947076201438904,
    "llm_vs_prof_backtrans_labse": 0.8013491630554199
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 50.83376480593194,
    "same_lang_chrf": 76.42558169277784,
    "same_lang_bertscore": 0.8061307072639465,
    "same_lang_comet": 0.8064068555831909,
    "cross_lang_xlm_roberta": 0.9439987540245056,
    "cross_lang_labse": 0.9101309776306152,
    "cross_lang_mbert": 0.917767345905304,
    "cross_lang_comet_qe": 0.09921324253082275,
    "backtrans_bleu": 65.23784302161569,
    "backtrans_chrf": 84.84042313709296,
    "backtrans_bertscore": 0.8757258653640747,
    "prof_backtrans_bleu": 48.49227279275704,
    "prof_backtrans_chrf": 76.94986797032534,
    "prof_backtrans_bertscore": 0.8870509266853333,
    "prof_backtrans_labse": 0.9211959838867188,
    "prof_backtrans_xlm_roberta": 0.9163268804550171,
    "llm_vs_prof_backtrans_bleu": 58.643245248719715,
    "llm_vs_prof_backtrans_chrf": 79.30734821689971,
    "llm_vs_prof_backtrans_bertscore": 0.8699095249176025,
    "llm_vs_prof_backtrans_labse": 0.8401374816894531
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.350532914921155,
    "same_lang_chrf": 42.673452520273656,
    "same_lang_bertscore": 0.7657800316810608,
    "same_lang_comet": 0.8185027837753296,
    "cross_lang_xlm_roberta": 0.948745846748352,
    "cross_lang_labse": 0.8939049243927002,
    "cross_lang_mbert": 0.921862781047821,
    "cross_lang_comet_qe": 0.07984521239995956,
    "backtrans_bleu": 59.770593579298954,
    "backtrans_chrf": 81.24288493140355,
    "backtrans_bertscore": 0.8719358444213867,
    "prof_backtrans_bleu": 47.66299716800662,
    "prof_backtrans_chrf": 77.48890230668894,
    "prof_backtrans_bertscore": 0.864867091178894,
    "prof_backtrans_labse": 0.8808958530426025,
    "prof_backtrans_xlm_roberta": 0.7010477781295776,
    "llm_vs_prof_backtrans_bleu": 52.12749164711168,
    "llm_vs_prof_backtrans_chrf": 74.43901619537385,
    "llm_vs_prof_backtrans_bertscore": 0.8823696970939636,
    "llm_vs_prof_backtrans_labse": 0.8380900621414185
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 40.42784453974207,
    "same_lang_chrf": 57.060259489221274,
    "same_lang_bertscore": 0.7872139811515808,
    "same_lang_comet": 0.8327456712722778,
    "cross_lang_xlm_roberta": 0.957073450088501,
    "cross_lang_labse": 0.9055257439613342,
    "cross_lang_mbert": 0.9374067187309265,
    "cross_lang_comet_qe": 0.10196515917778015,
    "backtrans_bleu": 72.30937777938946,
    "backtrans_chrf": 87.81736078753548,
    "backtrans_bertscore": 0.8839379549026489,
    "prof_backtrans_bleu": 58.522967092966105,
    "prof_backtrans_chrf": 80.63352433871724,
    "prof_backtrans_bertscore": 0.9030929207801819,
    "prof_backtrans_labse": 0.8889848589897156,
    "prof_backtrans_xlm_roberta": 0.6821778416633606,
    "llm_vs_prof_backtrans_bleu": 63.149881952474026,
    "llm_vs_prof_backtrans_chrf": 80.2078631818562,
    "llm_vs_prof_backtrans_bertscore": 0.8620498776435852,
    "llm_vs_prof_backtrans_labse": 0.7813159227371216
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 39.081030602324475,
    "same_lang_chrf": 68.59364015767075,
    "same_lang_bertscore": 0.7724341154098511,
    "same_lang_comet": 0.8218209147453308,
    "cross_lang_xlm_roberta": 0.9631557464599609,
    "cross_lang_labse": 0.9083613753318787,
    "cross_lang_mbert": 0.9377209544181824,
    "cross_lang_comet_qe": 0.07477518916130066,
    "backtrans_bleu": 68.99503995214181,
    "backtrans_chrf": 85.95463485662232,
    "backtrans_bertscore": 0.8785949349403381,
    "prof_backtrans_bleu": 57.36040209167606,
    "prof_backtrans_chrf": 82.65171174648498,
    "prof_backtrans_bertscore": 0.8979630470275879,
    "prof_backtrans_labse": 0.9146618843078613,
    "prof_backtrans_xlm_roberta": 0.7965490818023682,
    "llm_vs_prof_backtrans_bleu": 59.800809968910045,
    "llm_vs_prof_backtrans_chrf": 79.012748767202,
    "llm_vs_prof_backtrans_bertscore": 0.8635103702545166,
    "llm_vs_prof_backtrans_labse": 0.8397036194801331
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 33.08222166460663,
    "same_lang_chrf": 62.0071879469925,
    "same_lang_bertscore": 0.7863392233848572,
    "same_lang_comet": 0.8343433737754822,
    "cross_lang_xlm_roberta": 0.957539975643158,
    "cross_lang_labse": 0.9023917317390442,
    "cross_lang_mbert": 0.9146712422370911,
    "cross_lang_comet_qe": 0.11425669491291046,
    "backtrans_bleu": 62.444834489303766,
    "backtrans_chrf": 82.4570475059923,
    "backtrans_bertscore": 0.8762529492378235,
    "prof_backtrans_bleu": 43.78637581636294,
    "prof_backtrans_chrf": 74.82340537831666,
    "prof_backtrans_bertscore": 0.8847045302391052,
    "prof_backtrans_labse": 0.8944453001022339,
    "prof_backtrans_xlm_roberta": 0.7062325477600098,
    "llm_vs_prof_backtrans_bleu": 53.29278565037435,
    "llm_vs_prof_backtrans_chrf": 74.29565639259297,
    "llm_vs_prof_backtrans_bertscore": 0.8502399325370789,
    "llm_vs_prof_backtrans_labse": 0.8188190460205078
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 29.098475391870597,
    "same_lang_chrf": 48.199008400883784,
    "same_lang_bertscore": 0.7928947806358337,
    "same_lang_comet": 0.8435600399971008,
    "cross_lang_xlm_roberta": 0.9510675668716431,
    "cross_lang_labse": 0.9010618925094604,
    "cross_lang_mbert": 0.9463269710540771,
    "cross_lang_comet_qe": 0.0995788499712944,
    "backtrans_bleu": 54.29875364625226,
    "backtrans_chrf": 77.81430551083581,
    "backtrans_bertscore": 0.8680504560470581,
    "prof_backtrans_bleu": 47.94007827440107,
    "prof_backtrans_chrf": 77.17948185868693,
    "prof_backtrans_bertscore": 0.8908801078796387,
    "prof_backtrans_labse": 0.8855347633361816,
    "prof_backtrans_xlm_roberta": 0.7738862633705139,
    "llm_vs_prof_backtrans_bleu": 52.24659127969228,
    "llm_vs_prof_backtrans_chrf": 75.57500044462066,
    "llm_vs_prof_backtrans_bertscore": 0.8568302989006042,
    "llm_vs_prof_backtrans_labse": 0.7978764176368713
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 29.272655489409722,
    "same_lang_chrf": 57.78834404027508,
    "same_lang_bertscore": 0.7882444858551025,
    "same_lang_comet": 0.7827668190002441,
    "cross_lang_xlm_roberta": 0.9561111330986023,
    "cross_lang_labse": 0.9029468894004822,
    "cross_lang_mbert": 0.921535849571228,
    "cross_lang_comet_qe": 0.0905359610915184,
    "backtrans_bleu": 51.980427882866785,
    "backtrans_chrf": 77.56337715195308,
    "backtrans_bertscore": 0.8800718784332275,
    "prof_backtrans_bleu": 45.37789627954331,
    "prof_backtrans_chrf": 75.53393891271874,
    "prof_backtrans_bertscore": 0.8943527340888977,
    "prof_backtrans_labse": 0.9020337462425232,
    "prof_backtrans_xlm_roberta": 0.6706964373588562,
    "llm_vs_prof_backtrans_bleu": 42.70777743494338,
    "llm_vs_prof_backtrans_chrf": 69.06975311723362,
    "llm_vs_prof_backtrans_bertscore": 0.8613271117210388,
    "llm_vs_prof_backtrans_labse": 0.7997796535491943
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 49.24745435266034,
    "same_lang_chrf": 75.47699580056052,
    "same_lang_bertscore": 0.846031129360199,
    "same_lang_comet": 0.8065557479858398,
    "cross_lang_xlm_roberta": 0.9795358777046204,
    "cross_lang_labse": 0.9892100691795349,
    "cross_lang_mbert": 0.9466160535812378,
    "cross_lang_comet_qe": -0.19043508172035217,
    "backtrans_bleu": 61.05713456918617,
    "backtrans_chrf": 82.41853535067617,
    "backtrans_bertscore": 0.9132239818572998,
    "prof_backtrans_bleu": 45.58305838057299,
    "prof_backtrans_chrf": 76.24691867943478,
    "prof_backtrans_bertscore": 0.8889009952545166,
    "prof_backtrans_labse": 0.9192685484886169,
    "prof_backtrans_xlm_roberta": 0.9158088564872742,
    "llm_vs_prof_backtrans_bleu": 54.4443584339513,
    "llm_vs_prof_backtrans_chrf": 76.99145490592778,
    "llm_vs_prof_backtrans_bertscore": 0.9013169407844543,
    "llm_vs_prof_backtrans_labse": 0.9105749726295471
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 11.254002674880754,
    "same_lang_chrf": 37.27724606820023,
    "same_lang_bertscore": 0.797416090965271,
    "same_lang_comet": 0.8255994915962219,
    "cross_lang_xlm_roberta": 0.9322294592857361,
    "cross_lang_labse": 0.9745696783065796,
    "cross_lang_mbert": 0.9464582800865173,
    "cross_lang_comet_qe": -0.10702241957187653,
    "backtrans_bleu": 67.02227961627494,
    "backtrans_chrf": 84.74159483491293,
    "backtrans_bertscore": 0.9006030559539795,
    "prof_backtrans_bleu": 60.842119513345665,
    "prof_backtrans_chrf": 82.46918695175832,
    "prof_backtrans_bertscore": 0.878322958946228,
    "prof_backtrans_labse": 0.8738146424293518,
    "prof_backtrans_xlm_roberta": 0.7051469683647156,
    "llm_vs_prof_backtrans_bleu": 64.05613268300282,
    "llm_vs_prof_backtrans_chrf": 80.96150099059271,
    "llm_vs_prof_backtrans_bertscore": 0.8910028338432312,
    "llm_vs_prof_backtrans_labse": 0.8608677983283997
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 45.13107958065127,
    "same_lang_chrf": 62.72512974563775,
    "same_lang_bertscore": 0.8351593613624573,
    "same_lang_comet": 0.8478591442108154,
    "cross_lang_xlm_roberta": 0.9836089611053467,
    "cross_lang_labse": 0.9828119277954102,
    "cross_lang_mbert": 0.9798179864883423,
    "cross_lang_comet_qe": 0.06564435362815857,
    "backtrans_bleu": 73.1438745018143,
    "backtrans_chrf": 88.43741121518634,
    "backtrans_bertscore": 0.9089694619178772,
    "prof_backtrans_bleu": 59.847707959762985,
    "prof_backtrans_chrf": 81.4946664087007,
    "prof_backtrans_bertscore": 0.901991069316864,
    "prof_backtrans_labse": 0.8806231021881104,
    "prof_backtrans_xlm_roberta": 0.6807250380516052,
    "llm_vs_prof_backtrans_bleu": 65.56622430844429,
    "llm_vs_prof_backtrans_chrf": 82.15364941341441,
    "llm_vs_prof_backtrans_bertscore": 0.8885021209716797,
    "llm_vs_prof_backtrans_labse": 0.8781557083129883
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 39.77816437828963,
    "same_lang_chrf": 72.5108263185136,
    "same_lang_bertscore": 0.8399489521980286,
    "same_lang_comet": 0.8545539379119873,
    "cross_lang_xlm_roberta": 0.9783637523651123,
    "cross_lang_labse": 0.9944306015968323,
    "cross_lang_mbert": 0.975318431854248,
    "cross_lang_comet_qe": -0.16384874284267426,
    "backtrans_bleu": 79.97040314525174,
    "backtrans_chrf": 90.94664107112635,
    "backtrans_bertscore": 0.9679184556007385,
    "prof_backtrans_bleu": 54.47327976970473,
    "prof_backtrans_chrf": 79.87292312489838,
    "prof_backtrans_bertscore": 0.8951845169067383,
    "prof_backtrans_labse": 0.9231014251708984,
    "prof_backtrans_xlm_roberta": 0.8241469860076904,
    "llm_vs_prof_backtrans_bleu": 58.94690187912822,
    "llm_vs_prof_backtrans_chrf": 78.93972972899347,
    "llm_vs_prof_backtrans_bertscore": 0.8987454771995544,
    "llm_vs_prof_backtrans_labse": 0.9172910451889038
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 30.920411381064515,
    "same_lang_chrf": 63.09739022028506,
    "same_lang_bertscore": 0.8404930830001831,
    "same_lang_comet": 0.8211528062820435,
    "cross_lang_xlm_roberta": 0.9943830370903015,
    "cross_lang_labse": 0.9844664931297302,
    "cross_lang_mbert": 0.9801844358444214,
    "cross_lang_comet_qe": -0.2175731658935547,
    "backtrans_bleu": 62.24482768591868,
    "backtrans_chrf": 82.79066846774649,
    "backtrans_bertscore": 0.9314712882041931,
    "prof_backtrans_bleu": 37.94873596972064,
    "prof_backtrans_chrf": 72.96038048123968,
    "prof_backtrans_bertscore": 0.8848487138748169,
    "prof_backtrans_labse": 0.9005881547927856,
    "prof_backtrans_xlm_roberta": 0.7062927484512329,
    "llm_vs_prof_backtrans_bleu": 41.64767205602256,
    "llm_vs_prof_backtrans_chrf": 69.52298662414715,
    "llm_vs_prof_backtrans_bertscore": 0.8878650069236755,
    "llm_vs_prof_backtrans_labse": 0.8991251587867737
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 27.25339729158006,
    "same_lang_chrf": 43.675775409017795,
    "same_lang_bertscore": 0.8170232772827148,
    "same_lang_comet": 0.8323702812194824,
    "cross_lang_xlm_roberta": 0.9568843245506287,
    "cross_lang_labse": 0.981350302696228,
    "cross_lang_mbert": 0.9306305646896362,
    "cross_lang_comet_qe": -0.2254352867603302,
    "backtrans_bleu": 67.12406970122234,
    "backtrans_chrf": 84.17570631580601,
    "backtrans_bertscore": 0.9119755029678345,
    "prof_backtrans_bleu": 48.21961027145671,
    "prof_backtrans_chrf": 76.52734725779699,
    "prof_backtrans_bertscore": 0.8892796039581299,
    "prof_backtrans_labse": 0.8913945555686951,
    "prof_backtrans_xlm_roberta": 0.7705687880516052,
    "llm_vs_prof_backtrans_bleu": 49.74636123768917,
    "llm_vs_prof_backtrans_chrf": 75.0706731936759,
    "llm_vs_prof_backtrans_bertscore": 0.8820369839668274,
    "llm_vs_prof_backtrans_labse": 0.8693323731422424
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 26.724517442862684,
    "same_lang_chrf": 57.33604828515252,
    "same_lang_bertscore": 0.8020762205123901,
    "same_lang_comet": 0.81443852186203,
    "cross_lang_xlm_roberta": 0.9406381249427795,
    "cross_lang_labse": 0.8999730348587036,
    "cross_lang_mbert": 0.9144159555435181,
    "cross_lang_comet_qe": 0.05459146201610565,
    "backtrans_bleu": 70.97720654947408,
    "backtrans_chrf": 86.30606420118781,
    "backtrans_bertscore": 0.8745489716529846,
    "prof_backtrans_bleu": 42.01350746029041,
    "prof_backtrans_chrf": 74.14465618320732,
    "prof_backtrans_bertscore": 0.8946902751922607,
    "prof_backtrans_labse": 0.9024530649185181,
    "prof_backtrans_xlm_roberta": 0.6736341118812561,
    "llm_vs_prof_backtrans_bleu": 47.33322150553875,
    "llm_vs_prof_backtrans_chrf": 69.85764712967274,
    "llm_vs_prof_backtrans_bertscore": 0.8740603923797607,
    "llm_vs_prof_backtrans_labse": 0.7910772562026978
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 43.079186845342996,
    "same_lang_chrf": 73.97804256250116,
    "same_lang_bertscore": 0.784740686416626,
    "same_lang_comet": 0.7874174118041992,
    "cross_lang_xlm_roberta": 0.9202379584312439,
    "cross_lang_labse": 0.8526055812835693,
    "cross_lang_mbert": 0.8440143465995789,
    "cross_lang_comet_qe": 0.12785688042640686,
    "backtrans_bleu": 52.37415497825861,
    "backtrans_chrf": 82.15017653694773,
    "backtrans_bertscore": 0.860884964466095,
    "prof_backtrans_bleu": 40.649406619262166,
    "prof_backtrans_chrf": 73.32682198609113,
    "prof_backtrans_bertscore": 0.8800420761108398,
    "prof_backtrans_labse": 0.9174806475639343,
    "prof_backtrans_xlm_roberta": 0.9240977764129639,
    "llm_vs_prof_backtrans_bleu": 43.49867697400964,
    "llm_vs_prof_backtrans_chrf": 71.19343102845393,
    "llm_vs_prof_backtrans_bertscore": 0.8569380044937134,
    "llm_vs_prof_backtrans_labse": 0.7690104842185974
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.331793106904893,
    "same_lang_chrf": 38.26303742501491,
    "same_lang_bertscore": 0.7570875883102417,
    "same_lang_comet": 0.8155815005302429,
    "cross_lang_xlm_roberta": 0.9057294726371765,
    "cross_lang_labse": 0.8729832172393799,
    "cross_lang_mbert": 0.8440020680427551,
    "cross_lang_comet_qe": 0.10528440028429031,
    "backtrans_bleu": 73.37575145360341,
    "backtrans_chrf": 87.69353766080334,
    "backtrans_bertscore": 0.8622502684593201,
    "prof_backtrans_bleu": 32.781762088312334,
    "prof_backtrans_chrf": 70.29223449266917,
    "prof_backtrans_bertscore": 0.8630768060684204,
    "prof_backtrans_labse": 0.8436294794082642,
    "prof_backtrans_xlm_roberta": 0.7012219429016113,
    "llm_vs_prof_backtrans_bleu": 35.38668864836514,
    "llm_vs_prof_backtrans_chrf": 64.26276896497224,
    "llm_vs_prof_backtrans_bertscore": 0.8413993120193481,
    "llm_vs_prof_backtrans_labse": 0.7529008984565735
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 48.184887642371315,
    "same_lang_chrf": 64.4805603945934,
    "same_lang_bertscore": 0.7885476350784302,
    "same_lang_comet": 0.8251021504402161,
    "cross_lang_xlm_roberta": 0.864457368850708,
    "cross_lang_labse": 0.8903539776802063,
    "cross_lang_mbert": 0.8577994108200073,
    "cross_lang_comet_qe": 0.12844306230545044,
    "backtrans_bleu": 72.60551577188912,
    "backtrans_chrf": 88.07294041760125,
    "backtrans_bertscore": 0.8676705360412598,
    "prof_backtrans_bleu": 46.8052894647429,
    "prof_backtrans_chrf": 75.51097271152263,
    "prof_backtrans_bertscore": 0.8608230948448181,
    "prof_backtrans_labse": 0.8731796145439148,
    "prof_backtrans_xlm_roberta": 0.6749367713928223,
    "llm_vs_prof_backtrans_bleu": 55.16538927184669,
    "llm_vs_prof_backtrans_chrf": 74.75489450576687,
    "llm_vs_prof_backtrans_bertscore": 0.9151939153671265,
    "llm_vs_prof_backtrans_labse": 0.898459792137146
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 35.31854590905707,
    "same_lang_chrf": 68.0500521689751,
    "same_lang_bertscore": 0.7672373056411743,
    "same_lang_comet": 0.82239830493927,
    "cross_lang_xlm_roberta": 0.9217596054077148,
    "cross_lang_labse": 0.8554611206054688,
    "cross_lang_mbert": 0.8565619587898254,
    "cross_lang_comet_qe": 0.12942156195640564,
    "backtrans_bleu": 61.073089549785074,
    "backtrans_chrf": 85.51501311823787,
    "backtrans_bertscore": 0.8622615933418274,
    "prof_backtrans_bleu": 57.81351293682067,
    "prof_backtrans_chrf": 82.44975163277263,
    "prof_backtrans_bertscore": 0.8669204115867615,
    "prof_backtrans_labse": 0.8912836313247681,
    "prof_backtrans_xlm_roberta": 0.8037905097007751,
    "llm_vs_prof_backtrans_bleu": 61.00389663444372,
    "llm_vs_prof_backtrans_chrf": 78.47107218247413,
    "llm_vs_prof_backtrans_bertscore": 0.9051458835601807,
    "llm_vs_prof_backtrans_labse": 0.8855699896812439
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 34.57336689836913,
    "same_lang_chrf": 65.45618496654514,
    "same_lang_bertscore": 0.7784667611122131,
    "same_lang_comet": 0.8363085985183716,
    "cross_lang_xlm_roberta": 0.917862057685852,
    "cross_lang_labse": 0.8540687561035156,
    "cross_lang_mbert": 0.8666159510612488,
    "cross_lang_comet_qe": 0.1360701471567154,
    "backtrans_bleu": 47.8813808383959,
    "backtrans_chrf": 73.63024912380544,
    "backtrans_bertscore": 0.8605337142944336,
    "prof_backtrans_bleu": 33.12081568822247,
    "prof_backtrans_chrf": 69.17071304382209,
    "prof_backtrans_bertscore": 0.8810473084449768,
    "prof_backtrans_labse": 0.9028347730636597,
    "prof_backtrans_xlm_roberta": 0.7062927484512329,
    "llm_vs_prof_backtrans_bleu": 31.26933645477933,
    "llm_vs_prof_backtrans_chrf": 59.34102878685938,
    "llm_vs_prof_backtrans_bertscore": 0.8463951349258423,
    "llm_vs_prof_backtrans_labse": 0.8036223649978638
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 24.87555352999559,
    "same_lang_chrf": 45.12651038148748,
    "same_lang_bertscore": 0.7761491537094116,
    "same_lang_comet": 0.8440686464309692,
    "cross_lang_xlm_roberta": 0.9132804274559021,
    "cross_lang_labse": 0.8677321672439575,
    "cross_lang_mbert": 0.8490703105926514,
    "cross_lang_comet_qe": 0.06816374510526657,
    "backtrans_bleu": 55.44157682738649,
    "backtrans_chrf": 80.91466617077663,
    "backtrans_bertscore": 0.8581860065460205,
    "prof_backtrans_bleu": 35.24320521893236,
    "prof_backtrans_chrf": 69.7379547616088,
    "prof_backtrans_bertscore": 0.8789343237876892,
    "prof_backtrans_labse": 0.8811438083648682,
    "prof_backtrans_xlm_roberta": 0.7722858190536499,
    "llm_vs_prof_backtrans_bleu": 32.60258247291479,
    "llm_vs_prof_backtrans_chrf": 64.5897747175527,
    "llm_vs_prof_backtrans_bertscore": 0.8483392000198364,
    "llm_vs_prof_backtrans_labse": 0.7600351572036743
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 29.75659488553856,
    "same_lang_chrf": 59.52518524731628,
    "same_lang_bertscore": 0.793289840221405,
    "same_lang_comet": 0.8201382160186768,
    "cross_lang_xlm_roberta": 0.920637845993042,
    "cross_lang_labse": 0.8911896347999573,
    "cross_lang_mbert": 0.8784424066543579,
    "cross_lang_comet_qe": 0.11060729622840881,
    "backtrans_bleu": 56.849353887096974,
    "backtrans_chrf": 80.22699415768244,
    "backtrans_bertscore": 0.8638682961463928,
    "prof_backtrans_bleu": 34.62652641018995,
    "prof_backtrans_chrf": 70.14647533023098,
    "prof_backtrans_bertscore": 0.8590128421783447,
    "prof_backtrans_labse": 0.8620063662528992,
    "prof_backtrans_xlm_roberta": 0.7181593179702759,
    "llm_vs_prof_backtrans_bleu": 40.5255532769053,
    "llm_vs_prof_backtrans_chrf": 65.02044227101325,
    "llm_vs_prof_backtrans_bertscore": 0.9184005856513977,
    "llm_vs_prof_backtrans_labse": 0.8988572955131531
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 51.71084305416518,
    "same_lang_chrf": 75.73685477947622,
    "same_lang_bertscore": 0.8107283711433411,
    "same_lang_comet": 0.7929285168647766,
    "cross_lang_xlm_roberta": 0.9253427982330322,
    "cross_lang_labse": 0.8783388733863831,
    "cross_lang_mbert": 0.8593298196792603,
    "cross_lang_comet_qe": 0.1337646096944809,
    "backtrans_bleu": 57.270138094545544,
    "backtrans_chrf": 78.86970117045335,
    "backtrans_bertscore": 0.8575866222381592,
    "prof_backtrans_bleu": 45.1433776797273,
    "prof_backtrans_chrf": 75.2383236516982,
    "prof_backtrans_bertscore": 0.8714402318000793,
    "prof_backtrans_labse": 0.9233002662658691,
    "prof_backtrans_xlm_roberta": 0.927643358707428,
    "llm_vs_prof_backtrans_bleu": 59.03468374345715,
    "llm_vs_prof_backtrans_chrf": 76.29820949036818,
    "llm_vs_prof_backtrans_bertscore": 0.8866288065910339,
    "llm_vs_prof_backtrans_labse": 0.9165475368499756
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 6.874774595034904,
    "same_lang_chrf": 37.347159385055186,
    "same_lang_bertscore": 0.7636567950248718,
    "same_lang_comet": 0.8130565881729126,
    "cross_lang_xlm_roberta": 0.9228171706199646,
    "cross_lang_labse": 0.8660441637039185,
    "cross_lang_mbert": 0.883826732635498,
    "cross_lang_comet_qe": 0.18782548606395721,
    "backtrans_bleu": 45.17525008157478,
    "backtrans_chrf": 73.81530225762816,
    "backtrans_bertscore": 0.8631287217140198,
    "prof_backtrans_bleu": 48.39195514056748,
    "prof_backtrans_chrf": 75.59014552943425,
    "prof_backtrans_bertscore": 0.864355206489563,
    "prof_backtrans_labse": 0.8609989881515503,
    "prof_backtrans_xlm_roberta": 0.6961817741394043,
    "llm_vs_prof_backtrans_bleu": 50.40855074065085,
    "llm_vs_prof_backtrans_chrf": 74.95935220063771,
    "llm_vs_prof_backtrans_bertscore": 0.8573120832443237,
    "llm_vs_prof_backtrans_labse": 0.7717183828353882
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 44.100846378081656,
    "same_lang_chrf": 62.42578790125253,
    "same_lang_bertscore": 0.7956798076629639,
    "same_lang_comet": 0.8359686136245728,
    "cross_lang_xlm_roberta": 0.9274733066558838,
    "cross_lang_labse": 0.8922173976898193,
    "cross_lang_mbert": 0.8707432746887207,
    "cross_lang_comet_qe": 0.11789570748806,
    "backtrans_bleu": 60.14911818833395,
    "backtrans_chrf": 80.65821896858417,
    "backtrans_bertscore": 0.8709326982498169,
    "prof_backtrans_bleu": null,
    "prof_backtrans_chrf": null,
    "prof_backtrans_bertscore": null,
    "prof_backtrans_labse": null,
    "prof_backtrans_xlm_roberta": null,
    "llm_vs_prof_backtrans_bleu": null,
    "llm_vs_prof_backtrans_chrf": null,
    "llm_vs_prof_backtrans_bertscore": null,
    "llm_vs_prof_backtrans_labse": null
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": null,
    "same_lang_chrf": null,
    "same_lang_bertscore": null,
    "same_lang_comet": null,
    "cross_lang_xlm_roberta": null,
    "cross_lang_labse": null,
    "cross_lang_mbert": null,
    "cross_lang_comet_qe": null,
    "backtrans_bleu": null,
    "backtrans_chrf": null,
    "backtrans_bertscore": null,
    "prof_backtrans_bleu": null,
    "prof_backtrans_chrf": null,
    "prof_backtrans_bertscore": null,
    "prof_backtrans_labse": null,
    "prof_backtrans_xlm_roberta": null,
    "llm_vs_prof_backtrans_bleu": null,
    "llm_vs_prof_backtrans_chrf": null,
    "llm_vs_prof_backtrans_bertscore": null,
    "llm_vs_prof_backtrans_labse": null
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 35.88182736196924,
    "same_lang_chrf": 63.43445127678036,
    "same_lang_bertscore": 0.7862493395805359,
    "same_lang_comet": 0.8399406671524048,
    "cross_lang_xlm_roberta": 0.9467737674713135,
    "cross_lang_labse": 0.9073474407196045,
    "cross_lang_mbert": 0.9221329689025879,
    "cross_lang_comet_qe": 0.08210274577140808,
    "backtrans_bleu": 47.74082574985537,
    "backtrans_chrf": 74.32544799520035,
    "backtrans_bertscore": 0.8824571371078491,
    "prof_backtrans_bleu": 40.71141439994604,
    "prof_backtrans_chrf": 71.4247437910163,
    "prof_backtrans_bertscore": 0.8624820113182068,
    "prof_backtrans_labse": 0.864932656288147,
    "prof_backtrans_xlm_roberta": 0.6978251934051514,
    "llm_vs_prof_backtrans_bleu": 48.68124616811655,
    "llm_vs_prof_backtrans_chrf": 69.74236031766246,
    "llm_vs_prof_backtrans_bertscore": 0.9040082693099976,
    "llm_vs_prof_backtrans_labse": 0.8926451206207275
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 26.823499890880935,
    "same_lang_chrf": 44.25882171491701,
    "same_lang_bertscore": 0.789993166923523,
    "same_lang_comet": 0.8486317992210388,
    "cross_lang_xlm_roberta": 0.9327313303947449,
    "cross_lang_labse": 0.8854908347129822,
    "cross_lang_mbert": 0.8780810832977295,
    "cross_lang_comet_qe": 0.08395873755216599,
    "backtrans_bleu": 42.64550394971503,
    "backtrans_chrf": 73.46066554155365,
    "backtrans_bertscore": 0.8590357303619385,
    "prof_backtrans_bleu": 23.914984409117086,
    "prof_backtrans_chrf": 63.3556412526732,
    "prof_backtrans_bertscore": 0.8665403723716736,
    "prof_backtrans_labse": 0.8912724852561951,
    "prof_backtrans_xlm_roberta": 0.7747490406036377,
    "llm_vs_prof_backtrans_bleu": 31.331429959764705,
    "llm_vs_prof_backtrans_chrf": 69.89968416405705,
    "llm_vs_prof_backtrans_bertscore": 0.8476113080978394,
    "llm_vs_prof_backtrans_labse": 0.7989508509635925
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 23.995109019485387,
    "same_lang_chrf": 54.55357276690817,
    "same_lang_bertscore": 0.7834911346435547,
    "same_lang_comet": 0.8162959814071655,
    "cross_lang_xlm_roberta": 0.9196975231170654,
    "cross_lang_labse": 0.9026052355766296,
    "cross_lang_mbert": 0.8756605386734009,
    "cross_lang_comet_qe": -0.011547975242137909,
    "backtrans_bleu": 52.016649585931,
    "backtrans_chrf": 76.2273164873967,
    "backtrans_bertscore": 0.869152843952179,
    "prof_backtrans_bleu": 34.54265044666565,
    "prof_backtrans_chrf": 68.69299269024214,
    "prof_backtrans_bertscore": 0.8623042702674866,
    "prof_backtrans_labse": 0.8996025919914246,
    "prof_backtrans_xlm_roberta": 0.6843220591545105,
    "llm_vs_prof_backtrans_bleu": 38.54732089713452,
    "llm_vs_prof_backtrans_chrf": 64.50712207215442,
    "llm_vs_prof_backtrans_bertscore": 0.8992752432823181,
    "llm_vs_prof_backtrans_labse": 0.9044782519340515
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 50.36493309860557,
    "same_lang_chrf": 76.82652849996403,
    "same_lang_bertscore": 0.8215619325637817,
    "same_lang_comet": 0.7977519035339355,
    "cross_lang_xlm_roberta": 0.9350214004516602,
    "cross_lang_labse": 0.885047197341919,
    "cross_lang_mbert": 0.8722360134124756,
    "cross_lang_comet_qe": 0.0793389230966568,
    "backtrans_bleu": 63.48438819412352,
    "backtrans_chrf": 81.53294950223179,
    "backtrans_bertscore": 0.8645808100700378,
    "prof_backtrans_bleu": 46.29324826761205,
    "prof_backtrans_chrf": 75.93245419136568,
    "prof_backtrans_bertscore": 0.8830833435058594,
    "prof_backtrans_labse": 0.9199101328849792,
    "prof_backtrans_xlm_roberta": 0.8961262702941895,
    "llm_vs_prof_backtrans_bleu": 56.15477290271903,
    "llm_vs_prof_backtrans_chrf": 75.64370523604444,
    "llm_vs_prof_backtrans_bertscore": 0.8687636256217957,
    "llm_vs_prof_backtrans_labse": 0.8284235596656799
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.465017073192813,
    "same_lang_chrf": 42.401267762067015,
    "same_lang_bertscore": 0.7700555324554443,
    "same_lang_comet": 0.849441409111023,
    "cross_lang_xlm_roberta": 0.9325764775276184,
    "cross_lang_labse": 0.8813780546188354,
    "cross_lang_mbert": 0.8575506210327148,
    "cross_lang_comet_qe": 0.10469721257686615,
    "backtrans_bleu": 66.08275323542212,
    "backtrans_chrf": 84.64279474999641,
    "backtrans_bertscore": 0.868325412273407,
    "prof_backtrans_bleu": 50.81861790687232,
    "prof_backtrans_chrf": 73.3784796097208,
    "prof_backtrans_bertscore": 0.8765240907669067,
    "prof_backtrans_labse": 0.8996459245681763,
    "prof_backtrans_xlm_roberta": 0.6694732308387756,
    "llm_vs_prof_backtrans_bleu": 53.974814911696384,
    "llm_vs_prof_backtrans_chrf": 75.48260271085711,
    "llm_vs_prof_backtrans_bertscore": 0.8519755005836487,
    "llm_vs_prof_backtrans_labse": 0.8423137664794922
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 38.965006476025735,
    "same_lang_chrf": 58.45883355944162,
    "same_lang_bertscore": 0.778478741645813,
    "same_lang_comet": 0.8361896276473999,
    "cross_lang_xlm_roberta": 0.9547243714332581,
    "cross_lang_labse": 0.8972285389900208,
    "cross_lang_mbert": 0.9248763918876648,
    "cross_lang_comet_qe": 0.06602877378463745,
    "backtrans_bleu": 59.06520950841904,
    "backtrans_chrf": 82.08541018323234,
    "backtrans_bertscore": 0.8716959357261658,
    "prof_backtrans_bleu": 50.86255711483618,
    "prof_backtrans_chrf": 77.12799284954848,
    "prof_backtrans_bertscore": 0.9013944864273071,
    "prof_backtrans_labse": 0.9410804510116577,
    "prof_backtrans_xlm_roberta": 0.7392070293426514,
    "llm_vs_prof_backtrans_bleu": 51.46086793544574,
    "llm_vs_prof_backtrans_chrf": 77.60166508614226,
    "llm_vs_prof_backtrans_bertscore": 0.8525401949882507,
    "llm_vs_prof_backtrans_labse": 0.8593189716339111
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 36.63440497672405,
    "same_lang_chrf": 67.78793041216672,
    "same_lang_bertscore": 0.7682130336761475,
    "same_lang_comet": 0.8306808471679688,
    "cross_lang_xlm_roberta": 0.9239218235015869,
    "cross_lang_labse": 0.8910644054412842,
    "cross_lang_mbert": 0.8876010179519653,
    "cross_lang_comet_qe": 0.09659463167190552,
    "backtrans_bleu": 70.58365441112961,
    "backtrans_chrf": 86.54183871354185,
    "backtrans_bertscore": 0.8685145378112793,
    "prof_backtrans_bleu": 49.47120256739989,
    "prof_backtrans_chrf": 78.76749759157103,
    "prof_backtrans_bertscore": 0.8646301627159119,
    "prof_backtrans_labse": 0.8498848080635071,
    "prof_backtrans_xlm_roberta": 0.7868075370788574,
    "llm_vs_prof_backtrans_bleu": 58.057061661407836,
    "llm_vs_prof_backtrans_chrf": 78.95601987566072,
    "llm_vs_prof_backtrans_bertscore": 0.863268256187439,
    "llm_vs_prof_backtrans_labse": 0.8636926412582397
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 29.821543294883334,
    "same_lang_chrf": 61.396559221482505,
    "same_lang_bertscore": 0.797255277633667,
    "same_lang_comet": 0.8256247639656067,
    "cross_lang_xlm_roberta": 0.9783718585968018,
    "cross_lang_labse": 0.974108099937439,
    "cross_lang_mbert": 0.904217004776001,
    "cross_lang_comet_qe": -0.19505243003368378,
    "backtrans_bleu": 52.07271683163532,
    "backtrans_chrf": 77.12420769532685,
    "backtrans_bertscore": 0.9139666557312012,
    "prof_backtrans_bleu": 37.75401072293222,
    "prof_backtrans_chrf": 73.13300395310812,
    "prof_backtrans_bertscore": 0.8681519031524658,
    "prof_backtrans_labse": 0.8834423422813416,
    "prof_backtrans_xlm_roberta": 0.7248849868774414,
    "llm_vs_prof_backtrans_bleu": 39.50601958978478,
    "llm_vs_prof_backtrans_chrf": 67.19369960410596,
    "llm_vs_prof_backtrans_bertscore": 0.8670986890792847,
    "llm_vs_prof_backtrans_labse": 0.8866959810256958
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 27.41167751761183,
    "same_lang_chrf": 47.604544147184704,
    "same_lang_bertscore": 0.7811571955680847,
    "same_lang_comet": 0.8533151149749756,
    "cross_lang_xlm_roberta": 0.9316750764846802,
    "cross_lang_labse": 0.8819032907485962,
    "cross_lang_mbert": 0.8970022797584534,
    "cross_lang_comet_qe": 0.03836409002542496,
    "backtrans_bleu": 61.23974354758336,
    "backtrans_chrf": 81.50267667565751,
    "backtrans_bertscore": 0.8690162301063538,
    "prof_backtrans_bleu": 46.74953853014907,
    "prof_backtrans_chrf": 76.64514049144323,
    "prof_backtrans_bertscore": 0.8813722133636475,
    "prof_backtrans_labse": 0.9024290442466736,
    "prof_backtrans_xlm_roberta": 0.6572434902191162,
    "llm_vs_prof_backtrans_bleu": 50.51575640433638,
    "llm_vs_prof_backtrans_chrf": 76.38904434651454,
    "llm_vs_prof_backtrans_bertscore": 0.8563777208328247,
    "llm_vs_prof_backtrans_labse": 0.8273619413375854
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 30.170069944133818,
    "same_lang_chrf": 58.35565979112805,
    "same_lang_bertscore": 0.7833462953567505,
    "same_lang_comet": 0.762498140335083,
    "cross_lang_xlm_roberta": 0.9535698294639587,
    "cross_lang_labse": 0.8942787051200867,
    "cross_lang_mbert": 0.9118816256523132,
    "cross_lang_comet_qe": -0.07920794934034348,
    "backtrans_bleu": 58.38617480039859,
    "backtrans_chrf": 81.41016348683576,
    "backtrans_bertscore": 0.8716496229171753,
    "prof_backtrans_bleu": 44.90177756795346,
    "prof_backtrans_chrf": 74.38785604328665,
    "prof_backtrans_bertscore": 0.8519651293754578,
    "prof_backtrans_labse": 0.8649502396583557,
    "prof_backtrans_xlm_roberta": 0.8036720156669617,
    "llm_vs_prof_backtrans_bleu": 51.005402379712585,
    "llm_vs_prof_backtrans_chrf": 72.98070670774702,
    "llm_vs_prof_backtrans_bertscore": 0.8724666237831116,
    "llm_vs_prof_backtrans_labse": 0.9383606910705566
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 44.0487894770823,
    "same_lang_chrf": 74.48396493307708,
    "same_lang_bertscore": 0.8371596932411194,
    "same_lang_comet": 0.7944588661193848,
    "cross_lang_xlm_roberta": 0.9791494607925415,
    "cross_lang_labse": 0.9785487651824951,
    "cross_lang_mbert": 0.9298949241638184,
    "cross_lang_comet_qe": -0.1784699559211731,
    "backtrans_bleu": 62.1070753616262,
    "backtrans_chrf": 82.84178348871521,
    "backtrans_bertscore": 0.9077991843223572,
    "prof_backtrans_bleu": 44.029232631182424,
    "prof_backtrans_chrf": 75.70096440604523,
    "prof_backtrans_bertscore": 0.8836897015571594,
    "prof_backtrans_labse": 0.9160162210464478,
    "prof_backtrans_xlm_roberta": 0.8981677889823914,
    "llm_vs_prof_backtrans_bleu": 48.299111215588056,
    "llm_vs_prof_backtrans_chrf": 75.81818856916381,
    "llm_vs_prof_backtrans_bertscore": 0.8898075819015503,
    "llm_vs_prof_backtrans_labse": 0.9017868638038635
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 10.367527326382108,
    "same_lang_chrf": 37.90903706199325,
    "same_lang_bertscore": 0.8179588317871094,
    "same_lang_comet": 0.859656572341919,
    "cross_lang_xlm_roberta": 0.9802696704864502,
    "cross_lang_labse": 0.983482301235199,
    "cross_lang_mbert": 0.8630659580230713,
    "cross_lang_comet_qe": -0.15150196850299835,
    "backtrans_bleu": 63.32369121097316,
    "backtrans_chrf": 82.5811468501979,
    "backtrans_bertscore": 0.8928606510162354,
    "prof_backtrans_bleu": 49.556810544530734,
    "prof_backtrans_chrf": 79.00618463053178,
    "prof_backtrans_bertscore": 0.8865401744842529,
    "prof_backtrans_labse": 0.9151253700256348,
    "prof_backtrans_xlm_roberta": 0.665744423866272,
    "llm_vs_prof_backtrans_bleu": 51.736771182102,
    "llm_vs_prof_backtrans_chrf": 76.79375088144556,
    "llm_vs_prof_backtrans_bertscore": 0.8916475772857666,
    "llm_vs_prof_backtrans_labse": 0.9177731871604919
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 43.53607275410001,
    "same_lang_chrf": 62.91685518377233,
    "same_lang_bertscore": 0.8273175358772278,
    "same_lang_comet": 0.8262061476707458,
    "cross_lang_xlm_roberta": 0.9860865473747253,
    "cross_lang_labse": 0.9878287315368652,
    "cross_lang_mbert": 0.9479348063468933,
    "cross_lang_comet_qe": -0.1886042207479477,
    "backtrans_bleu": 66.098776481004,
    "backtrans_chrf": 84.67669877223746,
    "backtrans_bertscore": 0.9229578375816345,
    "prof_backtrans_bleu": 55.74338849056987,
    "prof_backtrans_chrf": 80.63201183588015,
    "prof_backtrans_bertscore": 0.8582611083984375,
    "prof_backtrans_labse": 0.8977138996124268,
    "prof_backtrans_xlm_roberta": 0.777554988861084,
    "llm_vs_prof_backtrans_bleu": 57.85070102592954,
    "llm_vs_prof_backtrans_chrf": 82.62495795464362,
    "llm_vs_prof_backtrans_bertscore": 0.868642270565033,
    "llm_vs_prof_backtrans_labse": 0.8979926109313965
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 35.898557354895,
    "same_lang_chrf": 69.15747399780255,
    "same_lang_bertscore": 0.804408073425293,
    "same_lang_comet": 0.8417822122573853,
    "cross_lang_xlm_roberta": 0.9920551776885986,
    "cross_lang_labse": 0.9741283655166626,
    "cross_lang_mbert": 0.9911229610443115,
    "cross_lang_comet_qe": 0.06784422695636749,
    "backtrans_bleu": 77.23613866061932,
    "backtrans_chrf": 90.68389036396265,
    "backtrans_bertscore": 0.9104254245758057,
    "prof_backtrans_bleu": 59.89372498481662,
    "prof_backtrans_chrf": 82.3301037864515,
    "prof_backtrans_bertscore": 0.859199583530426,
    "prof_backtrans_labse": 0.8771942853927612,
    "prof_backtrans_xlm_roberta": 0.840951144695282,
    "llm_vs_prof_backtrans_bleu": 69.49079369261344,
    "llm_vs_prof_backtrans_chrf": 84.18823833706716,
    "llm_vs_prof_backtrans_bertscore": 0.9012943506240845,
    "llm_vs_prof_backtrans_labse": 0.8887197971343994
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 28.625576897415673,
    "same_lang_chrf": 63.034924298656136,
    "same_lang_bertscore": 0.8119097948074341,
    "same_lang_comet": 0.8188716173171997,
    "cross_lang_xlm_roberta": 0.9915686249732971,
    "cross_lang_labse": 0.9918874502182007,
    "cross_lang_mbert": 0.9553015828132629,
    "cross_lang_comet_qe": -0.273712694644928,
    "backtrans_bleu": 68.40683587874825,
    "backtrans_chrf": 86.79061706440604,
    "backtrans_bertscore": 0.9174933433532715,
    "prof_backtrans_bleu": 33.590780247426295,
    "prof_backtrans_chrf": 71.48055014820717,
    "prof_backtrans_bertscore": 0.8702377676963806,
    "prof_backtrans_labse": 0.8986469507217407,
    "prof_backtrans_xlm_roberta": 0.7249686121940613,
    "llm_vs_prof_backtrans_bleu": 34.09920449092228,
    "llm_vs_prof_backtrans_chrf": 65.68883460793359,
    "llm_vs_prof_backtrans_bertscore": 0.8789402842521667,
    "llm_vs_prof_backtrans_labse": 0.8841602802276611
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 30.4342121709387,
    "same_lang_chrf": 47.31206220044753,
    "same_lang_bertscore": 0.8320128321647644,
    "same_lang_comet": 0.8602983355522156,
    "cross_lang_xlm_roberta": 0.891599178314209,
    "cross_lang_labse": 0.9590776562690735,
    "cross_lang_mbert": 0.8533048629760742,
    "cross_lang_comet_qe": -0.12397336959838867,
    "backtrans_bleu": 59.11659478081853,
    "backtrans_chrf": 80.01704359915207,
    "backtrans_bertscore": 0.892388641834259,
    "prof_backtrans_bleu": 57.127122110688234,
    "prof_backtrans_chrf": 80.32722491385799,
    "prof_backtrans_bertscore": 0.8586382269859314,
    "prof_backtrans_labse": 0.8707876205444336,
    "prof_backtrans_xlm_roberta": 0.6489291787147522,
    "llm_vs_prof_backtrans_bleu": 58.7851351794966,
    "llm_vs_prof_backtrans_chrf": 79.37783152977923,
    "llm_vs_prof_backtrans_bertscore": 0.8843280673027039,
    "llm_vs_prof_backtrans_labse": 0.8847264647483826
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 26.142827127902827,
    "same_lang_chrf": 56.32830910913216,
    "same_lang_bertscore": 0.8141289949417114,
    "same_lang_comet": 0.8246451616287231,
    "cross_lang_xlm_roberta": 0.9820141196250916,
    "cross_lang_labse": 0.9886186122894287,
    "cross_lang_mbert": 0.9162176847457886,
    "cross_lang_comet_qe": -0.2012181580066681,
    "backtrans_bleu": 64.5377822275664,
    "backtrans_chrf": 83.91303429076102,
    "backtrans_bertscore": 0.9065643548965454,
    "prof_backtrans_bleu": 43.0312437296679,
    "prof_backtrans_chrf": 74.50135705776613,
    "prof_backtrans_bertscore": 0.8902299404144287,
    "prof_backtrans_labse": 0.9342411756515503,
    "prof_backtrans_xlm_roberta": 0.7946874499320984,
    "llm_vs_prof_backtrans_bleu": 44.798270615341146,
    "llm_vs_prof_backtrans_chrf": 70.92315100625409,
    "llm_vs_prof_backtrans_bertscore": 0.8869524598121643,
    "llm_vs_prof_backtrans_labse": 0.9228953123092651
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 44.21960662901114,
    "same_lang_chrf": 74.89468539036096,
    "same_lang_bertscore": 0.804731011390686,
    "same_lang_comet": 0.7967550754547119,
    "cross_lang_xlm_roberta": 0.8851568102836609,
    "cross_lang_labse": 0.8097753524780273,
    "cross_lang_mbert": 0.7620187401771545,
    "cross_lang_comet_qe": 0.14033730328083038,
    "backtrans_bleu": 58.79897267347067,
    "backtrans_chrf": 80.95196598917802,
    "backtrans_bertscore": 0.8509154915809631,
    "prof_backtrans_bleu": 42.563863535022655,
    "prof_backtrans_chrf": 74.38191778898224,
    "prof_backtrans_bertscore": 0.8825799822807312,
    "prof_backtrans_labse": 0.9045401811599731,
    "prof_backtrans_xlm_roberta": 0.9022382497787476,
    "llm_vs_prof_backtrans_bleu": 45.29254570247106,
    "llm_vs_prof_backtrans_chrf": 68.49450198340108,
    "llm_vs_prof_backtrans_bertscore": 0.8533493280410767,
    "llm_vs_prof_backtrans_labse": 0.7773275971412659
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 16.10727762483765,
    "same_lang_chrf": 41.78969048301847,
    "same_lang_bertscore": 0.7678502798080444,
    "same_lang_comet": 0.851607084274292,
    "cross_lang_xlm_roberta": 0.8999291062355042,
    "cross_lang_labse": 0.8252142667770386,
    "cross_lang_mbert": 0.7309185266494751,
    "cross_lang_comet_qe": 0.2019759565591812,
    "backtrans_bleu": 68.55508942985632,
    "backtrans_chrf": 83.96651456262168,
    "backtrans_bertscore": 0.8542229533195496,
    "prof_backtrans_bleu": 39.562361034606596,
    "prof_backtrans_chrf": 73.89302128417677,
    "prof_backtrans_bertscore": 0.8470446467399597,
    "prof_backtrans_labse": 0.8440924286842346,
    "prof_backtrans_xlm_roberta": 0.6591320633888245,
    "llm_vs_prof_backtrans_bleu": 43.05095495187647,
    "llm_vs_prof_backtrans_chrf": 67.12990888007847,
    "llm_vs_prof_backtrans_bertscore": 0.886908769607544,
    "llm_vs_prof_backtrans_labse": 0.848660945892334
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 50.34849609098111,
    "same_lang_chrf": 67.17771738139977,
    "same_lang_bertscore": 0.7921348214149475,
    "same_lang_comet": 0.8316919207572937,
    "cross_lang_xlm_roberta": 0.9000701308250427,
    "cross_lang_labse": 0.8459882140159607,
    "cross_lang_mbert": 0.7667064666748047,
    "cross_lang_comet_qe": 0.0803031474351883,
    "backtrans_bleu": 58.68194256034607,
    "backtrans_chrf": 79.02313932045877,
    "backtrans_bertscore": 0.8541346192359924,
    "prof_backtrans_bleu": 41.95460179037046,
    "prof_backtrans_chrf": 72.82429077123501,
    "prof_backtrans_bertscore": 0.854607105255127,
    "prof_backtrans_labse": 0.8620330691337585,
    "prof_backtrans_xlm_roberta": 0.6928501129150391,
    "llm_vs_prof_backtrans_bleu": 50.96555941832039,
    "llm_vs_prof_backtrans_chrf": 72.52722155156653,
    "llm_vs_prof_backtrans_bertscore": 0.9045979976654053,
    "llm_vs_prof_backtrans_labse": 0.8662713766098022
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 40.20700785405266,
    "same_lang_chrf": 70.74673223602463,
    "same_lang_bertscore": 0.7857655882835388,
    "same_lang_comet": 0.8451557159423828,
    "cross_lang_xlm_roberta": 0.9001495242118835,
    "cross_lang_labse": 0.8756203651428223,
    "cross_lang_mbert": 0.7946351766586304,
    "cross_lang_comet_qe": 0.13468556106090546,
    "backtrans_bleu": 72.78172543490795,
    "backtrans_chrf": 88.42441354248628,
    "backtrans_bertscore": 0.8613588809967041,
    "prof_backtrans_bleu": 51.376526023336616,
    "prof_backtrans_chrf": 79.78811408454352,
    "prof_backtrans_bertscore": 0.8538497090339661,
    "prof_backtrans_labse": 0.8408020734786987,
    "prof_backtrans_xlm_roberta": 0.7994102835655212,
    "llm_vs_prof_backtrans_bleu": 63.11532333836,
    "llm_vs_prof_backtrans_chrf": 79.26593834511642,
    "llm_vs_prof_backtrans_bertscore": 0.9280722141265869,
    "llm_vs_prof_backtrans_labse": 0.9082825183868408
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 32.75186307471678,
    "same_lang_chrf": 64.42586198954825,
    "same_lang_bertscore": 0.781243085861206,
    "same_lang_comet": 0.8312510251998901,
    "cross_lang_xlm_roberta": 0.8914708495140076,
    "cross_lang_labse": 0.8626677989959717,
    "cross_lang_mbert": 0.8137461543083191,
    "cross_lang_comet_qe": 0.17562896013259888,
    "backtrans_bleu": 69.02919943987357,
    "backtrans_chrf": 85.3937500364446,
    "backtrans_bertscore": 0.8595836758613586,
    "prof_backtrans_bleu": 28.686730260635073,
    "prof_backtrans_chrf": 66.68022494602675,
    "prof_backtrans_bertscore": 0.8457313179969788,
    "prof_backtrans_labse": 0.8193711042404175,
    "prof_backtrans_xlm_roberta": 0.7007269263267517,
    "llm_vs_prof_backtrans_bleu": 34.74170110185498,
    "llm_vs_prof_backtrans_chrf": 57.676655612081696,
    "llm_vs_prof_backtrans_bertscore": 0.9055299758911133,
    "llm_vs_prof_backtrans_labse": 0.8758420944213867
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 30.87420056745017,
    "same_lang_chrf": 48.980052755049876,
    "same_lang_bertscore": 0.7759860754013062,
    "same_lang_comet": 0.8617241382598877,
    "cross_lang_xlm_roberta": 0.9118781089782715,
    "cross_lang_labse": 0.8713492155075073,
    "cross_lang_mbert": 0.7323808670043945,
    "cross_lang_comet_qe": 0.12929297983646393,
    "backtrans_bleu": 56.34773848604414,
    "backtrans_chrf": 80.17763513931978,
    "backtrans_bertscore": 0.852508544921875,
    "prof_backtrans_bleu": 44.11986853802118,
    "prof_backtrans_chrf": 75.40214750227396,
    "prof_backtrans_bertscore": 0.852429986000061,
    "prof_backtrans_labse": 0.8282368183135986,
    "prof_backtrans_xlm_roberta": 0.6529831290245056,
    "llm_vs_prof_backtrans_bleu": 50.824715930735906,
    "llm_vs_prof_backtrans_chrf": 71.81207879578743,
    "llm_vs_prof_backtrans_bertscore": 0.9163128137588501,
    "llm_vs_prof_backtrans_labse": 0.8842358589172363
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 30.035931733310452,
    "same_lang_chrf": 59.21280058801067,
    "same_lang_bertscore": 0.7825155854225159,
    "same_lang_comet": 0.8013240098953247,
    "cross_lang_xlm_roberta": 0.8915486931800842,
    "cross_lang_labse": 0.8792373538017273,
    "cross_lang_mbert": 0.8387376070022583,
    "cross_lang_comet_qe": 0.08799854665994644,
    "backtrans_bleu": 51.95866651492173,
    "backtrans_chrf": 77.89371208311705,
    "backtrans_bertscore": 0.8563486337661743,
    "prof_backtrans_bleu": 32.708601670449745,
    "prof_backtrans_chrf": 69.91972208500688,
    "prof_backtrans_bertscore": 0.8492343425750732,
    "prof_backtrans_labse": 0.8444418907165527,
    "prof_backtrans_xlm_roberta": 0.7025505900382996,
    "llm_vs_prof_backtrans_bleu": 44.53176179159008,
    "llm_vs_prof_backtrans_chrf": 68.75483583411915,
    "llm_vs_prof_backtrans_bertscore": 0.9075554013252258,
    "llm_vs_prof_backtrans_labse": 0.8679140210151672
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 48.54342355938787,
    "same_lang_chrf": 74.7631370236336,
    "same_lang_bertscore": 0.8027687072753906,
    "same_lang_comet": 0.8007423877716064,
    "cross_lang_xlm_roberta": 0.8792759776115417,
    "cross_lang_labse": 0.8686525821685791,
    "cross_lang_mbert": 0.7605130672454834,
    "cross_lang_comet_qe": 0.1360315978527069,
    "backtrans_bleu": 61.799850505091236,
    "backtrans_chrf": 80.35745248992085,
    "backtrans_bertscore": 0.8601844906806946,
    "prof_backtrans_bleu": 41.85841654843236,
    "prof_backtrans_chrf": 74.15331230580773,
    "prof_backtrans_bertscore": 0.8550360798835754,
    "prof_backtrans_labse": 0.8796250224113464,
    "prof_backtrans_xlm_roberta": 0.9009256958961487,
    "llm_vs_prof_backtrans_bleu": 52.012594272771814,
    "llm_vs_prof_backtrans_chrf": 71.08124000386078,
    "llm_vs_prof_backtrans_bertscore": 0.8999610543251038,
    "llm_vs_prof_backtrans_labse": 0.8785995841026306
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 5.555533216713362,
    "same_lang_chrf": 36.834453047003976,
    "same_lang_bertscore": 0.7750481963157654,
    "same_lang_comet": 0.8485641479492188,
    "cross_lang_xlm_roberta": 0.8876857161521912,
    "cross_lang_labse": 0.8826306462287903,
    "cross_lang_mbert": 0.8359875679016113,
    "cross_lang_comet_qe": 0.17852944135665894,
    "backtrans_bleu": 49.051743021413984,
    "backtrans_chrf": 77.12334384264663,
    "backtrans_bertscore": 0.8638541102409363,
    "prof_backtrans_bleu": 42.8637691665918,
    "prof_backtrans_chrf": 73.52572509826865,
    "prof_backtrans_bertscore": 0.8515239953994751,
    "prof_backtrans_labse": 0.8641520738601685,
    "prof_backtrans_xlm_roberta": 0.6550149321556091,
    "llm_vs_prof_backtrans_bleu": 53.62743025879081,
    "llm_vs_prof_backtrans_chrf": 74.56271524700227,
    "llm_vs_prof_backtrans_bertscore": 0.8955351114273071,
    "llm_vs_prof_backtrans_labse": 0.8943250775337219
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 44.27827581670813,
    "same_lang_chrf": 63.71807966801883,
    "same_lang_bertscore": 0.7906157374382019,
    "same_lang_comet": 0.8294048309326172,
    "cross_lang_xlm_roberta": 0.8912302255630493,
    "cross_lang_labse": 0.8726979494094849,
    "cross_lang_mbert": 0.7217475771903992,
    "cross_lang_comet_qe": 0.1240336149930954,
    "backtrans_bleu": 53.27929381548671,
    "backtrans_chrf": 75.47229531434789,
    "backtrans_bertscore": 0.8650844097137451,
    "prof_backtrans_bleu": 43.87194715171949,
    "prof_backtrans_chrf": 72.1804438619552,
    "prof_backtrans_bertscore": 0.8557983040809631,
    "prof_backtrans_labse": 0.8709084391593933,
    "prof_backtrans_xlm_roberta": 0.6888585686683655,
    "llm_vs_prof_backtrans_bleu": 61.585830247794846,
    "llm_vs_prof_backtrans_chrf": 79.5220083552492,
    "llm_vs_prof_backtrans_bertscore": 0.9144019484519958,
    "llm_vs_prof_backtrans_labse": 0.8964597582817078
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 35.645344276082454,
    "same_lang_chrf": 69.39039880555342,
    "same_lang_bertscore": 0.7788109183311462,
    "same_lang_comet": 0.8445255756378174,
    "cross_lang_xlm_roberta": 0.890728235244751,
    "cross_lang_labse": 0.8830385208129883,
    "cross_lang_mbert": 0.8407958745956421,
    "cross_lang_comet_qe": 0.06977922469377518,
    "backtrans_bleu": 55.180939540923596,
    "backtrans_chrf": 78.35836573399342,
    "backtrans_bertscore": 0.8596276640892029,
    "prof_backtrans_bleu": 41.820352284121796,
    "prof_backtrans_chrf": 75.36347614352138,
    "prof_backtrans_bertscore": 0.8563054203987122,
    "prof_backtrans_labse": 0.870620608329773,
    "prof_backtrans_xlm_roberta": 0.7932890057563782,
    "llm_vs_prof_backtrans_bleu": 49.11458067117928,
    "llm_vs_prof_backtrans_chrf": 71.074694501165,
    "llm_vs_prof_backtrans_bertscore": 0.8864049911499023,
    "llm_vs_prof_backtrans_labse": 0.8702394962310791
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 29.460037605909292,
    "same_lang_chrf": 61.0706368248623,
    "same_lang_bertscore": 0.7831166386604309,
    "same_lang_comet": 0.8239660263061523,
    "cross_lang_xlm_roberta": 0.9061667919158936,
    "cross_lang_labse": 0.8493528366088867,
    "cross_lang_mbert": 0.6969882845878601,
    "cross_lang_comet_qe": 0.05722387135028839,
    "backtrans_bleu": 36.935791225683055,
    "backtrans_chrf": 67.86996597761835,
    "backtrans_bertscore": 0.8496456742286682,
    "prof_backtrans_bleu": 28.675704374866243,
    "prof_backtrans_chrf": 65.1887039254778,
    "prof_backtrans_bertscore": 0.8520594239234924,
    "prof_backtrans_labse": 0.8497634530067444,
    "prof_backtrans_xlm_roberta": 0.7260657548904419,
    "llm_vs_prof_backtrans_bleu": 36.48511491144673,
    "llm_vs_prof_backtrans_chrf": 62.17573862294958,
    "llm_vs_prof_backtrans_bertscore": 0.8949819207191467,
    "llm_vs_prof_backtrans_labse": 0.8349543809890747
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 29.264503294776905,
    "same_lang_chrf": 46.21817135778464,
    "same_lang_bertscore": 0.7877675890922546,
    "same_lang_comet": 0.8636361360549927,
    "cross_lang_xlm_roberta": 0.9175054430961609,
    "cross_lang_labse": 0.858580470085144,
    "cross_lang_mbert": 0.809734046459198,
    "cross_lang_comet_qe": 0.09750159829854965,
    "backtrans_bleu": 41.01987685703826,
    "backtrans_chrf": 70.92414159652252,
    "backtrans_bertscore": 0.8568734526634216,
    "prof_backtrans_bleu": 49.649105346582424,
    "prof_backtrans_chrf": 77.79186107958613,
    "prof_backtrans_bertscore": 0.855802595615387,
    "prof_backtrans_labse": 0.880728006362915,
    "prof_backtrans_xlm_roberta": 0.7071985006332397,
    "llm_vs_prof_backtrans_bleu": 50.66502814719908,
    "llm_vs_prof_backtrans_chrf": 73.44885759370644,
    "llm_vs_prof_backtrans_bertscore": 0.8983851671218872,
    "llm_vs_prof_backtrans_labse": 0.8208563327789307
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 24.540201051896087,
    "same_lang_chrf": 52.973441367624076,
    "same_lang_bertscore": 0.7708827257156372,
    "same_lang_comet": 0.7685791254043579,
    "cross_lang_xlm_roberta": 0.8191903233528137,
    "cross_lang_labse": 0.8867207169532776,
    "cross_lang_mbert": 0.8194137811660767,
    "cross_lang_comet_qe": -0.015758052468299866,
    "backtrans_bleu": 45.425588847036046,
    "backtrans_chrf": 73.80435978479318,
    "backtrans_bertscore": 0.8558338284492493,
    "prof_backtrans_bleu": 36.38856146136707,
    "prof_backtrans_chrf": 67.33817740129814,
    "prof_backtrans_bertscore": 0.8565546870231628,
    "prof_backtrans_labse": 0.9090312123298645,
    "prof_backtrans_xlm_roberta": 0.7882590889930725,
    "llm_vs_prof_backtrans_bleu": 39.73457112358998,
    "llm_vs_prof_backtrans_chrf": 65.78872707147411,
    "llm_vs_prof_backtrans_bertscore": 0.8941987156867981,
    "llm_vs_prof_backtrans_labse": 0.8835987448692322
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 45.25143996918691,
    "same_lang_chrf": 74.53588053194949,
    "same_lang_bertscore": 0.8375557661056519,
    "same_lang_comet": 0.7724422216415405,
    "cross_lang_xlm_roberta": 0.9393539428710938,
    "cross_lang_labse": 0.9010449051856995,
    "cross_lang_mbert": 0.907922089099884,
    "cross_lang_comet_qe": 0.30342304706573486,
    "backtrans_bleu": 60.46113980887201,
    "backtrans_chrf": 84.88608287495461,
    "backtrans_bertscore": 0.8849654793739319,
    "prof_backtrans_bleu": 54.96890349823381,
    "prof_backtrans_chrf": 80.45593227805897,
    "prof_backtrans_bertscore": 0.8919646739959717,
    "prof_backtrans_labse": 0.9027878642082214,
    "prof_backtrans_xlm_roberta": 0.8915759325027466,
    "llm_vs_prof_backtrans_bleu": 58.11189594355593,
    "llm_vs_prof_backtrans_chrf": 83.04247001874701,
    "llm_vs_prof_backtrans_bertscore": 0.8732777237892151,
    "llm_vs_prof_backtrans_labse": 0.90928053855896
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.22461954720348,
    "same_lang_chrf": 48.3622075083001,
    "same_lang_bertscore": 0.7149667143821716,
    "same_lang_comet": 0.8443299531936646,
    "cross_lang_xlm_roberta": 0.9634236097335815,
    "cross_lang_labse": 0.8665960431098938,
    "cross_lang_mbert": 0.8482208251953125,
    "cross_lang_comet_qe": 0.16467563807964325,
    "backtrans_bleu": 51.36261562489518,
    "backtrans_chrf": 80.84446204206279,
    "backtrans_bertscore": 0.8722952604293823,
    "prof_backtrans_bleu": 52.69690461921468,
    "prof_backtrans_chrf": 79.60455911681377,
    "prof_backtrans_bertscore": 0.8601517081260681,
    "prof_backtrans_labse": 0.8894531726837158,
    "prof_backtrans_xlm_roberta": 0.8318036198616028,
    "llm_vs_prof_backtrans_bleu": 55.741020667033204,
    "llm_vs_prof_backtrans_chrf": 79.7809855519196,
    "llm_vs_prof_backtrans_bertscore": 0.9255704879760742,
    "llm_vs_prof_backtrans_labse": 0.8612997531890869
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 47.77887345122218,
    "same_lang_chrf": 68.58995727656534,
    "same_lang_bertscore": 0.8854962587356567,
    "same_lang_comet": 0.885766863822937,
    "cross_lang_xlm_roberta": 0.9677086472511292,
    "cross_lang_labse": 0.9764996767044067,
    "cross_lang_mbert": 0.9361207485198975,
    "cross_lang_comet_qe": 0.10604259371757507,
    "backtrans_bleu": 63.76806752710707,
    "backtrans_chrf": 80.99027212035614,
    "backtrans_bertscore": 0.9386692643165588,
    "prof_backtrans_bleu": 68.03320733667715,
    "prof_backtrans_chrf": 85.5209955760217,
    "prof_backtrans_bertscore": 0.9159919023513794,
    "prof_backtrans_labse": 0.9625515937805176,
    "prof_backtrans_xlm_roberta": 0.8491042852401733,
    "llm_vs_prof_backtrans_bleu": 58.90248455392848,
    "llm_vs_prof_backtrans_chrf": 78.69477360052626,
    "llm_vs_prof_backtrans_bertscore": 0.9132499098777771,
    "llm_vs_prof_backtrans_labse": 0.9503125548362732
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 45.326649314029936,
    "same_lang_chrf": 72.27127651907075,
    "same_lang_bertscore": 0.7980964779853821,
    "same_lang_comet": 0.8410584926605225,
    "cross_lang_xlm_roberta": 0.9616140127182007,
    "cross_lang_labse": 0.8755902051925659,
    "cross_lang_mbert": 0.9539667963981628,
    "cross_lang_comet_qe": 0.21059322357177734,
    "backtrans_bleu": 71.48452937336673,
    "backtrans_chrf": 88.9242391815302,
    "backtrans_bertscore": 0.8823467493057251,
    "prof_backtrans_bleu": 67.51550981541001,
    "prof_backtrans_chrf": 86.89016227574758,
    "prof_backtrans_bertscore": 0.8894003033638,
    "prof_backtrans_labse": 0.8905982971191406,
    "prof_backtrans_xlm_roberta": 0.9067994952201843,
    "llm_vs_prof_backtrans_bleu": 68.00838527916893,
    "llm_vs_prof_backtrans_chrf": 86.7769554945775,
    "llm_vs_prof_backtrans_bertscore": 0.8811267614364624,
    "llm_vs_prof_backtrans_labse": 0.8547476530075073
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 29.97083383916988,
    "same_lang_chrf": 63.30963019591852,
    "same_lang_bertscore": 0.75433349609375,
    "same_lang_comet": 0.7903974056243896,
    "cross_lang_xlm_roberta": 0.9850422739982605,
    "cross_lang_labse": 0.885103166103363,
    "cross_lang_mbert": 0.8512890338897705,
    "cross_lang_comet_qe": 0.20379669964313507,
    "backtrans_bleu": 63.219302642707405,
    "backtrans_chrf": 84.42589805494735,
    "backtrans_bertscore": 0.8743607997894287,
    "prof_backtrans_bleu": 37.00915656909173,
    "prof_backtrans_chrf": 71.34157613858699,
    "prof_backtrans_bertscore": 0.8540157675743103,
    "prof_backtrans_labse": 0.8665667772293091,
    "prof_backtrans_xlm_roberta": 0.8753979206085205,
    "llm_vs_prof_backtrans_bleu": 49.190268233286076,
    "llm_vs_prof_backtrans_chrf": 72.07826124449358,
    "llm_vs_prof_backtrans_bertscore": 0.9343373775482178,
    "llm_vs_prof_backtrans_labse": 0.9674137830734253
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 7.758976649758164,
    "same_lang_chrf": 44.30110451177316,
    "same_lang_bertscore": 0.748288094997406,
    "same_lang_comet": 0.8088012933731079,
    "cross_lang_xlm_roberta": 0.9569495916366577,
    "cross_lang_labse": 0.8794328570365906,
    "cross_lang_mbert": 0.9001917839050293,
    "cross_lang_comet_qe": 0.19072598218917847,
    "backtrans_bleu": 58.534791087883164,
    "backtrans_chrf": 81.5007541545277,
    "backtrans_bertscore": 0.8714693188667297,
    "prof_backtrans_bleu": 42.462379578709886,
    "prof_backtrans_chrf": 73.9229379549615,
    "prof_backtrans_bertscore": 0.8731173872947693,
    "prof_backtrans_labse": 0.8614357709884644,
    "prof_backtrans_xlm_roberta": 0.7847043871879578,
    "llm_vs_prof_backtrans_bleu": 45.71273233352351,
    "llm_vs_prof_backtrans_chrf": 75.26369647285438,
    "llm_vs_prof_backtrans_bertscore": 0.8454943895339966,
    "llm_vs_prof_backtrans_labse": 0.8349472880363464
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 26.186019511116715,
    "same_lang_chrf": 56.73228078209749,
    "same_lang_bertscore": 0.8020686507225037,
    "same_lang_comet": 0.6750794649124146,
    "cross_lang_xlm_roberta": 0.9928494095802307,
    "cross_lang_labse": 0.9833424091339111,
    "cross_lang_mbert": 0.9430953860282898,
    "cross_lang_comet_qe": 0.03777443617582321,
    "backtrans_bleu": 61.08568308217421,
    "backtrans_chrf": 83.9478631355248,
    "backtrans_bertscore": 0.9561318755149841,
    "prof_backtrans_bleu": 46.67708655564977,
    "prof_backtrans_chrf": 74.84838982829257,
    "prof_backtrans_bertscore": 0.9007406830787659,
    "prof_backtrans_labse": 0.9050307869911194,
    "prof_backtrans_xlm_roberta": 0.8569654226303101,
    "llm_vs_prof_backtrans_bleu": 44.779340269148165,
    "llm_vs_prof_backtrans_chrf": 72.73594186217407,
    "llm_vs_prof_backtrans_bertscore": 0.9001705050468445,
    "llm_vs_prof_backtrans_labse": 0.9147652983665466
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 46.94652729381961,
    "same_lang_chrf": 72.75080458668349,
    "same_lang_bertscore": 0.8350022435188293,
    "same_lang_comet": 0.7892405986785889,
    "cross_lang_xlm_roberta": 0.9849544763565063,
    "cross_lang_labse": 0.9866610169410706,
    "cross_lang_mbert": 0.9365646243095398,
    "cross_lang_comet_qe": 0.009891845285892487,
    "backtrans_bleu": 64.23873604727352,
    "backtrans_chrf": 85.29622350742675,
    "backtrans_bertscore": 0.9164637327194214,
    "prof_backtrans_bleu": 55.845800948151215,
    "prof_backtrans_chrf": 82.134350563164,
    "prof_backtrans_bertscore": 0.891668975353241,
    "prof_backtrans_labse": 0.8878930807113647,
    "prof_backtrans_xlm_roberta": 0.8931894302368164,
    "llm_vs_prof_backtrans_bleu": 62.15987542922175,
    "llm_vs_prof_backtrans_chrf": 84.14813590892737,
    "llm_vs_prof_backtrans_bertscore": 0.8906497955322266,
    "llm_vs_prof_backtrans_labse": 0.8953126668930054
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 7.694675936554923,
    "same_lang_chrf": 43.06039499139078,
    "same_lang_bertscore": 0.7955490350723267,
    "same_lang_comet": 0.8801725506782532,
    "cross_lang_xlm_roberta": 0.9766450524330139,
    "cross_lang_labse": 0.9618064761161804,
    "cross_lang_mbert": 0.8881498575210571,
    "cross_lang_comet_qe": 0.10563822090625763,
    "backtrans_bleu": 63.08012406612557,
    "backtrans_chrf": 83.60016138609893,
    "backtrans_bertscore": 0.9155286550521851,
    "prof_backtrans_bleu": 47.01549087988964,
    "prof_backtrans_chrf": 76.98757357389655,
    "prof_backtrans_bertscore": 0.9009394645690918,
    "prof_backtrans_labse": 0.9593333601951599,
    "prof_backtrans_xlm_roberta": 0.8470181226730347,
    "llm_vs_prof_backtrans_bleu": 49.46742293981141,
    "llm_vs_prof_backtrans_chrf": 76.04340026795046,
    "llm_vs_prof_backtrans_bertscore": 0.9056966304779053,
    "llm_vs_prof_backtrans_labse": 0.930281400680542
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 50.3798348810768,
    "same_lang_chrf": 69.78424242936799,
    "same_lang_bertscore": 0.9059843420982361,
    "same_lang_comet": 0.8852125406265259,
    "cross_lang_xlm_roberta": 0.9915757775306702,
    "cross_lang_labse": 0.9805879592895508,
    "cross_lang_mbert": 0.9511911273002625,
    "cross_lang_comet_qe": 0.03865181654691696,
    "backtrans_bleu": 67.31549867674146,
    "backtrans_chrf": 84.40889692659374,
    "backtrans_bertscore": 0.942046582698822,
    "prof_backtrans_bleu": 60.486224788086254,
    "prof_backtrans_chrf": 81.42566314526172,
    "prof_backtrans_bertscore": 0.9122533202171326,
    "prof_backtrans_labse": 0.9647113084793091,
    "prof_backtrans_xlm_roberta": 0.8339269757270813,
    "llm_vs_prof_backtrans_bleu": 66.81236879612902,
    "llm_vs_prof_backtrans_chrf": 85.98208351884799,
    "llm_vs_prof_backtrans_bertscore": 0.9275416135787964,
    "llm_vs_prof_backtrans_labse": 0.9813647270202637
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 46.7080804241293,
    "same_lang_chrf": 75.30646876285168,
    "same_lang_bertscore": 0.8355371356010437,
    "same_lang_comet": 0.8669617772102356,
    "cross_lang_xlm_roberta": 0.9946603178977966,
    "cross_lang_labse": 0.9906951189041138,
    "cross_lang_mbert": 0.9778314232826233,
    "cross_lang_comet_qe": 0.07210246473550797,
    "backtrans_bleu": 71.62926295054405,
    "backtrans_chrf": 88.67055502703538,
    "backtrans_bertscore": 0.9458888173103333,
    "prof_backtrans_bleu": 59.71070837750779,
    "prof_backtrans_chrf": 83.78638768017231,
    "prof_backtrans_bertscore": 0.8904657959938049,
    "prof_backtrans_labse": 0.8936939239501953,
    "prof_backtrans_xlm_roberta": 0.903261661529541,
    "llm_vs_prof_backtrans_bleu": 62.56027103993227,
    "llm_vs_prof_backtrans_chrf": 84.77511894443624,
    "llm_vs_prof_backtrans_bertscore": 0.8894909620285034,
    "llm_vs_prof_backtrans_labse": 0.8990044593811035
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 29.648981510523253,
    "same_lang_chrf": 66.60150071808285,
    "same_lang_bertscore": 0.8448046445846558,
    "same_lang_comet": 0.8168730735778809,
    "cross_lang_xlm_roberta": 0.9874560832977295,
    "cross_lang_labse": 0.9828575849533081,
    "cross_lang_mbert": 0.9325015544891357,
    "cross_lang_comet_qe": -0.07775118947029114,
    "backtrans_bleu": 57.93161667035718,
    "backtrans_chrf": 82.97266400940376,
    "backtrans_bertscore": 0.922322154045105,
    "prof_backtrans_bleu": 32.99331608675403,
    "prof_backtrans_chrf": 70.94653044726098,
    "prof_backtrans_bertscore": 0.8752748966217041,
    "prof_backtrans_labse": 0.9045588374137878,
    "prof_backtrans_xlm_roberta": 0.8227288722991943,
    "llm_vs_prof_backtrans_bleu": 38.49454427807942,
    "llm_vs_prof_backtrans_chrf": 68.3651357842208,
    "llm_vs_prof_backtrans_bertscore": 0.8776094913482666,
    "llm_vs_prof_backtrans_labse": 0.9037356972694397
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 5.458308449561086,
    "same_lang_chrf": 40.64445936701263,
    "same_lang_bertscore": 0.7593725323677063,
    "same_lang_comet": 0.823381245136261,
    "cross_lang_xlm_roberta": 0.9641482830047607,
    "cross_lang_labse": 0.9759446382522583,
    "cross_lang_mbert": 0.8747631311416626,
    "cross_lang_comet_qe": 0.1503310203552246,
    "backtrans_bleu": 60.333021098252814,
    "backtrans_chrf": 82.92595299164424,
    "backtrans_bertscore": 0.9168302416801453,
    "prof_backtrans_bleu": 61.065302132111455,
    "prof_backtrans_chrf": 84.0790226574755,
    "prof_backtrans_bertscore": 0.8702487945556641,
    "prof_backtrans_labse": 0.8562710881233215,
    "prof_backtrans_xlm_roberta": 0.7913957238197327,
    "llm_vs_prof_backtrans_bleu": 63.0173444337569,
    "llm_vs_prof_backtrans_chrf": 84.10893175218324,
    "llm_vs_prof_backtrans_bertscore": 0.8920857906341553,
    "llm_vs_prof_backtrans_labse": 0.8556613922119141
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 23.786141903027243,
    "same_lang_chrf": 55.4620404582542,
    "same_lang_bertscore": 0.8124454021453857,
    "same_lang_comet": 0.6876471042633057,
    "cross_lang_xlm_roberta": 0.9913349747657776,
    "cross_lang_labse": 0.9893683791160583,
    "cross_lang_mbert": 0.9383383393287659,
    "cross_lang_comet_qe": 0.030215203762054443,
    "backtrans_bleu": 73.9994535697665,
    "backtrans_chrf": 88.7788086557114,
    "backtrans_bertscore": 0.974510908126831,
    "prof_backtrans_bleu": 49.16830403229341,
    "prof_backtrans_chrf": 74.14965584191174,
    "prof_backtrans_bertscore": 0.867607593536377,
    "prof_backtrans_labse": 0.864550769329071,
    "prof_backtrans_xlm_roberta": 0.890460729598999,
    "llm_vs_prof_backtrans_bleu": 48.3096750646255,
    "llm_vs_prof_backtrans_chrf": 73.16282136310386,
    "llm_vs_prof_backtrans_bertscore": 0.8672565221786499,
    "llm_vs_prof_backtrans_labse": 0.8546168208122253
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 49.19312244007014,
    "same_lang_chrf": 72.80973789176021,
    "same_lang_bertscore": 0.8049497008323669,
    "same_lang_comet": 0.8084427118301392,
    "cross_lang_xlm_roberta": 0.9129303097724915,
    "cross_lang_labse": 0.8479206562042236,
    "cross_lang_mbert": 0.7912061810493469,
    "cross_lang_comet_qe": 0.3078794777393341,
    "backtrans_bleu": 62.78417208805679,
    "backtrans_chrf": 85.7350304058784,
    "backtrans_bertscore": 0.8711270689964294,
    "prof_backtrans_bleu": 48.511922855268764,
    "prof_backtrans_chrf": 77.56173500724182,
    "prof_backtrans_bertscore": 0.8698911666870117,
    "prof_backtrans_labse": 0.8770624399185181,
    "prof_backtrans_xlm_roberta": 0.9606432318687439,
    "llm_vs_prof_backtrans_bleu": 59.995447474776846,
    "llm_vs_prof_backtrans_chrf": 78.20264129603655,
    "llm_vs_prof_backtrans_bertscore": 0.9625002145767212,
    "llm_vs_prof_backtrans_labse": 0.9829931855201721
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 10.186952219217547,
    "same_lang_chrf": 45.37262015114451,
    "same_lang_bertscore": 0.725131630897522,
    "same_lang_comet": 0.8584644794464111,
    "cross_lang_xlm_roberta": 0.9213192462921143,
    "cross_lang_labse": 0.8419487476348877,
    "cross_lang_mbert": 0.7530327439308167,
    "cross_lang_comet_qe": 0.3241939842700958,
    "backtrans_bleu": 63.9684824817771,
    "backtrans_chrf": 84.82350263405647,
    "backtrans_bertscore": 0.8718193769454956,
    "prof_backtrans_bleu": 22.673978812386796,
    "prof_backtrans_chrf": 55.68742603845166,
    "prof_backtrans_bertscore": 0.8988362550735474,
    "prof_backtrans_labse": 0.9413300156593323,
    "prof_backtrans_xlm_roberta": 0.7987974882125854,
    "llm_vs_prof_backtrans_bleu": 31.30343224614954,
    "llm_vs_prof_backtrans_chrf": 72.40245193778321,
    "llm_vs_prof_backtrans_bertscore": 0.864208459854126,
    "llm_vs_prof_backtrans_labse": 0.7927110195159912
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 53.96633347324404,
    "same_lang_chrf": 72.42553126429905,
    "same_lang_bertscore": 0.8039674758911133,
    "same_lang_comet": 0.8463227152824402,
    "cross_lang_xlm_roberta": 0.9214247465133667,
    "cross_lang_labse": 0.8638609051704407,
    "cross_lang_mbert": 0.7965994477272034,
    "cross_lang_comet_qe": 0.2790954113006592,
    "backtrans_bleu": 73.61890935775072,
    "backtrans_chrf": 88.95842768259013,
    "backtrans_bertscore": 0.8761268258094788,
    "prof_backtrans_bleu": 70.20259518588807,
    "prof_backtrans_chrf": 86.06712116336561,
    "prof_backtrans_bertscore": 0.8688673973083496,
    "prof_backtrans_labse": 0.8634255528450012,
    "prof_backtrans_xlm_roberta": 0.7974357604980469,
    "llm_vs_prof_backtrans_bleu": 73.89678402804076,
    "llm_vs_prof_backtrans_chrf": 87.37864204759873,
    "llm_vs_prof_backtrans_bertscore": 0.9541379809379578,
    "llm_vs_prof_backtrans_labse": 0.9674254655838013
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 49.657513487937116,
    "same_lang_chrf": 79.0172696538478,
    "same_lang_bertscore": 0.8079203367233276,
    "same_lang_comet": 0.8486451506614685,
    "cross_lang_xlm_roberta": 0.9728915095329285,
    "cross_lang_labse": 0.8544744253158569,
    "cross_lang_mbert": 0.9086142778396606,
    "cross_lang_comet_qe": 0.27660882472991943,
    "backtrans_bleu": 66.12129266273674,
    "backtrans_chrf": 86.75175135818908,
    "backtrans_bertscore": 0.8766769170761108,
    "prof_backtrans_bleu": 66.08316586560697,
    "prof_backtrans_chrf": 86.42842509759575,
    "prof_backtrans_bertscore": 0.8709941506385803,
    "prof_backtrans_labse": 0.8467018008232117,
    "prof_backtrans_xlm_roberta": 0.8854868412017822,
    "llm_vs_prof_backtrans_bleu": 78.76381709784368,
    "llm_vs_prof_backtrans_chrf": 91.02494754699399,
    "llm_vs_prof_backtrans_bertscore": 0.9601868987083435,
    "llm_vs_prof_backtrans_labse": 0.9599443078041077
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 31.216792770736465,
    "same_lang_chrf": 64.8479292654492,
    "same_lang_bertscore": 0.8052681088447571,
    "same_lang_comet": 0.803299069404602,
    "cross_lang_xlm_roberta": 0.9160946607589722,
    "cross_lang_labse": 0.852685272693634,
    "cross_lang_mbert": 0.7545939683914185,
    "cross_lang_comet_qe": 0.31890326738357544,
    "backtrans_bleu": 60.931341721918564,
    "backtrans_chrf": 84.1526515851895,
    "backtrans_bertscore": 0.8719761967658997,
    "prof_backtrans_bleu": 29.935875747049234,
    "prof_backtrans_chrf": 66.88394101559628,
    "prof_backtrans_bertscore": 0.8612990379333496,
    "prof_backtrans_labse": 0.8587437868118286,
    "prof_backtrans_xlm_roberta": 0.8811516761779785,
    "llm_vs_prof_backtrans_bleu": 34.247148889070466,
    "llm_vs_prof_backtrans_chrf": 60.49596683471798,
    "llm_vs_prof_backtrans_bertscore": 0.913473904132843,
    "llm_vs_prof_backtrans_labse": 0.9332872033119202
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 7.052368073483986,
    "same_lang_chrf": 44.18012076053645,
    "same_lang_bertscore": 0.7426598072052002,
    "same_lang_comet": 0.8205159902572632,
    "cross_lang_xlm_roberta": 0.8873858451843262,
    "cross_lang_labse": 0.8208087682723999,
    "cross_lang_mbert": 0.725354790687561,
    "cross_lang_comet_qe": 0.28237926959991455,
    "backtrans_bleu": 63.00685625266681,
    "backtrans_chrf": 85.41978729443399,
    "backtrans_bertscore": 0.8665935397148132,
    "prof_backtrans_bleu": 58.91788492609094,
    "prof_backtrans_chrf": 83.58762213471046,
    "prof_backtrans_bertscore": 0.8767093420028687,
    "prof_backtrans_labse": 0.8554290533065796,
    "prof_backtrans_xlm_roberta": 0.7325628399848938,
    "llm_vs_prof_backtrans_bleu": 57.810401226718376,
    "llm_vs_prof_backtrans_chrf": 82.27006720907501,
    "llm_vs_prof_backtrans_bertscore": 0.9278467893600464,
    "llm_vs_prof_backtrans_labse": 0.7928603887557983
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 28.443408662071157,
    "same_lang_chrf": 59.6502406404005,
    "same_lang_bertscore": 0.7720855474472046,
    "same_lang_comet": 0.7576055526733398,
    "cross_lang_xlm_roberta": 0.9156875610351562,
    "cross_lang_labse": 0.8497083187103271,
    "cross_lang_mbert": 0.785546064376831,
    "cross_lang_comet_qe": 0.18730612099170685,
    "backtrans_bleu": 45.33119689797272,
    "backtrans_chrf": 77.5027443733389,
    "backtrans_bertscore": 0.8683969974517822,
    "prof_backtrans_bleu": 31.244012492135965,
    "prof_backtrans_chrf": 68.62919542966844,
    "prof_backtrans_bertscore": 0.8652605414390564,
    "prof_backtrans_labse": 0.8727200031280518,
    "prof_backtrans_xlm_roberta": 0.8519284129142761,
    "llm_vs_prof_backtrans_bleu": 34.003240889822216,
    "llm_vs_prof_backtrans_chrf": 65.77518570071817,
    "llm_vs_prof_backtrans_bertscore": 0.8813313841819763,
    "llm_vs_prof_backtrans_labse": 0.8770830035209656
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 56.81818186838807,
    "same_lang_chrf": 79.33112213220184,
    "same_lang_bertscore": 0.836226761341095,
    "same_lang_comet": 0.8244399428367615,
    "cross_lang_xlm_roberta": 0.8619766235351562,
    "cross_lang_labse": 0.8589950203895569,
    "cross_lang_mbert": 0.8107548356056213,
    "cross_lang_comet_qe": 0.27624621987342834,
    "backtrans_bleu": 51.20283973518563,
    "backtrans_chrf": 74.38253227036878,
    "backtrans_bertscore": 0.8546090722084045,
    "prof_backtrans_bleu": 36.756541257200745,
    "prof_backtrans_chrf": 73.00767603543909,
    "prof_backtrans_bertscore": 0.8870040774345398,
    "prof_backtrans_labse": 0.8963567614555359,
    "prof_backtrans_xlm_roberta": 0.9106748700141907,
    "llm_vs_prof_backtrans_bleu": 43.60206889836176,
    "llm_vs_prof_backtrans_chrf": 70.68197911260847,
    "llm_vs_prof_backtrans_bertscore": 0.8636095523834229,
    "llm_vs_prof_backtrans_labse": 0.855130136013031
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 6.206761021929501,
    "same_lang_chrf": 45.658522356305305,
    "same_lang_bertscore": 0.7771138548851013,
    "same_lang_comet": 0.8807598948478699,
    "cross_lang_xlm_roberta": 0.9535279870033264,
    "cross_lang_labse": 0.9553337097167969,
    "cross_lang_mbert": 0.842934787273407,
    "cross_lang_comet_qe": 0.30796754360198975,
    "backtrans_bleu": 50.50623831817794,
    "backtrans_chrf": 78.8972254604284,
    "backtrans_bertscore": 0.8938972353935242,
    "prof_backtrans_bleu": 26.37148180729954,
    "prof_backtrans_chrf": 66.68709644387278,
    "prof_backtrans_bertscore": 0.872597873210907,
    "prof_backtrans_labse": 0.9080414175987244,
    "prof_backtrans_xlm_roberta": 0.7951041460037231,
    "llm_vs_prof_backtrans_bleu": 33.324433164483615,
    "llm_vs_prof_backtrans_chrf": 71.68633158610069,
    "llm_vs_prof_backtrans_bertscore": 0.873045027256012,
    "llm_vs_prof_backtrans_labse": 0.8753851056098938
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 50.30832397750631,
    "same_lang_chrf": 68.7312233741429,
    "same_lang_bertscore": 0.7961241602897644,
    "same_lang_comet": 0.8501311540603638,
    "cross_lang_xlm_roberta": 0.9093208312988281,
    "cross_lang_labse": 0.8694247603416443,
    "cross_lang_mbert": 0.8161675333976746,
    "cross_lang_comet_qe": 0.30766019225120544,
    "backtrans_bleu": 60.99659786195104,
    "backtrans_chrf": 82.51447467745706,
    "backtrans_bertscore": 0.8768707513809204,
    "prof_backtrans_bleu": 51.90296902768928,
    "prof_backtrans_chrf": 75.76896431438726,
    "prof_backtrans_bertscore": 0.8651304244995117,
    "prof_backtrans_labse": 0.9113121628761292,
    "prof_backtrans_xlm_roberta": 0.8395708799362183,
    "llm_vs_prof_backtrans_bleu": 62.884230853482556,
    "llm_vs_prof_backtrans_chrf": 78.793173217649,
    "llm_vs_prof_backtrans_bertscore": 0.9266836047172546,
    "llm_vs_prof_backtrans_labse": 0.9435610771179199
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 42.4504375534609,
    "same_lang_chrf": 69.4293424582727,
    "same_lang_bertscore": 0.8059045672416687,
    "same_lang_comet": 0.8567603826522827,
    "cross_lang_xlm_roberta": 0.9431509971618652,
    "cross_lang_labse": 0.8715648651123047,
    "cross_lang_mbert": 0.8793525099754333,
    "cross_lang_comet_qe": 0.3029141128063202,
    "backtrans_bleu": 63.691681948688114,
    "backtrans_chrf": 83.95273870827471,
    "backtrans_bertscore": 0.8774033784866333,
    "prof_backtrans_bleu": 53.81955226101677,
    "prof_backtrans_chrf": 80.08401340999983,
    "prof_backtrans_bertscore": 0.874375581741333,
    "prof_backtrans_labse": 0.8577396869659424,
    "prof_backtrans_xlm_roberta": 0.8989928364753723,
    "llm_vs_prof_backtrans_bleu": 61.221304601643006,
    "llm_vs_prof_backtrans_chrf": 80.7836268046451,
    "llm_vs_prof_backtrans_bertscore": 0.9526720643043518,
    "llm_vs_prof_backtrans_labse": 0.9585160613059998
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 29.64999044197874,
    "same_lang_chrf": 65.36792563356461,
    "same_lang_bertscore": 0.8138472437858582,
    "same_lang_comet": 0.8096215724945068,
    "cross_lang_xlm_roberta": 0.9225050210952759,
    "cross_lang_labse": 0.8842213153839111,
    "cross_lang_mbert": 0.8141800165176392,
    "cross_lang_comet_qe": 0.24593394994735718,
    "backtrans_bleu": 44.73419221948426,
    "backtrans_chrf": 70.87919813624882,
    "backtrans_bertscore": 0.8656086325645447,
    "prof_backtrans_bleu": 31.935036333602827,
    "prof_backtrans_chrf": 68.89175404916504,
    "prof_backtrans_bertscore": 0.8589839935302734,
    "prof_backtrans_labse": 0.8632590770721436,
    "prof_backtrans_xlm_roberta": 0.853482186794281,
    "llm_vs_prof_backtrans_bleu": 38.69001640925888,
    "llm_vs_prof_backtrans_chrf": 65.03792310077924,
    "llm_vs_prof_backtrans_bertscore": 0.9099258184432983,
    "llm_vs_prof_backtrans_labse": 0.9635346531867981
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 5.441476337176137,
    "same_lang_chrf": 40.53858385654779,
    "same_lang_bertscore": 0.7571880221366882,
    "same_lang_comet": 0.8215992450714111,
    "cross_lang_xlm_roberta": 0.9023672342300415,
    "cross_lang_labse": 0.8391976952552795,
    "cross_lang_mbert": 0.8098239898681641,
    "cross_lang_comet_qe": 0.28219372034072876,
    "backtrans_bleu": 44.7337025192014,
    "backtrans_chrf": 76.0218649211515,
    "backtrans_bertscore": 0.8720138072967529,
    "prof_backtrans_bleu": 42.315503371887075,
    "prof_backtrans_chrf": 75.23272473262072,
    "prof_backtrans_bertscore": 0.8706954717636108,
    "prof_backtrans_labse": 0.8763861656188965,
    "prof_backtrans_xlm_roberta": 0.7840132117271423,
    "llm_vs_prof_backtrans_bleu": 50.44282748425906,
    "llm_vs_prof_backtrans_chrf": 77.73177459444493,
    "llm_vs_prof_backtrans_bertscore": 0.8936798572540283,
    "llm_vs_prof_backtrans_labse": 0.7946391105651855
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 21.750327731490984,
    "same_lang_chrf": 53.06581810024908,
    "same_lang_bertscore": 0.7742257714271545,
    "same_lang_comet": 0.7275509834289551,
    "cross_lang_xlm_roberta": 0.9402369856834412,
    "cross_lang_labse": 0.882047176361084,
    "cross_lang_mbert": 0.9429395198822021,
    "cross_lang_comet_qe": 0.21727991104125977,
    "backtrans_bleu": 67.08168808818412,
    "backtrans_chrf": 85.12212460253215,
    "backtrans_bertscore": 0.8904296159744263,
    "prof_backtrans_bleu": 22.048286187718617,
    "prof_backtrans_chrf": 59.40413735307734,
    "prof_backtrans_bertscore": 0.8743711113929749,
    "prof_backtrans_labse": 0.9067175388336182,
    "prof_backtrans_xlm_roberta": 0.8823119401931763,
    "llm_vs_prof_backtrans_bleu": 24.847922431558978,
    "llm_vs_prof_backtrans_chrf": 62.9588910849047,
    "llm_vs_prof_backtrans_bertscore": 0.8770679831504822,
    "llm_vs_prof_backtrans_labse": 0.8736834526062012
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 61.76195410686026,
    "same_lang_chrf": 80.0953398022124,
    "same_lang_bertscore": 0.9235338568687439,
    "same_lang_comet": 0.9098706245422363,
    "cross_lang_xlm_roberta": 0.9897477626800537,
    "cross_lang_labse": 0.9940829873085022,
    "cross_lang_mbert": 0.9869117736816406,
    "cross_lang_comet_qe": 0.5135585069656372,
    "backtrans_bleu": 79.89588496472841,
    "backtrans_chrf": 90.86977345443565,
    "backtrans_bertscore": 0.9841063618659973,
    "prof_backtrans_bleu": 69.67667807888009,
    "prof_backtrans_chrf": 85.29687062932639,
    "prof_backtrans_bertscore": 0.9505612850189209,
    "prof_backtrans_labse": 0.9912337064743042,
    "prof_backtrans_xlm_roberta": 0.9870409369468689,
    "llm_vs_prof_backtrans_bleu": 78.72888438992516,
    "llm_vs_prof_backtrans_chrf": 88.30965411903239,
    "llm_vs_prof_backtrans_bertscore": 0.960177481174469,
    "llm_vs_prof_backtrans_labse": 0.9920178651809692
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 17.638505395526714,
    "same_lang_chrf": 45.318829617428975,
    "same_lang_bertscore": 0.869452178478241,
    "same_lang_comet": 0.9176890850067139,
    "cross_lang_xlm_roberta": 0.9870913624763489,
    "cross_lang_labse": 0.9921576976776123,
    "cross_lang_mbert": 0.9815043807029724,
    "cross_lang_comet_qe": 0.5323266983032227,
    "backtrans_bleu": 62.172766365499825,
    "backtrans_chrf": 82.3668886483609,
    "backtrans_bertscore": 0.9699660539627075,
    "prof_backtrans_bleu": 63.351172557622235,
    "prof_backtrans_chrf": 81.98795532490108,
    "prof_backtrans_bertscore": 0.9740211963653564,
    "prof_backtrans_labse": 0.9890226125717163,
    "prof_backtrans_xlm_roberta": 0.9933446049690247,
    "llm_vs_prof_backtrans_bleu": 64.18288576744182,
    "llm_vs_prof_backtrans_chrf": 83.06216726163896,
    "llm_vs_prof_backtrans_bertscore": 0.9709576368331909,
    "llm_vs_prof_backtrans_labse": 0.9875773191452026
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 52.010612381591855,
    "same_lang_chrf": 68.87954354208141,
    "same_lang_bertscore": 0.8853127360343933,
    "same_lang_comet": 0.9132068753242493,
    "cross_lang_xlm_roberta": 0.9880791306495667,
    "cross_lang_labse": 0.9894752502441406,
    "cross_lang_mbert": 0.9798305630683899,
    "cross_lang_comet_qe": 0.5034083127975464,
    "backtrans_bleu": 69.99466617004614,
    "backtrans_chrf": 84.94715811558024,
    "backtrans_bertscore": 0.9713973999023438,
    "prof_backtrans_bleu": 64.2962204219148,
    "prof_backtrans_chrf": 82.71402756693261,
    "prof_backtrans_bertscore": 0.9630183577537537,
    "prof_backtrans_labse": 0.9848119616508484,
    "prof_backtrans_xlm_roberta": 0.9223819971084595,
    "llm_vs_prof_backtrans_bleu": 67.9900161868448,
    "llm_vs_prof_backtrans_chrf": 82.92647635645707,
    "llm_vs_prof_backtrans_bertscore": 0.96929532289505,
    "llm_vs_prof_backtrans_labse": 0.9812175035476685
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 44.140630476315465,
    "same_lang_chrf": 74.27536542268861,
    "same_lang_bertscore": 0.876899242401123,
    "same_lang_comet": 0.9135541319847107,
    "cross_lang_xlm_roberta": 0.9942260384559631,
    "cross_lang_labse": 0.995144248008728,
    "cross_lang_mbert": 0.9925392270088196,
    "cross_lang_comet_qe": 0.5253217816352844,
    "backtrans_bleu": 73.89529772734936,
    "backtrans_chrf": 87.23811573132842,
    "backtrans_bertscore": 0.9750217199325562,
    "prof_backtrans_bleu": 73.34297957214729,
    "prof_backtrans_chrf": 87.35193694596813,
    "prof_backtrans_bertscore": 0.9422835111618042,
    "prof_backtrans_labse": 0.9838301539421082,
    "prof_backtrans_xlm_roberta": 0.9503243565559387,
    "llm_vs_prof_backtrans_bleu": 77.33277216807268,
    "llm_vs_prof_backtrans_chrf": 88.14840858462219,
    "llm_vs_prof_backtrans_bertscore": 0.9451221823692322,
    "llm_vs_prof_backtrans_labse": 0.9798536896705627
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 39.09984647801404,
    "same_lang_chrf": 68.63238051482237,
    "same_lang_bertscore": 0.8843520283699036,
    "same_lang_comet": 0.9329841732978821,
    "cross_lang_xlm_roberta": 0.9834938645362854,
    "cross_lang_labse": 0.9877484440803528,
    "cross_lang_mbert": 0.9813095927238464,
    "cross_lang_comet_qe": 0.5028724670410156,
    "backtrans_bleu": 61.10387911940401,
    "backtrans_chrf": 81.00964763797127,
    "backtrans_bertscore": 0.9709212183952332,
    "prof_backtrans_bleu": 57.73283661443606,
    "prof_backtrans_chrf": 80.58886171372176,
    "prof_backtrans_bertscore": 0.9433928728103638,
    "prof_backtrans_labse": 0.973760724067688,
    "prof_backtrans_xlm_roberta": 0.9650502800941467,
    "llm_vs_prof_backtrans_bleu": 62.87309501938378,
    "llm_vs_prof_backtrans_chrf": 81.23090826462943,
    "llm_vs_prof_backtrans_bertscore": 0.9520659446716309,
    "llm_vs_prof_backtrans_labse": 0.9749888777732849
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 28.879354993539636,
    "same_lang_chrf": 47.89419438322879,
    "same_lang_bertscore": 0.8840300440788269,
    "same_lang_comet": 0.9286187291145325,
    "cross_lang_xlm_roberta": 0.9903995990753174,
    "cross_lang_labse": 0.9888033866882324,
    "cross_lang_mbert": 0.9929744601249695,
    "cross_lang_comet_qe": 0.5201330780982971,
    "backtrans_bleu": 62.620602742768284,
    "backtrans_chrf": 83.33458859771144,
    "backtrans_bertscore": 0.9704086184501648,
    "prof_backtrans_bleu": 62.45408511844086,
    "prof_backtrans_chrf": 82.06764510936154,
    "prof_backtrans_bertscore": 0.9678582549095154,
    "prof_backtrans_labse": 0.9853694438934326,
    "prof_backtrans_xlm_roberta": 0.9042583107948303,
    "llm_vs_prof_backtrans_bleu": 64.5183223053243,
    "llm_vs_prof_backtrans_chrf": 83.81378439459661,
    "llm_vs_prof_backtrans_bertscore": 0.9668459892272949,
    "llm_vs_prof_backtrans_labse": 0.9879359602928162
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 48.722989399887396,
    "same_lang_chrf": 72.9647917508823,
    "same_lang_bertscore": 0.8705904483795166,
    "same_lang_comet": 0.7994842529296875,
    "cross_lang_xlm_roberta": 0.9875087141990662,
    "cross_lang_labse": 0.9896265268325806,
    "cross_lang_mbert": 0.9926484823226929,
    "cross_lang_comet_qe": 0.47131335735321045,
    "backtrans_bleu": 70.1230589055332,
    "backtrans_chrf": 86.18102721827535,
    "backtrans_bertscore": 0.9739901423454285,
    "prof_backtrans_bleu": 72.77748384035202,
    "prof_backtrans_chrf": 86.96712563613266,
    "prof_backtrans_bertscore": 0.9474964141845703,
    "prof_backtrans_labse": 0.9866588115692139,
    "prof_backtrans_xlm_roberta": 0.8755951523780823,
    "llm_vs_prof_backtrans_bleu": 77.14324821487844,
    "llm_vs_prof_backtrans_chrf": 88.32939096350779,
    "llm_vs_prof_backtrans_bertscore": 0.9499874114990234,
    "llm_vs_prof_backtrans_labse": 0.9756355285644531
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 64.10889717199846,
    "same_lang_chrf": 82.08683423810645,
    "same_lang_bertscore": 0.9191926717758179,
    "same_lang_comet": 0.91419517993927,
    "cross_lang_xlm_roberta": 0.9947072863578796,
    "cross_lang_labse": 0.9939658641815186,
    "cross_lang_mbert": 0.9846777319908142,
    "cross_lang_comet_qe": 0.5342915058135986,
    "backtrans_bleu": 81.85030284199028,
    "backtrans_chrf": 91.01829070652916,
    "backtrans_bertscore": 0.9851298332214355,
    "prof_backtrans_bleu": 67.25891343461612,
    "prof_backtrans_chrf": 83.43790629390537,
    "prof_backtrans_bertscore": 0.9703678488731384,
    "prof_backtrans_labse": 0.9939200282096863,
    "prof_backtrans_xlm_roberta": 0.9916396737098694,
    "llm_vs_prof_backtrans_bleu": 75.71170299610478,
    "llm_vs_prof_backtrans_chrf": 86.6596768764632,
    "llm_vs_prof_backtrans_bertscore": 0.9710707068443298,
    "llm_vs_prof_backtrans_labse": 0.9910929799079895
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.970241303441025,
    "same_lang_chrf": 57.563666952822004,
    "same_lang_bertscore": 0.9326399564743042,
    "same_lang_comet": 0.93532794713974,
    "cross_lang_xlm_roberta": 0.9887142181396484,
    "cross_lang_labse": 0.9897013902664185,
    "cross_lang_mbert": 0.949307918548584,
    "cross_lang_comet_qe": 0.5261772274971008,
    "backtrans_bleu": 63.49234034973168,
    "backtrans_chrf": 81.83640438482074,
    "backtrans_bertscore": 0.975564181804657,
    "prof_backtrans_bleu": 58.95018479254303,
    "prof_backtrans_chrf": 79.6596931088159,
    "prof_backtrans_bertscore": 0.9709610342979431,
    "prof_backtrans_labse": 0.991561770439148,
    "prof_backtrans_xlm_roberta": 0.991004228591919,
    "llm_vs_prof_backtrans_bleu": 64.65934838631179,
    "llm_vs_prof_backtrans_chrf": 82.25556617910416,
    "llm_vs_prof_backtrans_bertscore": 0.9688360691070557,
    "llm_vs_prof_backtrans_labse": 0.9866001605987549
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 51.26501635114342,
    "same_lang_chrf": 67.39512377931939,
    "same_lang_bertscore": 0.8966371417045593,
    "same_lang_comet": 0.9146051406860352,
    "cross_lang_xlm_roberta": 0.9834367632865906,
    "cross_lang_labse": 0.9933719635009766,
    "cross_lang_mbert": 0.9790289402008057,
    "cross_lang_comet_qe": 0.4873943328857422,
    "backtrans_bleu": 65.78247632266321,
    "backtrans_chrf": 81.79726082661725,
    "backtrans_bertscore": 0.976233184337616,
    "prof_backtrans_bleu": 57.98466223372168,
    "prof_backtrans_chrf": 79.01948550068752,
    "prof_backtrans_bertscore": 0.9577056765556335,
    "prof_backtrans_labse": 0.9809112548828125,
    "prof_backtrans_xlm_roberta": 0.9276070594787598,
    "llm_vs_prof_backtrans_bleu": 63.4049925312599,
    "llm_vs_prof_backtrans_chrf": 79.6252232313418,
    "llm_vs_prof_backtrans_bertscore": 0.9644573330879211,
    "llm_vs_prof_backtrans_labse": 0.9800838828086853
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 48.196299123111245,
    "same_lang_chrf": 76.7989358087655,
    "same_lang_bertscore": 0.8632772564888,
    "same_lang_comet": 0.9190346002578735,
    "cross_lang_xlm_roberta": 0.9955958724021912,
    "cross_lang_labse": 0.9952535033226013,
    "cross_lang_mbert": 0.9967371821403503,
    "cross_lang_comet_qe": 0.5180414319038391,
    "backtrans_bleu": 77.8638907473742,
    "backtrans_chrf": 89.04063417330126,
    "backtrans_bertscore": 0.9866524934768677,
    "prof_backtrans_bleu": 71.35650495831202,
    "prof_backtrans_chrf": 85.82007947603915,
    "prof_backtrans_bertscore": 0.9686979055404663,
    "prof_backtrans_labse": 0.9842978119850159,
    "prof_backtrans_xlm_roberta": 0.9516193866729736,
    "llm_vs_prof_backtrans_bleu": 77.15702096703046,
    "llm_vs_prof_backtrans_chrf": 87.903943678961,
    "llm_vs_prof_backtrans_bertscore": 0.9724248647689819,
    "llm_vs_prof_backtrans_labse": 0.9816628694534302
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 37.16134952553589,
    "same_lang_chrf": 70.0740535360124,
    "same_lang_bertscore": 0.8678554892539978,
    "same_lang_comet": 0.9368287324905396,
    "cross_lang_xlm_roberta": 0.9929956197738647,
    "cross_lang_labse": 0.9872099161148071,
    "cross_lang_mbert": 0.9907007217407227,
    "cross_lang_comet_qe": 0.5204746723175049,
    "backtrans_bleu": 57.25851065122173,
    "backtrans_chrf": 78.22245932799242,
    "backtrans_bertscore": 0.9657163023948669,
    "prof_backtrans_bleu": 55.457265902512844,
    "prof_backtrans_chrf": 78.99449257839622,
    "prof_backtrans_bertscore": 0.9590340852737427,
    "prof_backtrans_labse": 0.9768620133399963,
    "prof_backtrans_xlm_roberta": 0.9755079746246338,
    "llm_vs_prof_backtrans_bleu": 56.944885058463164,
    "llm_vs_prof_backtrans_chrf": 78.47582043404073,
    "llm_vs_prof_backtrans_bertscore": 0.9468334913253784,
    "llm_vs_prof_backtrans_labse": 0.9646832942962646
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 33.643805396472594,
    "same_lang_chrf": 52.13209271630106,
    "same_lang_bertscore": 0.9157524704933167,
    "same_lang_comet": 0.9377885460853577,
    "cross_lang_xlm_roberta": 0.9948092699050903,
    "cross_lang_labse": 0.9909223914146423,
    "cross_lang_mbert": 0.9943303465843201,
    "cross_lang_comet_qe": 0.5074694156646729,
    "backtrans_bleu": 63.093264914835665,
    "backtrans_chrf": 82.19023368533371,
    "backtrans_bertscore": 0.9757452011108398,
    "prof_backtrans_bleu": 59.1079468291771,
    "prof_backtrans_chrf": 80.47713481169724,
    "prof_backtrans_bertscore": 0.9726200103759766,
    "prof_backtrans_labse": 0.9845969676971436,
    "prof_backtrans_xlm_roberta": 0.9317832589149475,
    "llm_vs_prof_backtrans_bleu": 65.87948611293146,
    "llm_vs_prof_backtrans_chrf": 83.75781459846074,
    "llm_vs_prof_backtrans_bertscore": 0.9710034728050232,
    "llm_vs_prof_backtrans_labse": 0.9860315918922424
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 50.49261550454572,
    "same_lang_chrf": 73.05232779299348,
    "same_lang_bertscore": 0.8842843174934387,
    "same_lang_comet": 0.8154480457305908,
    "cross_lang_xlm_roberta": 0.9912794828414917,
    "cross_lang_labse": 0.9951237440109253,
    "cross_lang_mbert": 0.9958325028419495,
    "cross_lang_comet_qe": 0.49952125549316406,
    "backtrans_bleu": 78.64993613720256,
    "backtrans_chrf": 89.26452412764564,
    "backtrans_bertscore": 0.9849624037742615,
    "prof_backtrans_bleu": 68.6798236950083,
    "prof_backtrans_chrf": 84.72262960538418,
    "prof_backtrans_bertscore": 0.9710431098937988,
    "prof_backtrans_labse": 0.9859828948974609,
    "prof_backtrans_xlm_roberta": 0.8699209094047546,
    "llm_vs_prof_backtrans_bleu": 75.65923865802209,
    "llm_vs_prof_backtrans_chrf": 87.09480755040326,
    "llm_vs_prof_backtrans_bertscore": 0.9730449318885803,
    "llm_vs_prof_backtrans_labse": 0.9795145988464355
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 71.58019210683192,
    "same_lang_chrf": 85.58778080685734,
    "same_lang_bertscore": 0.939075231552124,
    "same_lang_comet": 0.9275282621383667,
    "cross_lang_xlm_roberta": 0.9809807538986206,
    "cross_lang_labse": 0.9881762862205505,
    "cross_lang_mbert": 0.9812643527984619,
    "cross_lang_comet_qe": 0.5091635584831238,
    "backtrans_bleu": 74.67145517333186,
    "backtrans_chrf": 88.1034976313604,
    "backtrans_bertscore": 0.9715415835380554,
    "prof_backtrans_bleu": 67.92502562400439,
    "prof_backtrans_chrf": 84.32734900451969,
    "prof_backtrans_bertscore": 0.9668048024177551,
    "prof_backtrans_labse": 0.9902182817459106,
    "prof_backtrans_xlm_roberta": 0.9803618788719177,
    "llm_vs_prof_backtrans_bleu": 76.29443988801164,
    "llm_vs_prof_backtrans_chrf": 87.87907016830613,
    "llm_vs_prof_backtrans_bertscore": 0.9669173955917358,
    "llm_vs_prof_backtrans_labse": 0.9799912571907043
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 20.826207153971755,
    "same_lang_chrf": 59.28847810864222,
    "same_lang_bertscore": 0.9331311583518982,
    "same_lang_comet": 0.9374357461929321,
    "cross_lang_xlm_roberta": 0.9937676191329956,
    "cross_lang_labse": 0.9932882189750671,
    "cross_lang_mbert": 0.9885866045951843,
    "cross_lang_comet_qe": 0.5361361503601074,
    "backtrans_bleu": 61.134706685560076,
    "backtrans_chrf": 80.46384060743166,
    "backtrans_bertscore": 0.9739581346511841,
    "prof_backtrans_bleu": 48.543714506044445,
    "prof_backtrans_chrf": 74.57402357571786,
    "prof_backtrans_bertscore": 0.9648045897483826,
    "prof_backtrans_labse": 0.991689920425415,
    "prof_backtrans_xlm_roberta": 0.9893668293952942,
    "llm_vs_prof_backtrans_bleu": 63.24531811455087,
    "llm_vs_prof_backtrans_chrf": 80.41732701577443,
    "llm_vs_prof_backtrans_bertscore": 0.9765486121177673,
    "llm_vs_prof_backtrans_labse": 0.9950500726699829
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 64.4904294816615,
    "same_lang_chrf": 76.8131465186989,
    "same_lang_bertscore": 0.9169982075691223,
    "same_lang_comet": 0.9297130107879639,
    "cross_lang_xlm_roberta": 0.9889206886291504,
    "cross_lang_labse": 0.9890245795249939,
    "cross_lang_mbert": 0.9818155169487,
    "cross_lang_comet_qe": 0.48042625188827515,
    "backtrans_bleu": 64.56019424219365,
    "backtrans_chrf": 81.67937395271285,
    "backtrans_bertscore": 0.9742085933685303,
    "prof_backtrans_bleu": 58.28177117076769,
    "prof_backtrans_chrf": 79.47660750232293,
    "prof_backtrans_bertscore": 0.9563548564910889,
    "prof_backtrans_labse": 0.981056809425354,
    "prof_backtrans_xlm_roberta": 0.9297171235084534,
    "llm_vs_prof_backtrans_bleu": 67.64376443233219,
    "llm_vs_prof_backtrans_chrf": 82.56926892045587,
    "llm_vs_prof_backtrans_bertscore": 0.9566016793251038,
    "llm_vs_prof_backtrans_labse": 0.980197548866272
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 40.959738203187435,
    "same_lang_chrf": 74.01849207692732,
    "same_lang_bertscore": 0.8483451008796692,
    "same_lang_comet": 0.9017478227615356,
    "cross_lang_xlm_roberta": 0.9794694781303406,
    "cross_lang_labse": 0.944404125213623,
    "cross_lang_mbert": 0.9856925010681152,
    "cross_lang_comet_qe": 0.433940589427948,
    "backtrans_bleu": 60.112879168522134,
    "backtrans_chrf": 82.89036474290698,
    "backtrans_bertscore": 0.9438748955726624,
    "prof_backtrans_bleu": 68.99925119050158,
    "prof_backtrans_chrf": 85.67273462646804,
    "prof_backtrans_bertscore": 0.9599968791007996,
    "prof_backtrans_labse": 0.9830469489097595,
    "prof_backtrans_xlm_roberta": 0.9425691962242126,
    "llm_vs_prof_backtrans_bleu": 60.9582458944372,
    "llm_vs_prof_backtrans_chrf": 81.69256939011031,
    "llm_vs_prof_backtrans_bertscore": 0.9547114372253418,
    "llm_vs_prof_backtrans_labse": 0.9433026909828186
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 41.39986348896751,
    "same_lang_chrf": 71.80775082773168,
    "same_lang_bertscore": 0.8864716291427612,
    "same_lang_comet": 0.9378142356872559,
    "cross_lang_xlm_roberta": 0.9957820177078247,
    "cross_lang_labse": 0.9896894693374634,
    "cross_lang_mbert": 0.9933006763458252,
    "cross_lang_comet_qe": 0.5358657836914062,
    "backtrans_bleu": 58.55343503644478,
    "backtrans_chrf": 79.49705538862655,
    "backtrans_bertscore": 0.9669316411018372,
    "prof_backtrans_bleu": 46.19039285821175,
    "prof_backtrans_chrf": 74.62701915741906,
    "prof_backtrans_bertscore": 0.9374545812606812,
    "prof_backtrans_labse": 0.9657807350158691,
    "prof_backtrans_xlm_roberta": 0.9661949276924133,
    "llm_vs_prof_backtrans_bleu": 51.598274117406774,
    "llm_vs_prof_backtrans_chrf": 74.58609221588213,
    "llm_vs_prof_backtrans_bertscore": 0.9411327242851257,
    "llm_vs_prof_backtrans_labse": 0.959550678730011
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 33.743169789093585,
    "same_lang_chrf": 55.12991433644843,
    "same_lang_bertscore": 0.9054175615310669,
    "same_lang_comet": 0.9340134859085083,
    "cross_lang_xlm_roberta": 0.9893576502799988,
    "cross_lang_labse": 0.9889065623283386,
    "cross_lang_mbert": 0.9885286092758179,
    "cross_lang_comet_qe": 0.49695366621017456,
    "backtrans_bleu": 50.047929031044845,
    "backtrans_chrf": 75.33094558800599,
    "backtrans_bertscore": 0.9626720547676086,
    "prof_backtrans_bleu": 49.15148441118233,
    "prof_backtrans_chrf": 75.52883117815622,
    "prof_backtrans_bertscore": 0.9628216028213501,
    "prof_backtrans_labse": 0.9790014624595642,
    "prof_backtrans_xlm_roberta": 0.913114070892334,
    "llm_vs_prof_backtrans_bleu": 57.43655765093892,
    "llm_vs_prof_backtrans_chrf": 79.72130855413576,
    "llm_vs_prof_backtrans_bertscore": 0.9600141048431396,
    "llm_vs_prof_backtrans_labse": 0.9795291423797607
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 55.84294327314628,
    "same_lang_chrf": 76.56392562281084,
    "same_lang_bertscore": 0.8935659527778625,
    "same_lang_comet": 0.8323209285736084,
    "cross_lang_xlm_roberta": 0.9891758561134338,
    "cross_lang_labse": 0.9949676394462585,
    "cross_lang_mbert": 0.9937140345573425,
    "cross_lang_comet_qe": 0.4941788613796234,
    "backtrans_bleu": 71.09016374567818,
    "backtrans_chrf": 86.3728128353726,
    "backtrans_bertscore": 0.9819313287734985,
    "prof_backtrans_bleu": 61.18954900566833,
    "prof_backtrans_chrf": 81.07129293187302,
    "prof_backtrans_bertscore": 0.964378297328949,
    "prof_backtrans_labse": 0.9866886138916016,
    "prof_backtrans_xlm_roberta": 0.8705088496208191,
    "llm_vs_prof_backtrans_bleu": 70.15614842977436,
    "llm_vs_prof_backtrans_chrf": 83.03851359594965,
    "llm_vs_prof_backtrans_bertscore": 0.9682446122169495,
    "llm_vs_prof_backtrans_labse": 0.9835343360900879
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 62.52598694005732,
    "same_lang_chrf": 80.79269470708597,
    "same_lang_bertscore": 0.9166770577430725,
    "same_lang_comet": 0.9072645902633667,
    "cross_lang_xlm_roberta": 0.9765351414680481,
    "cross_lang_labse": 0.9898784160614014,
    "cross_lang_mbert": 0.9768421649932861,
    "cross_lang_comet_qe": 0.4868774116039276,
    "backtrans_bleu": 63.122547183810646,
    "backtrans_chrf": 82.37596712787368,
    "backtrans_bertscore": 0.9678049683570862,
    "prof_backtrans_bleu": 49.32777878664825,
    "prof_backtrans_chrf": 75.52015952643029,
    "prof_backtrans_bertscore": 0.9520280957221985,
    "prof_backtrans_labse": 0.9868030548095703,
    "prof_backtrans_xlm_roberta": 0.9744859933853149,
    "llm_vs_prof_backtrans_bleu": 66.88207189657153,
    "llm_vs_prof_backtrans_chrf": 80.4878864802633,
    "llm_vs_prof_backtrans_bertscore": 0.9683395028114319,
    "llm_vs_prof_backtrans_labse": 0.9867867231369019
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 16.533087013160806,
    "same_lang_chrf": 56.92264950792626,
    "same_lang_bertscore": 0.9227312803268433,
    "same_lang_comet": 0.9367709159851074,
    "cross_lang_xlm_roberta": 0.9938081502914429,
    "cross_lang_labse": 0.9883292317390442,
    "cross_lang_mbert": 0.9909954071044922,
    "cross_lang_comet_qe": 0.5334150791168213,
    "backtrans_bleu": 52.33379435425691,
    "backtrans_chrf": 75.15258324085607,
    "backtrans_bertscore": 0.9679673314094543,
    "prof_backtrans_bleu": 51.28253592107906,
    "prof_backtrans_chrf": 75.10726667862917,
    "prof_backtrans_bertscore": 0.9666115045547485,
    "prof_backtrans_labse": 0.9893819093704224,
    "prof_backtrans_xlm_roberta": 0.9923412203788757,
    "llm_vs_prof_backtrans_bleu": 66.98584234733075,
    "llm_vs_prof_backtrans_chrf": 82.92603100678981,
    "llm_vs_prof_backtrans_bertscore": 0.9741145968437195,
    "llm_vs_prof_backtrans_labse": 0.9922708868980408
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 51.06760249038071,
    "same_lang_chrf": 67.51489861810775,
    "same_lang_bertscore": 0.8859817981719971,
    "same_lang_comet": 0.9194815158843994,
    "cross_lang_xlm_roberta": 0.9868777394294739,
    "cross_lang_labse": 0.9832782745361328,
    "cross_lang_mbert": 0.9884713888168335,
    "cross_lang_comet_qe": 0.520807147026062,
    "backtrans_bleu": 59.832272565218005,
    "backtrans_chrf": 78.91315828449895,
    "backtrans_bertscore": 0.9696448445320129,
    "prof_backtrans_bleu": 52.95554113936037,
    "prof_backtrans_chrf": 76.61271626342054,
    "prof_backtrans_bertscore": 0.9486920833587646,
    "prof_backtrans_labse": 0.9752880334854126,
    "prof_backtrans_xlm_roberta": 0.9222074151039124,
    "llm_vs_prof_backtrans_bleu": 63.15501873301192,
    "llm_vs_prof_backtrans_chrf": 80.44565397844656,
    "llm_vs_prof_backtrans_bertscore": 0.9584735035896301,
    "llm_vs_prof_backtrans_labse": 0.9618497490882874
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 45.8652589403112,
    "same_lang_chrf": 73.54177375870974,
    "same_lang_bertscore": 0.858191192150116,
    "same_lang_comet": 0.9106302261352539,
    "cross_lang_xlm_roberta": 0.990680992603302,
    "cross_lang_labse": 0.9877054691314697,
    "cross_lang_mbert": 0.9878876805305481,
    "cross_lang_comet_qe": 0.4371526837348938,
    "backtrans_bleu": 66.49542116276585,
    "backtrans_chrf": 82.8561455514298,
    "backtrans_bertscore": 0.9770393371582031,
    "prof_backtrans_bleu": 62.56345966201683,
    "prof_backtrans_chrf": 81.93994041948096,
    "prof_backtrans_bertscore": 0.9523444175720215,
    "prof_backtrans_labse": 0.9782193303108215,
    "prof_backtrans_xlm_roberta": 0.9393059015274048,
    "llm_vs_prof_backtrans_bleu": 64.7621503607868,
    "llm_vs_prof_backtrans_chrf": 81.82650744461199,
    "llm_vs_prof_backtrans_bertscore": 0.9498049020767212,
    "llm_vs_prof_backtrans_labse": 0.9655042886734009
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 41.00514066823909,
    "same_lang_chrf": 70.64571433354033,
    "same_lang_bertscore": 0.8702538013458252,
    "same_lang_comet": 0.9361907839775085,
    "cross_lang_xlm_roberta": 0.9910966157913208,
    "cross_lang_labse": 0.9848147630691528,
    "cross_lang_mbert": 0.9792566895484924,
    "cross_lang_comet_qe": 0.4950700104236603,
    "backtrans_bleu": 55.228459623630556,
    "backtrans_chrf": 77.33048022140788,
    "backtrans_bertscore": 0.9641692638397217,
    "prof_backtrans_bleu": 32.34457380372563,
    "prof_backtrans_chrf": 66.7702647515938,
    "prof_backtrans_bertscore": 0.9260811805725098,
    "prof_backtrans_labse": 0.9625213742256165,
    "prof_backtrans_xlm_roberta": 0.9642908573150635,
    "llm_vs_prof_backtrans_bleu": 43.04202404826114,
    "llm_vs_prof_backtrans_chrf": 68.41672230595805,
    "llm_vs_prof_backtrans_bertscore": 0.9315571784973145,
    "llm_vs_prof_backtrans_labse": 0.9602805376052856
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 32.10551963021374,
    "same_lang_chrf": 51.44971193092061,
    "same_lang_bertscore": 0.8850847482681274,
    "same_lang_comet": 0.9366600513458252,
    "cross_lang_xlm_roberta": 0.9888445138931274,
    "cross_lang_labse": 0.961117684841156,
    "cross_lang_mbert": 0.9830037355422974,
    "cross_lang_comet_qe": 0.49549663066864014,
    "backtrans_bleu": 44.503829818163375,
    "backtrans_chrf": 71.19114490927086,
    "backtrans_bertscore": 0.9459952116012573,
    "prof_backtrans_bleu": 45.69952481156186,
    "prof_backtrans_chrf": 72.59239959437456,
    "prof_backtrans_bertscore": 0.9589974284172058,
    "prof_backtrans_labse": 0.977860689163208,
    "prof_backtrans_xlm_roberta": 0.9177259206771851,
    "llm_vs_prof_backtrans_bleu": 52.38112448624976,
    "llm_vs_prof_backtrans_chrf": 75.76103996363392,
    "llm_vs_prof_backtrans_bertscore": 0.9506470561027527,
    "llm_vs_prof_backtrans_labse": 0.9706372022628784
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 43.59670235927849,
    "same_lang_chrf": 69.80650871121796,
    "same_lang_bertscore": 0.865382194519043,
    "same_lang_comet": 0.818537175655365,
    "cross_lang_xlm_roberta": 0.9694533944129944,
    "cross_lang_labse": 0.9845845103263855,
    "cross_lang_mbert": 0.814866304397583,
    "cross_lang_comet_qe": 0.352745920419693,
    "backtrans_bleu": 66.87391957451757,
    "backtrans_chrf": 83.52791071438506,
    "backtrans_bertscore": 0.9615458846092224,
    "prof_backtrans_bleu": 53.39959175628038,
    "prof_backtrans_chrf": 77.65861252353471,
    "prof_backtrans_bertscore": 0.9600747227668762,
    "prof_backtrans_labse": 0.9811539053916931,
    "prof_backtrans_xlm_roberta": 0.8611701130867004,
    "llm_vs_prof_backtrans_bleu": 63.02555457599647,
    "llm_vs_prof_backtrans_chrf": 79.73060177544505,
    "llm_vs_prof_backtrans_bertscore": 0.9601960778236389,
    "llm_vs_prof_backtrans_labse": 0.9647786617279053
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 66.39739160165048,
    "same_lang_chrf": 82.57206973058435,
    "same_lang_bertscore": 0.9534962177276611,
    "same_lang_comet": 0.9122824668884277,
    "cross_lang_xlm_roberta": 0.9971417188644409,
    "cross_lang_labse": 0.9966768026351929,
    "cross_lang_mbert": 0.9838234782218933,
    "cross_lang_comet_qe": 0.5420438647270203,
    "backtrans_bleu": 79.71042334934069,
    "backtrans_chrf": 90.43997909183979,
    "backtrans_bertscore": 0.9851192831993103,
    "prof_backtrans_bleu": 70.30956701071162,
    "prof_backtrans_chrf": 85.47069961706939,
    "prof_backtrans_bertscore": 0.9552474021911621,
    "prof_backtrans_labse": 0.990326464176178,
    "prof_backtrans_xlm_roberta": 0.996638834476471,
    "llm_vs_prof_backtrans_bleu": 80.66141384423143,
    "llm_vs_prof_backtrans_chrf": 88.71789993512314,
    "llm_vs_prof_backtrans_bertscore": 0.9607841968536377,
    "llm_vs_prof_backtrans_labse": 0.9913379549980164
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 16.198684960896543,
    "same_lang_chrf": 46.89967511943014,
    "same_lang_bertscore": 0.8804990649223328,
    "same_lang_comet": 0.9125124216079712,
    "cross_lang_xlm_roberta": 0.9787406325340271,
    "cross_lang_labse": 0.984539806842804,
    "cross_lang_mbert": 0.9846974015235901,
    "cross_lang_comet_qe": 0.5288845300674438,
    "backtrans_bleu": 61.663884043672105,
    "backtrans_chrf": 82.3229723655846,
    "backtrans_bertscore": 0.9666759967803955,
    "prof_backtrans_bleu": 65.30951384142121,
    "prof_backtrans_chrf": 82.72289858736585,
    "prof_backtrans_bertscore": 0.9733356833457947,
    "prof_backtrans_labse": 0.9921552538871765,
    "prof_backtrans_xlm_roberta": 0.9787716269493103,
    "llm_vs_prof_backtrans_bleu": 67.37438524804597,
    "llm_vs_prof_backtrans_chrf": 85.21393608804783,
    "llm_vs_prof_backtrans_bertscore": 0.9594834446907043,
    "llm_vs_prof_backtrans_labse": 0.9816285371780396
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 53.95436017022619,
    "same_lang_chrf": 69.66728147296112,
    "same_lang_bertscore": 0.8957483172416687,
    "same_lang_comet": 0.9175312519073486,
    "cross_lang_xlm_roberta": 0.990244448184967,
    "cross_lang_labse": 0.9888752698898315,
    "cross_lang_mbert": 0.9831625819206238,
    "cross_lang_comet_qe": 0.4983949363231659,
    "backtrans_bleu": 70.6415633050001,
    "backtrans_chrf": 84.85168343922554,
    "backtrans_bertscore": 0.9754218459129333,
    "prof_backtrans_bleu": 64.2326572440146,
    "prof_backtrans_chrf": 82.28632985275159,
    "prof_backtrans_bertscore": 0.9673676490783691,
    "prof_backtrans_labse": 0.9804577827453613,
    "prof_backtrans_xlm_roberta": 0.942301332950592,
    "llm_vs_prof_backtrans_bleu": 69.55884279884381,
    "llm_vs_prof_backtrans_chrf": 82.79897335587818,
    "llm_vs_prof_backtrans_bertscore": 0.9735150337219238,
    "llm_vs_prof_backtrans_labse": 0.9796178340911865
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 49.57200295547885,
    "same_lang_chrf": 78.12216951464215,
    "same_lang_bertscore": 0.8918483853340149,
    "same_lang_comet": 0.9097141027450562,
    "cross_lang_xlm_roberta": 0.9959430694580078,
    "cross_lang_labse": 0.9957321286201477,
    "cross_lang_mbert": 0.9903571009635925,
    "cross_lang_comet_qe": 0.5276214480400085,
    "backtrans_bleu": 74.9154409129776,
    "backtrans_chrf": 87.93273553934614,
    "backtrans_bertscore": 0.9759666919708252,
    "prof_backtrans_bleu": 75.76727796622362,
    "prof_backtrans_chrf": 87.9184935198164,
    "prof_backtrans_bertscore": 0.9704646468162537,
    "prof_backtrans_labse": 0.9812469482421875,
    "prof_backtrans_xlm_roberta": 0.9566237926483154,
    "llm_vs_prof_backtrans_bleu": 78.38219699867784,
    "llm_vs_prof_backtrans_chrf": 89.08816792433657,
    "llm_vs_prof_backtrans_bertscore": 0.9743679165840149,
    "llm_vs_prof_backtrans_labse": 0.9807885885238647
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 41.672361190436774,
    "same_lang_chrf": 69.8486847305548,
    "same_lang_bertscore": 0.8903757333755493,
    "same_lang_comet": 0.9315359592437744,
    "cross_lang_xlm_roberta": 0.9960096478462219,
    "cross_lang_labse": 0.9793424606323242,
    "cross_lang_mbert": 0.9838643074035645,
    "cross_lang_comet_qe": 0.5212030410766602,
    "backtrans_bleu": 60.76209228532578,
    "backtrans_chrf": 80.44091937146855,
    "backtrans_bertscore": 0.9680835604667664,
    "prof_backtrans_bleu": 62.790607079894684,
    "prof_backtrans_chrf": 82.68332720068958,
    "prof_backtrans_bertscore": 0.9364621043205261,
    "prof_backtrans_labse": 0.9693825244903564,
    "prof_backtrans_xlm_roberta": 0.9768925309181213,
    "llm_vs_prof_backtrans_bleu": 64.71883134342212,
    "llm_vs_prof_backtrans_chrf": 81.39291594894102,
    "llm_vs_prof_backtrans_bertscore": 0.945249080657959,
    "llm_vs_prof_backtrans_labse": 0.9670760631561279
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 24.022705870057663,
    "same_lang_chrf": 46.01820917583549,
    "same_lang_bertscore": 0.8745077252388,
    "same_lang_comet": 0.9270068407058716,
    "cross_lang_xlm_roberta": 0.983309805393219,
    "cross_lang_labse": 0.9802258014678955,
    "cross_lang_mbert": 0.9911623001098633,
    "cross_lang_comet_qe": 0.5420156717300415,
    "backtrans_bleu": 59.63549750858176,
    "backtrans_chrf": 81.09184182311418,
    "backtrans_bertscore": 0.9695776700973511,
    "prof_backtrans_bleu": 65.26148519859093,
    "prof_backtrans_chrf": 83.6431852238862,
    "prof_backtrans_bertscore": 0.9449551105499268,
    "prof_backtrans_labse": 0.9779385328292847,
    "prof_backtrans_xlm_roberta": 0.8954820036888123,
    "llm_vs_prof_backtrans_bleu": 66.43882707553378,
    "llm_vs_prof_backtrans_chrf": 82.80412363369652,
    "llm_vs_prof_backtrans_bertscore": 0.9400891661643982,
    "llm_vs_prof_backtrans_labse": 0.9682766795158386
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 52.25040831988085,
    "same_lang_chrf": 74.99447476548244,
    "same_lang_bertscore": 0.8927172422409058,
    "same_lang_comet": 0.8352819681167603,
    "cross_lang_xlm_roberta": 0.9959723949432373,
    "cross_lang_labse": 0.9890047311782837,
    "cross_lang_mbert": 0.9941272139549255,
    "cross_lang_comet_qe": 0.49778664112091064,
    "backtrans_bleu": 71.46669786399102,
    "backtrans_chrf": 86.42915863036303,
    "backtrans_bertscore": 0.9778793454170227,
    "prof_backtrans_bleu": 74.12343596438129,
    "prof_backtrans_chrf": 87.02940928533923,
    "prof_backtrans_bertscore": 0.9740288853645325,
    "prof_backtrans_labse": 0.9812523126602173,
    "prof_backtrans_xlm_roberta": 0.9013186693191528,
    "llm_vs_prof_backtrans_bleu": 78.0836714462778,
    "llm_vs_prof_backtrans_chrf": 87.97503373445696,
    "llm_vs_prof_backtrans_bertscore": 0.9739405512809753,
    "llm_vs_prof_backtrans_labse": 0.9811005592346191
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 66.02440115880904,
    "same_lang_chrf": 83.17749065763911,
    "same_lang_bertscore": 0.9484446048736572,
    "same_lang_comet": 0.910020112991333,
    "cross_lang_xlm_roberta": 0.9980759620666504,
    "cross_lang_labse": 0.9979296326637268,
    "cross_lang_mbert": 0.9864241480827332,
    "cross_lang_comet_qe": 0.5525955557823181,
    "backtrans_bleu": 84.31078326385047,
    "backtrans_chrf": 91.9287101007684,
    "backtrans_bertscore": 0.9898216724395752,
    "prof_backtrans_bleu": 67.7842657674554,
    "prof_backtrans_chrf": 83.60416427413924,
    "prof_backtrans_bertscore": 0.9733309149742126,
    "prof_backtrans_labse": 0.9920901656150818,
    "prof_backtrans_xlm_roberta": 0.9958294034004211,
    "llm_vs_prof_backtrans_bleu": 77.87846422047288,
    "llm_vs_prof_backtrans_chrf": 87.29881302613661,
    "llm_vs_prof_backtrans_bertscore": 0.9744119048118591,
    "llm_vs_prof_backtrans_labse": 0.9889314770698547
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 22.015276943137234,
    "same_lang_chrf": 60.04141978746576,
    "same_lang_bertscore": 0.9359287023544312,
    "same_lang_comet": 0.9329662322998047,
    "cross_lang_xlm_roberta": 0.9807905554771423,
    "cross_lang_labse": 0.9898810386657715,
    "cross_lang_mbert": 0.9309536814689636,
    "cross_lang_comet_qe": 0.5544813275337219,
    "backtrans_bleu": 67.46743680907258,
    "backtrans_chrf": 82.97680013988243,
    "backtrans_bertscore": 0.9800937175750732,
    "prof_backtrans_bleu": 57.230639704320254,
    "prof_backtrans_chrf": 78.70850091550155,
    "prof_backtrans_bertscore": 0.9723616242408752,
    "prof_backtrans_labse": 0.9944999814033508,
    "prof_backtrans_xlm_roberta": 0.9792445302009583,
    "llm_vs_prof_backtrans_bleu": 68.33387325402369,
    "llm_vs_prof_backtrans_chrf": 83.89340254106662,
    "llm_vs_prof_backtrans_bertscore": 0.9740419387817383,
    "llm_vs_prof_backtrans_labse": 0.9881613254547119
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 54.60235792024531,
    "same_lang_chrf": 69.07924976440697,
    "same_lang_bertscore": 0.8989682197570801,
    "same_lang_comet": 0.9216709733009338,
    "cross_lang_xlm_roberta": 0.9905205368995667,
    "cross_lang_labse": 0.9952856302261353,
    "cross_lang_mbert": 0.9865365028381348,
    "cross_lang_comet_qe": 0.5359621047973633,
    "backtrans_bleu": 67.41816936863675,
    "backtrans_chrf": 82.1066463605509,
    "backtrans_bertscore": 0.9826838970184326,
    "prof_backtrans_bleu": 62.05894686027628,
    "prof_backtrans_chrf": 80.81824827016058,
    "prof_backtrans_bertscore": 0.9641619920730591,
    "prof_backtrans_labse": 0.9783961176872253,
    "prof_backtrans_xlm_roberta": 0.9387918710708618,
    "llm_vs_prof_backtrans_bleu": 65.06581331213465,
    "llm_vs_prof_backtrans_chrf": 79.87224440915264,
    "llm_vs_prof_backtrans_bertscore": 0.9704023003578186,
    "llm_vs_prof_backtrans_labse": 0.9794535636901855
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 51.83529938734295,
    "same_lang_chrf": 78.71833671169674,
    "same_lang_bertscore": 0.8895888924598694,
    "same_lang_comet": 0.9111487865447998,
    "cross_lang_xlm_roberta": 0.9970642328262329,
    "cross_lang_labse": 0.9976292848587036,
    "cross_lang_mbert": 0.9942450523376465,
    "cross_lang_comet_qe": 0.531743049621582,
    "backtrans_bleu": 80.49236951351234,
    "backtrans_chrf": 89.85180451399698,
    "backtrans_bertscore": 0.9889962077140808,
    "prof_backtrans_bleu": 71.48981958088261,
    "prof_backtrans_chrf": 85.215883854296,
    "prof_backtrans_bertscore": 0.9741807579994202,
    "prof_backtrans_labse": 0.988656759262085,
    "prof_backtrans_xlm_roberta": 0.9543761014938354,
    "llm_vs_prof_backtrans_bleu": 77.38037468501746,
    "llm_vs_prof_backtrans_chrf": 87.52191065003835,
    "llm_vs_prof_backtrans_bertscore": 0.972891628742218,
    "llm_vs_prof_backtrans_labse": 0.9870669841766357
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 41.546634248410136,
    "same_lang_chrf": 71.44390720286935,
    "same_lang_bertscore": 0.8809961676597595,
    "same_lang_comet": 0.9355906248092651,
    "cross_lang_xlm_roberta": 0.9954059720039368,
    "cross_lang_labse": 0.997108519077301,
    "cross_lang_mbert": 0.9958300590515137,
    "cross_lang_comet_qe": 0.5422625541687012,
    "backtrans_bleu": 68.44720280111385,
    "backtrans_chrf": 83.66066705892065,
    "backtrans_bertscore": 0.9831938147544861,
    "prof_backtrans_bleu": 52.31804308734492,
    "prof_backtrans_chrf": 77.32814823284039,
    "prof_backtrans_bertscore": 0.9469547271728516,
    "prof_backtrans_labse": 0.9559840559959412,
    "prof_backtrans_xlm_roberta": 0.9792478680610657,
    "llm_vs_prof_backtrans_bleu": 59.94181515048946,
    "llm_vs_prof_backtrans_chrf": 78.48850189385446,
    "llm_vs_prof_backtrans_bertscore": 0.9508459568023682,
    "llm_vs_prof_backtrans_labse": 0.949927568435669
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 30.266413325695734,
    "same_lang_chrf": 50.28410691218308,
    "same_lang_bertscore": 0.9162033796310425,
    "same_lang_comet": 0.9334921836853027,
    "cross_lang_xlm_roberta": 0.9959319829940796,
    "cross_lang_labse": 0.9923848509788513,
    "cross_lang_mbert": 0.9965842366218567,
    "cross_lang_comet_qe": 0.5426093935966492,
    "backtrans_bleu": 65.6468652475241,
    "backtrans_chrf": 82.90795894995252,
    "backtrans_bertscore": 0.9770099520683289,
    "prof_backtrans_bleu": 59.04734074138372,
    "prof_backtrans_chrf": 79.77932813834983,
    "prof_backtrans_bertscore": 0.9705995321273804,
    "prof_backtrans_labse": 0.9860461950302124,
    "prof_backtrans_xlm_roberta": 0.8990958333015442,
    "llm_vs_prof_backtrans_bleu": 65.35462925346525,
    "llm_vs_prof_backtrans_chrf": 81.8582225985959,
    "llm_vs_prof_backtrans_bertscore": 0.9647756218910217,
    "llm_vs_prof_backtrans_labse": 0.9859028458595276
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 51.82731936481788,
    "same_lang_chrf": 75.1154390634169,
    "same_lang_bertscore": 0.9022717475891113,
    "same_lang_comet": 0.8100805282592773,
    "cross_lang_xlm_roberta": 0.9949547052383423,
    "cross_lang_labse": 0.9982785582542419,
    "cross_lang_mbert": 0.9960745573043823,
    "cross_lang_comet_qe": 0.5181720852851868,
    "backtrans_bleu": 78.77944827464766,
    "backtrans_chrf": 89.1221962868401,
    "backtrans_bertscore": 0.9886302351951599,
    "prof_backtrans_bleu": 69.6831318409897,
    "prof_backtrans_chrf": 84.6634643144203,
    "prof_backtrans_bertscore": 0.9711770415306091,
    "prof_backtrans_labse": 0.980985701084137,
    "prof_backtrans_xlm_roberta": 0.904932975769043,
    "llm_vs_prof_backtrans_bleu": 77.05072471908402,
    "llm_vs_prof_backtrans_chrf": 86.42930122982776,
    "llm_vs_prof_backtrans_bertscore": 0.9718124866485596,
    "llm_vs_prof_backtrans_labse": 0.9782866835594177
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 70.35187711153951,
    "same_lang_chrf": 85.59186269015522,
    "same_lang_bertscore": 0.9543037414550781,
    "same_lang_comet": 0.919452965259552,
    "cross_lang_xlm_roberta": 0.9899244904518127,
    "cross_lang_labse": 0.9913321733474731,
    "cross_lang_mbert": 0.9826971888542175,
    "cross_lang_comet_qe": 0.5167149901390076,
    "backtrans_bleu": 77.21688487065529,
    "backtrans_chrf": 88.84508788416204,
    "backtrans_bertscore": 0.9798992276191711,
    "prof_backtrans_bleu": 72.23072566561395,
    "prof_backtrans_chrf": 86.28012932291253,
    "prof_backtrans_bertscore": 0.9712164998054504,
    "prof_backtrans_labse": 0.9911280274391174,
    "prof_backtrans_xlm_roberta": 0.9953570365905762,
    "llm_vs_prof_backtrans_bleu": 78.11931708960455,
    "llm_vs_prof_backtrans_chrf": 88.21444979671426,
    "llm_vs_prof_backtrans_bertscore": 0.9750667214393616,
    "llm_vs_prof_backtrans_labse": 0.9891917705535889
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 20.72314029466837,
    "same_lang_chrf": 60.580605624610016,
    "same_lang_bertscore": 0.9294812083244324,
    "same_lang_comet": 0.9341569542884827,
    "cross_lang_xlm_roberta": 0.9894245862960815,
    "cross_lang_labse": 0.9878272414207458,
    "cross_lang_mbert": 0.9866142272949219,
    "cross_lang_comet_qe": 0.520931601524353,
    "backtrans_bleu": 59.370380586351764,
    "backtrans_chrf": 79.79168397899974,
    "backtrans_bertscore": 0.9689970016479492,
    "prof_backtrans_bleu": 48.85943915381206,
    "prof_backtrans_chrf": 74.87040155322741,
    "prof_backtrans_bertscore": 0.9675615429878235,
    "prof_backtrans_labse": 0.9941495656967163,
    "prof_backtrans_xlm_roberta": 0.9760816097259521,
    "llm_vs_prof_backtrans_bleu": 64.08187181897904,
    "llm_vs_prof_backtrans_chrf": 81.91223216582627,
    "llm_vs_prof_backtrans_bertscore": 0.9736800193786621,
    "llm_vs_prof_backtrans_labse": 0.984466552734375
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 62.2396063280716,
    "same_lang_chrf": 75.12368829732988,
    "same_lang_bertscore": 0.916508674621582,
    "same_lang_comet": 0.9316296577453613,
    "cross_lang_xlm_roberta": 0.9892274141311646,
    "cross_lang_labse": 0.9937669634819031,
    "cross_lang_mbert": 0.9879134893417358,
    "cross_lang_comet_qe": 0.5107846856117249,
    "backtrans_bleu": 59.40769161362701,
    "backtrans_chrf": 78.60648825704492,
    "backtrans_bertscore": 0.9714674353599548,
    "prof_backtrans_bleu": 59.47894113289668,
    "prof_backtrans_chrf": 79.94578001424988,
    "prof_backtrans_bertscore": 0.9638152718544006,
    "prof_backtrans_labse": 0.9803981184959412,
    "prof_backtrans_xlm_roberta": 0.938230574131012,
    "llm_vs_prof_backtrans_bleu": 68.72133653841443,
    "llm_vs_prof_backtrans_chrf": 83.54932974785945,
    "llm_vs_prof_backtrans_bertscore": 0.9738176465034485,
    "llm_vs_prof_backtrans_labse": 0.9850555658340454
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 50.43346018926108,
    "same_lang_chrf": 77.03988978114786,
    "same_lang_bertscore": 0.8810967803001404,
    "same_lang_comet": 0.905964732170105,
    "cross_lang_xlm_roberta": 0.9962871670722961,
    "cross_lang_labse": 0.9966512322425842,
    "cross_lang_mbert": 0.9901974201202393,
    "cross_lang_comet_qe": 0.49508994817733765,
    "backtrans_bleu": 74.31513257302535,
    "backtrans_chrf": 86.96864650935406,
    "backtrans_bertscore": 0.978973925113678,
    "prof_backtrans_bleu": 69.31820770658555,
    "prof_backtrans_chrf": 84.97449181515023,
    "prof_backtrans_bertscore": 0.9663431644439697,
    "prof_backtrans_labse": 0.982660710811615,
    "prof_backtrans_xlm_roberta": 0.9507839679718018,
    "llm_vs_prof_backtrans_bleu": 73.89695528627225,
    "llm_vs_prof_backtrans_chrf": 85.8573458075044,
    "llm_vs_prof_backtrans_bertscore": 0.9681176543235779,
    "llm_vs_prof_backtrans_labse": 0.981067955493927
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 39.30316015457648,
    "same_lang_chrf": 69.79081309558288,
    "same_lang_bertscore": 0.8818878531455994,
    "same_lang_comet": 0.933835506439209,
    "cross_lang_xlm_roberta": 0.9918980598449707,
    "cross_lang_labse": 0.992513120174408,
    "cross_lang_mbert": 0.988675057888031,
    "cross_lang_comet_qe": 0.5208491683006287,
    "backtrans_bleu": 48.330572753285075,
    "backtrans_chrf": 73.21606650988487,
    "backtrans_bertscore": 0.9637544751167297,
    "prof_backtrans_bleu": 48.16751345202945,
    "prof_backtrans_chrf": 75.18114536707374,
    "prof_backtrans_bertscore": 0.9378076195716858,
    "prof_backtrans_labse": 0.9564642906188965,
    "prof_backtrans_xlm_roberta": 0.975968062877655,
    "llm_vs_prof_backtrans_bleu": 51.92181460187374,
    "llm_vs_prof_backtrans_chrf": 75.70147314282492,
    "llm_vs_prof_backtrans_bertscore": 0.9398332238197327,
    "llm_vs_prof_backtrans_labse": 0.9447674751281738
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 32.6302988618096,
    "same_lang_chrf": 53.02886149989606,
    "same_lang_bertscore": 0.9086258411407471,
    "same_lang_comet": 0.938032865524292,
    "cross_lang_xlm_roberta": 0.9848198890686035,
    "cross_lang_labse": 0.9862111806869507,
    "cross_lang_mbert": 0.9826788902282715,
    "cross_lang_comet_qe": 0.5258309245109558,
    "backtrans_bleu": 55.11991600060591,
    "backtrans_chrf": 77.2405520622317,
    "backtrans_bertscore": 0.966170072555542,
    "prof_backtrans_bleu": 43.36474302286585,
    "prof_backtrans_chrf": 71.60692091546298,
    "prof_backtrans_bertscore": 0.9584637880325317,
    "prof_backtrans_labse": 0.984237015247345,
    "prof_backtrans_xlm_roberta": 0.9018998146057129,
    "llm_vs_prof_backtrans_bleu": 50.35949074669926,
    "llm_vs_prof_backtrans_chrf": 73.8427272581879,
    "llm_vs_prof_backtrans_bertscore": 0.958720862865448,
    "llm_vs_prof_backtrans_labse": 0.9819034934043884
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 59.11903673920168,
    "same_lang_chrf": 78.3870096927986,
    "same_lang_bertscore": 0.9036808013916016,
    "same_lang_comet": 0.8333070278167725,
    "cross_lang_xlm_roberta": 0.9833372235298157,
    "cross_lang_labse": 0.9915304183959961,
    "cross_lang_mbert": 0.9887259602546692,
    "cross_lang_comet_qe": 0.4945841133594513,
    "backtrans_bleu": 64.30326946512872,
    "backtrans_chrf": 83.53780599185757,
    "backtrans_bertscore": 0.9696069359779358,
    "prof_backtrans_bleu": 63.797812286086554,
    "prof_backtrans_chrf": 81.87042005248634,
    "prof_backtrans_bertscore": 0.9622495770454407,
    "prof_backtrans_labse": 0.9763498902320862,
    "prof_backtrans_xlm_roberta": 0.8935332894325256,
    "llm_vs_prof_backtrans_bleu": 67.08744484347413,
    "llm_vs_prof_backtrans_chrf": 82.18050259381215,
    "llm_vs_prof_backtrans_bertscore": 0.9620959162712097,
    "llm_vs_prof_backtrans_labse": 0.9754154682159424
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 65.10382574203484,
    "same_lang_chrf": 82.57453951961743,
    "same_lang_bertscore": 0.9418767094612122,
    "same_lang_comet": 0.9123210906982422,
    "cross_lang_xlm_roberta": 0.9941912293434143,
    "cross_lang_labse": 0.9927091598510742,
    "cross_lang_mbert": 0.98686283826828,
    "cross_lang_comet_qe": 0.4934525191783905,
    "backtrans_bleu": 67.10018517494628,
    "backtrans_chrf": 83.41532890676197,
    "backtrans_bertscore": 0.9725567698478699,
    "prof_backtrans_bleu": 50.24336495938689,
    "prof_backtrans_chrf": 75.94684108681976,
    "prof_backtrans_bertscore": 0.9549411535263062,
    "prof_backtrans_labse": 0.9812403321266174,
    "prof_backtrans_xlm_roberta": 0.9925507307052612,
    "llm_vs_prof_backtrans_bleu": 65.2306023025016,
    "llm_vs_prof_backtrans_chrf": 79.81663016948251,
    "llm_vs_prof_backtrans_bertscore": 0.9668203592300415,
    "llm_vs_prof_backtrans_labse": 0.9776841998100281
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.799104725694416,
    "same_lang_chrf": 56.85543447497574,
    "same_lang_bertscore": 0.9099677205085754,
    "same_lang_comet": 0.9323630332946777,
    "cross_lang_xlm_roberta": 0.9889371991157532,
    "cross_lang_labse": 0.9885715246200562,
    "cross_lang_mbert": 0.9904259443283081,
    "cross_lang_comet_qe": 0.5260509252548218,
    "backtrans_bleu": 50.06824515182904,
    "backtrans_chrf": 73.42310046458685,
    "backtrans_bertscore": 0.9636877179145813,
    "prof_backtrans_bleu": 50.44985332371937,
    "prof_backtrans_chrf": 74.4085534520025,
    "prof_backtrans_bertscore": 0.9643913507461548,
    "prof_backtrans_labse": 0.991166353225708,
    "prof_backtrans_xlm_roberta": 0.975673496723175,
    "llm_vs_prof_backtrans_bleu": 64.02914976275429,
    "llm_vs_prof_backtrans_chrf": 81.68336576503764,
    "llm_vs_prof_backtrans_bertscore": 0.9716105461120605,
    "llm_vs_prof_backtrans_labse": 0.9915246367454529
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 59.62000625465892,
    "same_lang_chrf": 72.91275490217966,
    "same_lang_bertscore": 0.8969875574111938,
    "same_lang_comet": 0.9311699867248535,
    "cross_lang_xlm_roberta": 0.9728243350982666,
    "cross_lang_labse": 0.9914218187332153,
    "cross_lang_mbert": 0.9895098209381104,
    "cross_lang_comet_qe": 0.5040282011032104,
    "backtrans_bleu": 60.22327595115433,
    "backtrans_chrf": 78.94915214194485,
    "backtrans_bertscore": 0.9733932018280029,
    "prof_backtrans_bleu": 53.347213180495686,
    "prof_backtrans_chrf": 76.43335536855592,
    "prof_backtrans_bertscore": 0.9587936401367188,
    "prof_backtrans_labse": 0.9784696102142334,
    "prof_backtrans_xlm_roberta": 0.9358617663383484,
    "llm_vs_prof_backtrans_bleu": 63.64219028423783,
    "llm_vs_prof_backtrans_chrf": 80.07168889589362,
    "llm_vs_prof_backtrans_bertscore": 0.9650537371635437,
    "llm_vs_prof_backtrans_labse": 0.9775023460388184
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 48.419865245412744,
    "same_lang_chrf": 76.66603020140683,
    "same_lang_bertscore": 0.8988533020019531,
    "same_lang_comet": 0.9097095131874084,
    "cross_lang_xlm_roberta": 0.968167781829834,
    "cross_lang_labse": 0.9800114631652832,
    "cross_lang_mbert": 0.9271531701087952,
    "cross_lang_comet_qe": 0.4657765030860901,
    "backtrans_bleu": 61.02305912579491,
    "backtrans_chrf": 77.92303363899413,
    "backtrans_bertscore": 0.9618091583251953,
    "prof_backtrans_bleu": 63.63950712692653,
    "prof_backtrans_chrf": 80.86131370831583,
    "prof_backtrans_bertscore": 0.9618110060691833,
    "prof_backtrans_labse": 0.9811667203903198,
    "prof_backtrans_xlm_roberta": 0.9526662230491638,
    "llm_vs_prof_backtrans_bleu": 65.08706381657852,
    "llm_vs_prof_backtrans_chrf": 80.72684346960007,
    "llm_vs_prof_backtrans_bertscore": 0.9681800603866577,
    "llm_vs_prof_backtrans_labse": 0.9812573790550232
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 43.01909377142615,
    "same_lang_chrf": 71.4828452926387,
    "same_lang_bertscore": 0.8745564818382263,
    "same_lang_comet": 0.9330234527587891,
    "cross_lang_xlm_roberta": 0.9883909225463867,
    "cross_lang_labse": 0.9855075478553772,
    "cross_lang_mbert": 0.9699304699897766,
    "cross_lang_comet_qe": 0.4522494971752167,
    "backtrans_bleu": 47.3386103214444,
    "backtrans_chrf": 73.95653741288413,
    "backtrans_bertscore": 0.95765221118927,
    "prof_backtrans_bleu": 39.92988487379947,
    "prof_backtrans_chrf": 70.4598200923214,
    "prof_backtrans_bertscore": 0.9378656148910522,
    "prof_backtrans_labse": 0.9614769816398621,
    "prof_backtrans_xlm_roberta": 0.9750005006790161,
    "llm_vs_prof_backtrans_bleu": 50.73210974752505,
    "llm_vs_prof_backtrans_chrf": 73.65126121329443,
    "llm_vs_prof_backtrans_bertscore": 0.9421912431716919,
    "llm_vs_prof_backtrans_labse": 0.9689235687255859
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 27.819191058969526,
    "same_lang_chrf": 48.25286297249994,
    "same_lang_bertscore": 0.8960503935813904,
    "same_lang_comet": 0.9354729652404785,
    "cross_lang_xlm_roberta": 0.9877120852470398,
    "cross_lang_labse": 0.9773141145706177,
    "cross_lang_mbert": 0.9694463610649109,
    "cross_lang_comet_qe": 0.5051029920578003,
    "backtrans_bleu": 44.437336172475625,
    "backtrans_chrf": 70.81161434587283,
    "backtrans_bertscore": 0.9495442509651184,
    "prof_backtrans_bleu": 42.60915724598487,
    "prof_backtrans_chrf": 70.07738344042106,
    "prof_backtrans_bertscore": 0.9534159898757935,
    "prof_backtrans_labse": 0.978886067867279,
    "prof_backtrans_xlm_roberta": 0.894245445728302,
    "llm_vs_prof_backtrans_bleu": 51.352972966036695,
    "llm_vs_prof_backtrans_chrf": 74.46847306214342,
    "llm_vs_prof_backtrans_bertscore": 0.9490292072296143,
    "llm_vs_prof_backtrans_labse": 0.9771521091461182
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 48.2275779452814,
    "same_lang_chrf": 72.12500473298368,
    "same_lang_bertscore": 0.8725646138191223,
    "same_lang_comet": 0.8069377541542053,
    "cross_lang_xlm_roberta": 0.9771475195884705,
    "cross_lang_labse": 0.9908491969108582,
    "cross_lang_mbert": 0.9903927445411682,
    "cross_lang_comet_qe": 0.4652308225631714,
    "backtrans_bleu": 68.56335577017155,
    "backtrans_chrf": 83.95250129366003,
    "backtrans_bertscore": 0.9765307903289795,
    "prof_backtrans_bleu": 59.29209656209256,
    "prof_backtrans_chrf": 80.09878596661231,
    "prof_backtrans_bertscore": 0.9662255048751831,
    "prof_backtrans_labse": 0.9830900430679321,
    "prof_backtrans_xlm_roberta": 0.8848505616188049,
    "llm_vs_prof_backtrans_bleu": 66.27185610217715,
    "llm_vs_prof_backtrans_chrf": 81.12668216504976,
    "llm_vs_prof_backtrans_bertscore": 0.9624636173248291,
    "llm_vs_prof_backtrans_labse": 0.9787798523902893
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 67.62168810311248,
    "same_lang_chrf": 83.43714573167344,
    "same_lang_bertscore": 0.9687386155128479,
    "same_lang_comet": 0.9155716896057129,
    "cross_lang_xlm_roberta": 0.9900036454200745,
    "cross_lang_labse": 0.9925747513771057,
    "cross_lang_mbert": 0.9882340431213379,
    "cross_lang_comet_qe": 0.5444614887237549,
    "backtrans_bleu": 78.01795952208603,
    "backtrans_chrf": 89.64423086078794,
    "backtrans_bertscore": 0.9831997752189636,
    "prof_backtrans_bleu": 71.19737269389742,
    "prof_backtrans_chrf": 85.37224949180352,
    "prof_backtrans_bertscore": 0.9745234847068787,
    "prof_backtrans_labse": 0.991888165473938,
    "prof_backtrans_xlm_roberta": 0.9892265200614929,
    "llm_vs_prof_backtrans_bleu": 82.78949979444396,
    "llm_vs_prof_backtrans_chrf": 90.72856499823672,
    "llm_vs_prof_backtrans_bertscore": 0.9745689630508423,
    "llm_vs_prof_backtrans_labse": 0.9928042888641357
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 15.089091095793766,
    "same_lang_chrf": 48.88662624225301,
    "same_lang_bertscore": 0.9109362363815308,
    "same_lang_comet": 0.9133385419845581,
    "cross_lang_xlm_roberta": 0.9891328811645508,
    "cross_lang_labse": 0.9924862384796143,
    "cross_lang_mbert": 0.9878179430961609,
    "cross_lang_comet_qe": 0.5405475497245789,
    "backtrans_bleu": 61.33970090702457,
    "backtrans_chrf": 82.2038453067153,
    "backtrans_bertscore": 0.9728835821151733,
    "prof_backtrans_bleu": 60.33957524172807,
    "prof_backtrans_chrf": 80.3468423131928,
    "prof_backtrans_bertscore": 0.9715355634689331,
    "prof_backtrans_labse": 0.9926956295967102,
    "prof_backtrans_xlm_roberta": 0.991091787815094,
    "llm_vs_prof_backtrans_bleu": 66.50286523981097,
    "llm_vs_prof_backtrans_chrf": 84.36484460394944,
    "llm_vs_prof_backtrans_bertscore": 0.970497727394104,
    "llm_vs_prof_backtrans_labse": 0.993182897567749
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 55.93651567010859,
    "same_lang_chrf": 71.5566647118989,
    "same_lang_bertscore": 0.8979678153991699,
    "same_lang_comet": 0.9136834144592285,
    "cross_lang_xlm_roberta": 0.9913438558578491,
    "cross_lang_labse": 0.9929825067520142,
    "cross_lang_mbert": 0.9821221828460693,
    "cross_lang_comet_qe": 0.5311574935913086,
    "backtrans_bleu": 72.07064497350784,
    "backtrans_chrf": 85.93018750405345,
    "backtrans_bertscore": 0.9770790934562683,
    "prof_backtrans_bleu": 66.62162186126518,
    "prof_backtrans_chrf": 84.31787554087016,
    "prof_backtrans_bertscore": 0.9662150144577026,
    "prof_backtrans_labse": 0.9814958572387695,
    "prof_backtrans_xlm_roberta": 0.9464897513389587,
    "llm_vs_prof_backtrans_bleu": 69.20110395245666,
    "llm_vs_prof_backtrans_chrf": 83.56114881836618,
    "llm_vs_prof_backtrans_bertscore": 0.9712109565734863,
    "llm_vs_prof_backtrans_labse": 0.9851482510566711
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 50.068200730186895,
    "same_lang_chrf": 78.95695191474411,
    "same_lang_bertscore": 0.8972446918487549,
    "same_lang_comet": 0.9120529890060425,
    "cross_lang_xlm_roberta": 0.983307421207428,
    "cross_lang_labse": 0.9934424161911011,
    "cross_lang_mbert": 0.9855341911315918,
    "cross_lang_comet_qe": 0.553217887878418,
    "backtrans_bleu": 75.12663327510782,
    "backtrans_chrf": 87.6241609910245,
    "backtrans_bertscore": 0.9766281843185425,
    "prof_backtrans_bleu": 77.64639690011262,
    "prof_backtrans_chrf": 88.82141608954076,
    "prof_backtrans_bertscore": 0.9748440980911255,
    "prof_backtrans_labse": 0.988552451133728,
    "prof_backtrans_xlm_roberta": 0.9617899060249329,
    "llm_vs_prof_backtrans_bleu": 79.35311853187103,
    "llm_vs_prof_backtrans_chrf": 88.8528603179444,
    "llm_vs_prof_backtrans_bertscore": 0.9775515794754028,
    "llm_vs_prof_backtrans_labse": 0.9895361661911011
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 41.149640122629286,
    "same_lang_chrf": 69.59671385844624,
    "same_lang_bertscore": 0.8850899338722229,
    "same_lang_comet": 0.9215341806411743,
    "cross_lang_xlm_roberta": 0.9881928563117981,
    "cross_lang_labse": 0.9935506582260132,
    "cross_lang_mbert": 0.984228253364563,
    "cross_lang_comet_qe": 0.5153998136520386,
    "backtrans_bleu": 64.54919076164931,
    "backtrans_chrf": 82.77114547968138,
    "backtrans_bertscore": 0.977486252784729,
    "prof_backtrans_bleu": 58.639323466443024,
    "prof_backtrans_chrf": 80.55639818966233,
    "prof_backtrans_bertscore": 0.9454527497291565,
    "prof_backtrans_labse": 0.9691470265388489,
    "prof_backtrans_xlm_roberta": 0.9707408547401428,
    "llm_vs_prof_backtrans_bleu": 62.850037329169005,
    "llm_vs_prof_backtrans_chrf": 79.6817577535547,
    "llm_vs_prof_backtrans_bertscore": 0.949164867401123,
    "llm_vs_prof_backtrans_labse": 0.9732165336608887
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 25.338213212365105,
    "same_lang_chrf": 45.386301990180016,
    "same_lang_bertscore": 0.8753097057342529,
    "same_lang_comet": 0.9219435453414917,
    "cross_lang_xlm_roberta": 0.9765413999557495,
    "cross_lang_labse": 0.9829660654067993,
    "cross_lang_mbert": 0.8571680188179016,
    "cross_lang_comet_qe": 0.5350834131240845,
    "backtrans_bleu": 60.56842115039635,
    "backtrans_chrf": 80.4466461613809,
    "backtrans_bertscore": 0.9626535773277283,
    "prof_backtrans_bleu": 63.86663881022168,
    "prof_backtrans_chrf": 81.87402158634542,
    "prof_backtrans_bertscore": 0.9732226729393005,
    "prof_backtrans_labse": 0.9800789952278137,
    "prof_backtrans_xlm_roberta": 0.9120716452598572,
    "llm_vs_prof_backtrans_bleu": 63.985452484324064,
    "llm_vs_prof_backtrans_chrf": 81.35248541370797,
    "llm_vs_prof_backtrans_bertscore": 0.9606990814208984,
    "llm_vs_prof_backtrans_labse": 0.9835166931152344
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 51.760719172834555,
    "same_lang_chrf": 74.47753649559445,
    "same_lang_bertscore": 0.8990971446037292,
    "same_lang_comet": 0.8313854336738586,
    "cross_lang_xlm_roberta": 0.9722602963447571,
    "cross_lang_labse": 0.9842166304588318,
    "cross_lang_mbert": 0.977751612663269,
    "cross_lang_comet_qe": 0.4926203191280365,
    "backtrans_bleu": 73.2216657246181,
    "backtrans_chrf": 87.17608361673784,
    "backtrans_bertscore": 0.9725437760353088,
    "prof_backtrans_bleu": 74.96165034964486,
    "prof_backtrans_chrf": 87.50466999105446,
    "prof_backtrans_bertscore": 0.9529037475585938,
    "prof_backtrans_labse": 0.9860637784004211,
    "prof_backtrans_xlm_roberta": 0.8832957148551941,
    "llm_vs_prof_backtrans_bleu": 76.65372738894945,
    "llm_vs_prof_backtrans_chrf": 87.184129277233,
    "llm_vs_prof_backtrans_bertscore": 0.9538149833679199,
    "llm_vs_prof_backtrans_labse": 0.9810470342636108
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 66.34646265606997,
    "same_lang_chrf": 83.5108465750609,
    "same_lang_bertscore": 0.9503929615020752,
    "same_lang_comet": 0.912867546081543,
    "cross_lang_xlm_roberta": 0.9923856854438782,
    "cross_lang_labse": 0.9864092469215393,
    "cross_lang_mbert": 0.9890817403793335,
    "cross_lang_comet_qe": 0.46347373723983765,
    "backtrans_bleu": 81.99844725599687,
    "backtrans_chrf": 90.71454799431736,
    "backtrans_bertscore": 0.9858459830284119,
    "prof_backtrans_bleu": 65.557323262722,
    "prof_backtrans_chrf": 81.76079358086298,
    "prof_backtrans_bertscore": 0.9694844484329224,
    "prof_backtrans_labse": 0.9898884892463684,
    "prof_backtrans_xlm_roberta": 0.9610609412193298,
    "llm_vs_prof_backtrans_bleu": 75.56244163977524,
    "llm_vs_prof_backtrans_chrf": 85.29143076649474,
    "llm_vs_prof_backtrans_bertscore": 0.9731043577194214,
    "llm_vs_prof_backtrans_labse": 0.9802341461181641
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 14.692244994926101,
    "same_lang_chrf": 60.59888949222349,
    "same_lang_bertscore": 0.927764356136322,
    "same_lang_comet": 0.9299663305282593,
    "cross_lang_xlm_roberta": 0.9867744445800781,
    "cross_lang_labse": 0.9887861013412476,
    "cross_lang_mbert": 0.9592933654785156,
    "cross_lang_comet_qe": 0.5393187999725342,
    "backtrans_bleu": 61.83856629982569,
    "backtrans_chrf": 80.6157789084386,
    "backtrans_bertscore": 0.9733599424362183,
    "prof_backtrans_bleu": 55.7748659546111,
    "prof_backtrans_chrf": 77.55182776342153,
    "prof_backtrans_bertscore": 0.9696252346038818,
    "prof_backtrans_labse": 0.9884291887283325,
    "prof_backtrans_xlm_roberta": 0.9710177779197693,
    "llm_vs_prof_backtrans_bleu": 67.22691371581865,
    "llm_vs_prof_backtrans_chrf": 83.5025629120643,
    "llm_vs_prof_backtrans_bertscore": 0.9699599146842957,
    "llm_vs_prof_backtrans_labse": 0.9848385453224182
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 53.010124604467954,
    "same_lang_chrf": 68.80518804983929,
    "same_lang_bertscore": 0.896206259727478,
    "same_lang_comet": 0.9089415073394775,
    "cross_lang_xlm_roberta": 0.9931961894035339,
    "cross_lang_labse": 0.9967308640480042,
    "cross_lang_mbert": 0.9832963347434998,
    "cross_lang_comet_qe": 0.5321321487426758,
    "backtrans_bleu": 67.42617103770877,
    "backtrans_chrf": 82.7561460108138,
    "backtrans_bertscore": 0.9799362421035767,
    "prof_backtrans_bleu": 57.04029658933863,
    "prof_backtrans_chrf": 78.21251409707423,
    "prof_backtrans_bertscore": 0.9574671983718872,
    "prof_backtrans_labse": 0.978435218334198,
    "prof_backtrans_xlm_roberta": 0.9462261199951172,
    "llm_vs_prof_backtrans_bleu": 62.60315184513316,
    "llm_vs_prof_backtrans_chrf": 78.63059415349328,
    "llm_vs_prof_backtrans_bertscore": 0.9576184153556824,
    "llm_vs_prof_backtrans_labse": 0.9798690676689148
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 53.36117904064416,
    "same_lang_chrf": 77.45202122054688,
    "same_lang_bertscore": 0.8798521161079407,
    "same_lang_comet": 0.9061847925186157,
    "cross_lang_xlm_roberta": 0.9869828224182129,
    "cross_lang_labse": 0.9939244985580444,
    "cross_lang_mbert": 0.9962623715400696,
    "cross_lang_comet_qe": 0.5326533317565918,
    "backtrans_bleu": 81.03197758130213,
    "backtrans_chrf": 90.16488291395845,
    "backtrans_bertscore": 0.9859325885772705,
    "prof_backtrans_bleu": 68.54215125452825,
    "prof_backtrans_chrf": 83.75214219713656,
    "prof_backtrans_bertscore": 0.9645191431045532,
    "prof_backtrans_labse": 0.9808983206748962,
    "prof_backtrans_xlm_roberta": 0.9165358543395996,
    "llm_vs_prof_backtrans_bleu": 75.58840231351711,
    "llm_vs_prof_backtrans_chrf": 85.47400228044395,
    "llm_vs_prof_backtrans_bertscore": 0.9597193598747253,
    "llm_vs_prof_backtrans_labse": 0.9745635390281677
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 42.087293623367934,
    "same_lang_chrf": 72.27280208018617,
    "same_lang_bertscore": 0.8876876831054688,
    "same_lang_comet": 0.9271663427352905,
    "cross_lang_xlm_roberta": 0.9938017725944519,
    "cross_lang_labse": 0.9938360452651978,
    "cross_lang_mbert": 0.9937355518341064,
    "cross_lang_comet_qe": 0.5432031154632568,
    "backtrans_bleu": 61.15453341955965,
    "backtrans_chrf": 79.88413214078535,
    "backtrans_bertscore": 0.9770807027816772,
    "prof_backtrans_bleu": 58.00470545217925,
    "prof_backtrans_chrf": 79.53362123991687,
    "prof_backtrans_bertscore": 0.9611108899116516,
    "prof_backtrans_labse": 0.9741876721382141,
    "prof_backtrans_xlm_roberta": 0.9755176305770874,
    "llm_vs_prof_backtrans_bleu": 61.91216699542441,
    "llm_vs_prof_backtrans_chrf": 80.45806716552661,
    "llm_vs_prof_backtrans_bertscore": 0.9587993025779724,
    "llm_vs_prof_backtrans_labse": 0.9733809232711792
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 30.278196355456178,
    "same_lang_chrf": 50.06054761774629,
    "same_lang_bertscore": 0.9050917029380798,
    "same_lang_comet": 0.9359872341156006,
    "cross_lang_xlm_roberta": 0.9934303164482117,
    "cross_lang_labse": 0.9843633770942688,
    "cross_lang_mbert": 0.9928274154663086,
    "cross_lang_comet_qe": 0.5285758376121521,
    "backtrans_bleu": 57.9677490908694,
    "backtrans_chrf": 78.86993183706987,
    "backtrans_bertscore": 0.9714813828468323,
    "prof_backtrans_bleu": 59.182604459626724,
    "prof_backtrans_chrf": 79.50363048253259,
    "prof_backtrans_bertscore": 0.9725567102432251,
    "prof_backtrans_labse": 0.9839789271354675,
    "prof_backtrans_xlm_roberta": 0.9261186122894287,
    "llm_vs_prof_backtrans_bleu": 63.45135331690677,
    "llm_vs_prof_backtrans_chrf": 81.85431904991444,
    "llm_vs_prof_backtrans_bertscore": 0.9664061665534973,
    "llm_vs_prof_backtrans_labse": 0.9867053627967834
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 51.688045629825886,
    "same_lang_chrf": 75.28553050868398,
    "same_lang_bertscore": 0.8891507387161255,
    "same_lang_comet": 0.8231592178344727,
    "cross_lang_xlm_roberta": 0.9668566584587097,
    "cross_lang_labse": 0.9985911250114441,
    "cross_lang_mbert": 0.9770643711090088,
    "cross_lang_comet_qe": 0.5468356609344482,
    "backtrans_bleu": 79.28223376199603,
    "backtrans_chrf": 89.23388592538092,
    "backtrans_bertscore": 0.9903333783149719,
    "prof_backtrans_bleu": 63.84129765798618,
    "prof_backtrans_chrf": 82.09807968859818,
    "prof_backtrans_bertscore": 0.9637583494186401,
    "prof_backtrans_labse": 0.9813138246536255,
    "prof_backtrans_xlm_roberta": 0.8613035082817078,
    "llm_vs_prof_backtrans_bleu": 72.06630189899207,
    "llm_vs_prof_backtrans_chrf": 84.36073941680215,
    "llm_vs_prof_backtrans_bertscore": 0.964521586894989,
    "llm_vs_prof_backtrans_labse": 0.9823486804962158
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 69.83860599399313,
    "same_lang_chrf": 84.68699566129145,
    "same_lang_bertscore": 0.956853985786438,
    "same_lang_comet": 0.9184343814849854,
    "cross_lang_xlm_roberta": 0.9866842031478882,
    "cross_lang_labse": 0.9909882545471191,
    "cross_lang_mbert": 0.9875087141990662,
    "cross_lang_comet_qe": 0.5454167127609253,
    "backtrans_bleu": 75.85931820270841,
    "backtrans_chrf": 88.13421090284159,
    "backtrans_bertscore": 0.9800525903701782,
    "prof_backtrans_bleu": 70.28162771744837,
    "prof_backtrans_chrf": 84.64393454038225,
    "prof_backtrans_bertscore": 0.973052978515625,
    "prof_backtrans_labse": 0.9913141131401062,
    "prof_backtrans_xlm_roberta": 0.9864705204963684,
    "llm_vs_prof_backtrans_bleu": 76.14044004039258,
    "llm_vs_prof_backtrans_chrf": 87.10327756271829,
    "llm_vs_prof_backtrans_bertscore": 0.9738141298294067,
    "llm_vs_prof_backtrans_labse": 0.9912928938865662
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 21.127618621015003,
    "same_lang_chrf": 59.75573728879478,
    "same_lang_bertscore": 0.9265051484107971,
    "same_lang_comet": 0.9298487901687622,
    "cross_lang_xlm_roberta": 0.9934800267219543,
    "cross_lang_labse": 0.9921262860298157,
    "cross_lang_mbert": 0.987625002861023,
    "cross_lang_comet_qe": 0.5397628545761108,
    "backtrans_bleu": 57.25618270301983,
    "backtrans_chrf": 78.75231198242453,
    "backtrans_bertscore": 0.9701047539710999,
    "prof_backtrans_bleu": 46.91354218461185,
    "prof_backtrans_chrf": 73.34548006793547,
    "prof_backtrans_bertscore": 0.9649856686592102,
    "prof_backtrans_labse": 0.9891721606254578,
    "prof_backtrans_xlm_roberta": 0.9861826300621033,
    "llm_vs_prof_backtrans_bleu": 61.206988022442076,
    "llm_vs_prof_backtrans_chrf": 79.88000813467214,
    "llm_vs_prof_backtrans_bertscore": 0.9708642363548279,
    "llm_vs_prof_backtrans_labse": 0.9900603890419006
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 64.8393599146713,
    "same_lang_chrf": 77.31563320384717,
    "same_lang_bertscore": 0.910232424736023,
    "same_lang_comet": 0.9148973226547241,
    "cross_lang_xlm_roberta": 0.9936792254447937,
    "cross_lang_labse": 0.9960129857063293,
    "cross_lang_mbert": 0.9853770136833191,
    "cross_lang_comet_qe": 0.5472144484519958,
    "backtrans_bleu": 68.78817623691172,
    "backtrans_chrf": 83.42713999301039,
    "backtrans_bertscore": 0.9807630777359009,
    "prof_backtrans_bleu": 58.219824214861944,
    "prof_backtrans_chrf": 79.89587049769823,
    "prof_backtrans_bertscore": 0.9595571756362915,
    "prof_backtrans_labse": 0.9811999201774597,
    "prof_backtrans_xlm_roberta": 0.949617862701416,
    "llm_vs_prof_backtrans_bleu": 65.30841889010148,
    "llm_vs_prof_backtrans_chrf": 80.87229453004217,
    "llm_vs_prof_backtrans_bertscore": 0.9586775898933411,
    "llm_vs_prof_backtrans_labse": 0.9818561673164368
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 56.74147996737153,
    "same_lang_chrf": 80.4126424334807,
    "same_lang_bertscore": 0.888515055179596,
    "same_lang_comet": 0.9133393168449402,
    "cross_lang_xlm_roberta": 0.9856738448143005,
    "cross_lang_labse": 0.9974049925804138,
    "cross_lang_mbert": 0.9899224638938904,
    "cross_lang_comet_qe": 0.5029983520507812,
    "backtrans_bleu": 75.21515904098425,
    "backtrans_chrf": 87.31944947000977,
    "backtrans_bertscore": 0.9823086857795715,
    "prof_backtrans_bleu": 69.6814330380443,
    "prof_backtrans_chrf": 85.02332946809375,
    "prof_backtrans_bertscore": 0.9598418474197388,
    "prof_backtrans_labse": 0.9784735441207886,
    "prof_backtrans_xlm_roberta": 0.957619309425354,
    "llm_vs_prof_backtrans_bleu": 73.86959085830647,
    "llm_vs_prof_backtrans_chrf": 85.49043846338607,
    "llm_vs_prof_backtrans_bertscore": 0.959466814994812,
    "llm_vs_prof_backtrans_labse": 0.9761885404586792
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 40.13985045452602,
    "same_lang_chrf": 68.66023284083447,
    "same_lang_bertscore": 0.8884180188179016,
    "same_lang_comet": 0.9248334169387817,
    "cross_lang_xlm_roberta": 0.9930018186569214,
    "cross_lang_labse": 0.983389675617218,
    "cross_lang_mbert": 0.9835846424102783,
    "cross_lang_comet_qe": 0.5677711963653564,
    "backtrans_bleu": 56.92427980669683,
    "backtrans_chrf": 76.94924720476226,
    "backtrans_bertscore": 0.9664452075958252,
    "prof_backtrans_bleu": 47.16992363125279,
    "prof_backtrans_chrf": 74.14286228627653,
    "prof_backtrans_bertscore": 0.9423242211341858,
    "prof_backtrans_labse": 0.9611977934837341,
    "prof_backtrans_xlm_roberta": 0.9743685722351074,
    "llm_vs_prof_backtrans_bleu": 48.024120407287356,
    "llm_vs_prof_backtrans_chrf": 71.32344707383798,
    "llm_vs_prof_backtrans_bertscore": 0.9400720596313477,
    "llm_vs_prof_backtrans_labse": 0.9636320471763611
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 33.153105257596756,
    "same_lang_chrf": 54.30910620221194,
    "same_lang_bertscore": 0.8928318619728088,
    "same_lang_comet": 0.9361602067947388,
    "cross_lang_xlm_roberta": 0.9823217391967773,
    "cross_lang_labse": 0.973275899887085,
    "cross_lang_mbert": 0.9857956171035767,
    "cross_lang_comet_qe": 0.49695584177970886,
    "backtrans_bleu": 43.63854766326302,
    "backtrans_chrf": 70.99870208308991,
    "backtrans_bertscore": 0.9567274451255798,
    "prof_backtrans_bleu": 42.89163176734774,
    "prof_backtrans_chrf": 71.63250181577705,
    "prof_backtrans_bertscore": 0.956420361995697,
    "prof_backtrans_labse": 0.9728966951370239,
    "prof_backtrans_xlm_roberta": 0.9108443260192871,
    "llm_vs_prof_backtrans_bleu": 50.40467838638087,
    "llm_vs_prof_backtrans_chrf": 75.3906575338318,
    "llm_vs_prof_backtrans_bertscore": 0.9578831195831299,
    "llm_vs_prof_backtrans_labse": 0.9733047485351562
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 58.43844952652246,
    "same_lang_chrf": 78.06400117878107,
    "same_lang_bertscore": 0.9203805327415466,
    "same_lang_comet": 0.8557442426681519,
    "cross_lang_xlm_roberta": 0.9676370620727539,
    "cross_lang_labse": 0.989734947681427,
    "cross_lang_mbert": 0.9890254735946655,
    "cross_lang_comet_qe": 0.449714720249176,
    "backtrans_bleu": 65.7308881230696,
    "backtrans_chrf": 84.33591543246794,
    "backtrans_bertscore": 0.9707993268966675,
    "prof_backtrans_bleu": 64.86595825079065,
    "prof_backtrans_chrf": 81.83487956780941,
    "prof_backtrans_bertscore": 0.9683449864387512,
    "prof_backtrans_labse": 0.9851345419883728,
    "prof_backtrans_xlm_roberta": 0.8847630620002747,
    "llm_vs_prof_backtrans_bleu": 66.81491555099954,
    "llm_vs_prof_backtrans_chrf": 82.62935535008144,
    "llm_vs_prof_backtrans_bertscore": 0.9665762782096863,
    "llm_vs_prof_backtrans_labse": 0.983588695526123
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 67.62041931080498,
    "same_lang_chrf": 84.04487180665355,
    "same_lang_bertscore": 0.9594520926475525,
    "same_lang_comet": 0.9079118371009827,
    "cross_lang_xlm_roberta": 0.9600070118904114,
    "cross_lang_labse": 0.9887356162071228,
    "cross_lang_mbert": 0.9657288193702698,
    "cross_lang_comet_qe": 0.5101898908615112,
    "backtrans_bleu": 59.901808349155985,
    "backtrans_chrf": 79.30559139600668,
    "backtrans_bertscore": 0.9643973708152771,
    "prof_backtrans_bleu": 51.596020096966754,
    "prof_backtrans_chrf": 75.75631769540185,
    "prof_backtrans_bertscore": 0.9657192826271057,
    "prof_backtrans_labse": 0.9819082617759705,
    "prof_backtrans_xlm_roberta": 0.9549728631973267,
    "llm_vs_prof_backtrans_bleu": 66.78645627341325,
    "llm_vs_prof_backtrans_chrf": 81.20069948084824,
    "llm_vs_prof_backtrans_bertscore": 0.9768355488777161,
    "llm_vs_prof_backtrans_labse": 0.9916982650756836
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 21.95551856484964,
    "same_lang_chrf": 56.56830094015709,
    "same_lang_bertscore": 0.9267182946205139,
    "same_lang_comet": 0.9250333309173584,
    "cross_lang_xlm_roberta": 0.9610527157783508,
    "cross_lang_labse": 0.990044116973877,
    "cross_lang_mbert": 0.9689652323722839,
    "cross_lang_comet_qe": 0.5110088586807251,
    "backtrans_bleu": 53.77171886396817,
    "backtrans_chrf": 75.82559742513443,
    "backtrans_bertscore": 0.973726749420166,
    "prof_backtrans_bleu": 47.96174720871975,
    "prof_backtrans_chrf": 73.19851006944738,
    "prof_backtrans_bertscore": 0.9626262187957764,
    "prof_backtrans_labse": 0.9892140626907349,
    "prof_backtrans_xlm_roberta": 0.9634101390838623,
    "llm_vs_prof_backtrans_bleu": 62.139087077688195,
    "llm_vs_prof_backtrans_chrf": 80.05008075425584,
    "llm_vs_prof_backtrans_bertscore": 0.9707834124565125,
    "llm_vs_prof_backtrans_labse": 0.9903357028961182
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 53.65111147537672,
    "same_lang_chrf": 69.31834643982806,
    "same_lang_bertscore": 0.8938570618629456,
    "same_lang_comet": 0.907922625541687,
    "cross_lang_xlm_roberta": 0.9626948237419128,
    "cross_lang_labse": 0.9902744889259338,
    "cross_lang_mbert": 0.971825897693634,
    "cross_lang_comet_qe": 0.49073776602745056,
    "backtrans_bleu": 55.756099223847805,
    "backtrans_chrf": 75.828274296201,
    "backtrans_bertscore": 0.9618042707443237,
    "prof_backtrans_bleu": 48.38681695094134,
    "prof_backtrans_chrf": 74.72030243584219,
    "prof_backtrans_bertscore": 0.9449286460876465,
    "prof_backtrans_labse": 0.9715278148651123,
    "prof_backtrans_xlm_roberta": 0.9090030789375305,
    "llm_vs_prof_backtrans_bleu": 56.31385661313571,
    "llm_vs_prof_backtrans_chrf": 76.67469875143576,
    "llm_vs_prof_backtrans_bertscore": 0.9485450983047485,
    "llm_vs_prof_backtrans_labse": 0.9694516062736511
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 44.939455153632984,
    "same_lang_chrf": 74.0537724176211,
    "same_lang_bertscore": 0.8521958589553833,
    "same_lang_comet": 0.8966178297996521,
    "cross_lang_xlm_roberta": 0.9899022579193115,
    "cross_lang_labse": 0.9946914315223694,
    "cross_lang_mbert": 0.9903151392936707,
    "cross_lang_comet_qe": 0.5495907068252563,
    "backtrans_bleu": 70.98362943721318,
    "backtrans_chrf": 84.54502001061498,
    "backtrans_bertscore": 0.9813652634620667,
    "prof_backtrans_bleu": 61.82270570055685,
    "prof_backtrans_chrf": 81.24326160433888,
    "prof_backtrans_bertscore": 0.9522175788879395,
    "prof_backtrans_labse": 0.980351448059082,
    "prof_backtrans_xlm_roberta": 0.9208818078041077,
    "llm_vs_prof_backtrans_bleu": 64.62140426309325,
    "llm_vs_prof_backtrans_chrf": 80.84510066313982,
    "llm_vs_prof_backtrans_bertscore": 0.9514143466949463,
    "llm_vs_prof_backtrans_labse": 0.9757905602455139
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 41.11934148752912,
    "same_lang_chrf": 69.15538768911334,
    "same_lang_bertscore": 0.8881194591522217,
    "same_lang_comet": 0.9265645742416382,
    "cross_lang_xlm_roberta": 0.9515135288238525,
    "cross_lang_labse": 0.98919278383255,
    "cross_lang_mbert": 0.9212572574615479,
    "cross_lang_comet_qe": 0.5527585744857788,
    "backtrans_bleu": 55.005571565447404,
    "backtrans_chrf": 76.70817120268485,
    "backtrans_bertscore": 0.9662876129150391,
    "prof_backtrans_bleu": 34.923613034612885,
    "prof_backtrans_chrf": 67.04129083282085,
    "prof_backtrans_bertscore": 0.9245145320892334,
    "prof_backtrans_labse": 0.9546915888786316,
    "prof_backtrans_xlm_roberta": 0.909205973148346,
    "llm_vs_prof_backtrans_bleu": 44.42773184289181,
    "llm_vs_prof_backtrans_chrf": 67.88861117078497,
    "llm_vs_prof_backtrans_bertscore": 0.9326174259185791,
    "llm_vs_prof_backtrans_labse": 0.960213303565979
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 29.065249351515384,
    "same_lang_chrf": 51.753782266302714,
    "same_lang_bertscore": 0.8967431783676147,
    "same_lang_comet": 0.9359037280082703,
    "cross_lang_xlm_roberta": 0.9742311835289001,
    "cross_lang_labse": 0.9646866321563721,
    "cross_lang_mbert": 0.8516741394996643,
    "cross_lang_comet_qe": 0.4763562083244324,
    "backtrans_bleu": 46.80714112994902,
    "backtrans_chrf": 71.97992161566552,
    "backtrans_bertscore": 0.9374986290931702,
    "prof_backtrans_bleu": 47.58769015024176,
    "prof_backtrans_chrf": 73.49261035118573,
    "prof_backtrans_bertscore": 0.9638882875442505,
    "prof_backtrans_labse": 0.9737603664398193,
    "prof_backtrans_xlm_roberta": 0.9130194187164307,
    "llm_vs_prof_backtrans_bleu": 50.11315869016779,
    "llm_vs_prof_backtrans_chrf": 73.81946325929862,
    "llm_vs_prof_backtrans_bertscore": 0.9364252090454102,
    "llm_vs_prof_backtrans_labse": 0.9724420309066772
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 43.98624831820901,
    "same_lang_chrf": 70.57271390206043,
    "same_lang_bertscore": 0.8620073199272156,
    "same_lang_comet": 0.8042963743209839,
    "cross_lang_xlm_roberta": 0.9596836566925049,
    "cross_lang_labse": 0.9858098030090332,
    "cross_lang_mbert": 0.9744555354118347,
    "cross_lang_comet_qe": -0.39273953437805176,
    "backtrans_bleu": 45.971738781960426,
    "backtrans_chrf": 73.67931005813783,
    "backtrans_bertscore": 0.9361304640769958,
    "prof_backtrans_bleu": 57.91587472120195,
    "prof_backtrans_chrf": 78.79040831767463,
    "prof_backtrans_bertscore": 0.9633983373641968,
    "prof_backtrans_labse": 0.9840395450592041,
    "prof_backtrans_xlm_roberta": 0.8624703288078308,
    "llm_vs_prof_backtrans_bleu": 45.58417602679682,
    "llm_vs_prof_backtrans_chrf": 72.48441088136464,
    "llm_vs_prof_backtrans_bertscore": 0.9274100661277771,
    "llm_vs_prof_backtrans_labse": 0.9790520668029785
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 66.03276912409066,
    "same_lang_chrf": 82.6367247399864,
    "same_lang_bertscore": 0.9522085785865784,
    "same_lang_comet": 0.9249298572540283,
    "cross_lang_xlm_roberta": 0.9771530628204346,
    "cross_lang_labse": 0.9877901077270508,
    "cross_lang_mbert": 0.9899308681488037,
    "cross_lang_comet_qe": 0.5267592668533325,
    "backtrans_bleu": 75.14335853087192,
    "backtrans_chrf": 88.10433275250013,
    "backtrans_bertscore": 0.976062536239624,
    "prof_backtrans_bleu": 72.91563255258573,
    "prof_backtrans_chrf": 86.01768752556974,
    "prof_backtrans_bertscore": 0.9563243985176086,
    "prof_backtrans_labse": 0.9935356974601746,
    "prof_backtrans_xlm_roberta": 0.9799644947052002,
    "llm_vs_prof_backtrans_bleu": 78.21596600857924,
    "llm_vs_prof_backtrans_chrf": 88.85868893751537,
    "llm_vs_prof_backtrans_bertscore": 0.9600133895874023,
    "llm_vs_prof_backtrans_labse": 0.9867110252380371
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 17.80899165828842,
    "same_lang_chrf": 44.305502714137425,
    "same_lang_bertscore": 0.9130043983459473,
    "same_lang_comet": 0.9297057390213013,
    "cross_lang_xlm_roberta": 0.937089741230011,
    "cross_lang_labse": 0.9935716390609741,
    "cross_lang_mbert": 0.9847639799118042,
    "cross_lang_comet_qe": 0.47431254386901855,
    "backtrans_bleu": 62.29439142680519,
    "backtrans_chrf": 81.70677476027018,
    "backtrans_bertscore": 0.9766931533813477,
    "prof_backtrans_bleu": 62.38961030255645,
    "prof_backtrans_chrf": 80.69734650190597,
    "prof_backtrans_bertscore": 0.9742244482040405,
    "prof_backtrans_labse": 0.9939786791801453,
    "prof_backtrans_xlm_roberta": 0.9285637736320496,
    "llm_vs_prof_backtrans_bleu": 65.74304602805293,
    "llm_vs_prof_backtrans_chrf": 83.91434823469596,
    "llm_vs_prof_backtrans_bertscore": 0.9730364084243774,
    "llm_vs_prof_backtrans_labse": 0.9940764904022217
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 51.40038611718991,
    "same_lang_chrf": 68.56556540678457,
    "same_lang_bertscore": 0.9005788564682007,
    "same_lang_comet": 0.9242088794708252,
    "cross_lang_xlm_roberta": 0.9769635200500488,
    "cross_lang_labse": 0.9910541772842407,
    "cross_lang_mbert": 0.9833018183708191,
    "cross_lang_comet_qe": 0.5169858336448669,
    "backtrans_bleu": 73.00431514584783,
    "backtrans_chrf": 85.81074643992416,
    "backtrans_bertscore": 0.9772728681564331,
    "prof_backtrans_bleu": 65.26631073929148,
    "prof_backtrans_chrf": 83.05585390151734,
    "prof_backtrans_bertscore": 0.9618833661079407,
    "prof_backtrans_labse": 0.9820159077644348,
    "prof_backtrans_xlm_roberta": 0.9084261655807495,
    "llm_vs_prof_backtrans_bleu": 67.18372378081224,
    "llm_vs_prof_backtrans_chrf": 82.6349311106043,
    "llm_vs_prof_backtrans_bertscore": 0.9673689007759094,
    "llm_vs_prof_backtrans_labse": 0.9806206822395325
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 44.79430485899724,
    "same_lang_chrf": 73.8140501696884,
    "same_lang_bertscore": 0.8710489273071289,
    "same_lang_comet": 0.9194823503494263,
    "cross_lang_xlm_roberta": 0.9758954048156738,
    "cross_lang_labse": 0.9910827875137329,
    "cross_lang_mbert": 0.9948284029960632,
    "cross_lang_comet_qe": 0.5446474552154541,
    "backtrans_bleu": 76.70267107005486,
    "backtrans_chrf": 87.93060649362651,
    "backtrans_bertscore": 0.9811768531799316,
    "prof_backtrans_bleu": 77.54676272212136,
    "prof_backtrans_chrf": 88.64747413846165,
    "prof_backtrans_bertscore": 0.9735519886016846,
    "prof_backtrans_labse": 0.9897146821022034,
    "prof_backtrans_xlm_roberta": 0.9403944611549377,
    "llm_vs_prof_backtrans_bleu": 81.1995210313851,
    "llm_vs_prof_backtrans_chrf": 89.53718894923082,
    "llm_vs_prof_backtrans_bertscore": 0.9772411584854126,
    "llm_vs_prof_backtrans_labse": 0.9853706359863281
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 40.03116097617878,
    "same_lang_chrf": 68.42052067675728,
    "same_lang_bertscore": 0.8478412628173828,
    "same_lang_comet": 0.923001766204834,
    "cross_lang_xlm_roberta": 0.9370904564857483,
    "cross_lang_labse": 0.9890377521514893,
    "cross_lang_mbert": 0.9789038300514221,
    "cross_lang_comet_qe": 0.5000779628753662,
    "backtrans_bleu": 64.74494934643707,
    "backtrans_chrf": 82.16702119523042,
    "backtrans_bertscore": 0.9742028117179871,
    "prof_backtrans_bleu": 64.18754764850871,
    "prof_backtrans_chrf": 82.75849811593096,
    "prof_backtrans_bertscore": 0.9586829543113708,
    "prof_backtrans_labse": 0.9634618759155273,
    "prof_backtrans_xlm_roberta": 0.9224109649658203,
    "llm_vs_prof_backtrans_bleu": 65.36438447666204,
    "llm_vs_prof_backtrans_chrf": 81.3020557703579,
    "llm_vs_prof_backtrans_bertscore": 0.9612443447113037,
    "llm_vs_prof_backtrans_labse": 0.973667323589325
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 24.70778132515663,
    "same_lang_chrf": 45.086221838218556,
    "same_lang_bertscore": 0.8870515823364258,
    "same_lang_comet": 0.9300328493118286,
    "cross_lang_xlm_roberta": 0.9669860601425171,
    "cross_lang_labse": 0.9881801605224609,
    "cross_lang_mbert": 0.9905744194984436,
    "cross_lang_comet_qe": 0.5126925110816956,
    "backtrans_bleu": 62.028503890885986,
    "backtrans_chrf": 82.19675799645884,
    "backtrans_bertscore": 0.9715584516525269,
    "prof_backtrans_bleu": 66.09615832076086,
    "prof_backtrans_chrf": 83.28071249750383,
    "prof_backtrans_bertscore": 0.9514321088790894,
    "prof_backtrans_labse": 0.9887184500694275,
    "prof_backtrans_xlm_roberta": 0.9198623895645142,
    "llm_vs_prof_backtrans_bleu": 65.91952661731372,
    "llm_vs_prof_backtrans_chrf": 83.81485326991435,
    "llm_vs_prof_backtrans_bertscore": 0.9531053900718689,
    "llm_vs_prof_backtrans_labse": 0.9869104623794556
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 48.59281019483853,
    "same_lang_chrf": 73.4294912900778,
    "same_lang_bertscore": 0.8859695792198181,
    "same_lang_comet": 0.8262514472007751,
    "cross_lang_xlm_roberta": 0.9731919169425964,
    "cross_lang_labse": 0.9885371923446655,
    "cross_lang_mbert": 0.985149085521698,
    "cross_lang_comet_qe": 0.4717579782009125,
    "backtrans_bleu": 71.11415506598875,
    "backtrans_chrf": 86.88660380846025,
    "backtrans_bertscore": 0.973883330821991,
    "prof_backtrans_bleu": 73.85284295378258,
    "prof_backtrans_chrf": 86.38625143951928,
    "prof_backtrans_bertscore": 0.9774391651153564,
    "prof_backtrans_labse": 0.9899834394454956,
    "prof_backtrans_xlm_roberta": 0.8921393752098083,
    "llm_vs_prof_backtrans_bleu": 71.93659782092918,
    "llm_vs_prof_backtrans_chrf": 85.71616278102833,
    "llm_vs_prof_backtrans_bertscore": 0.9732791781425476,
    "llm_vs_prof_backtrans_labse": 0.9806331396102905
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 67.21491493798901,
    "same_lang_chrf": 83.66575371770416,
    "same_lang_bertscore": 0.9417710304260254,
    "same_lang_comet": 0.9236600399017334,
    "cross_lang_xlm_roberta": 0.9595812559127808,
    "cross_lang_labse": 0.9972193241119385,
    "cross_lang_mbert": 0.987055242061615,
    "cross_lang_comet_qe": 0.5068076848983765,
    "backtrans_bleu": 82.12883601940895,
    "backtrans_chrf": 90.52244468094494,
    "backtrans_bertscore": 0.9868239760398865,
    "prof_backtrans_bleu": 69.6225489000731,
    "prof_backtrans_chrf": 83.90463320843804,
    "prof_backtrans_bertscore": 0.9696113467216492,
    "prof_backtrans_labse": 0.9931052923202515,
    "prof_backtrans_xlm_roberta": 0.9804990887641907,
    "llm_vs_prof_backtrans_bleu": 79.50172642449108,
    "llm_vs_prof_backtrans_chrf": 88.75132923019869,
    "llm_vs_prof_backtrans_bertscore": 0.9742370843887329,
    "llm_vs_prof_backtrans_labse": 0.9914198517799377
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 16.469946312804684,
    "same_lang_chrf": 55.70091193093752,
    "same_lang_bertscore": 0.9235551357269287,
    "same_lang_comet": 0.9347898960113525,
    "cross_lang_xlm_roberta": 0.9295595288276672,
    "cross_lang_labse": 0.9856011867523193,
    "cross_lang_mbert": 0.9375303983688354,
    "cross_lang_comet_qe": 0.5291178822517395,
    "backtrans_bleu": 58.42555872321227,
    "backtrans_chrf": 78.6502592364912,
    "backtrans_bertscore": 0.9722182154655457,
    "prof_backtrans_bleu": 55.49156738131405,
    "prof_backtrans_chrf": 77.07445501275268,
    "prof_backtrans_bertscore": 0.9692149758338928,
    "prof_backtrans_labse": 0.9902724623680115,
    "prof_backtrans_xlm_roberta": 0.9378025531768799,
    "llm_vs_prof_backtrans_bleu": 68.12049949651299,
    "llm_vs_prof_backtrans_chrf": 84.45157582816637,
    "llm_vs_prof_backtrans_bertscore": 0.9705086946487427,
    "llm_vs_prof_backtrans_labse": 0.9858947396278381
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 53.986018262442975,
    "same_lang_chrf": 69.36998371614207,
    "same_lang_bertscore": 0.8984798192977905,
    "same_lang_comet": 0.924127459526062,
    "cross_lang_xlm_roberta": 0.9704647660255432,
    "cross_lang_labse": 0.9876373410224915,
    "cross_lang_mbert": 0.9859530925750732,
    "cross_lang_comet_qe": 0.48826202750205994,
    "backtrans_bleu": 63.502780344775665,
    "backtrans_chrf": 80.52477347883914,
    "backtrans_bertscore": 0.9721165895462036,
    "prof_backtrans_bleu": 59.82395263755284,
    "prof_backtrans_chrf": 79.32302267225553,
    "prof_backtrans_bertscore": 0.9604167938232422,
    "prof_backtrans_labse": 0.9791017174720764,
    "prof_backtrans_xlm_roberta": 0.9130010008811951,
    "llm_vs_prof_backtrans_bleu": 66.59340151589956,
    "llm_vs_prof_backtrans_chrf": 81.27368137222135,
    "llm_vs_prof_backtrans_bertscore": 0.9677096605300903,
    "llm_vs_prof_backtrans_labse": 0.9850733280181885
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 53.264127094879214,
    "same_lang_chrf": 78.83856072217532,
    "same_lang_bertscore": 0.8892444968223572,
    "same_lang_comet": 0.9230450391769409,
    "cross_lang_xlm_roberta": 0.9917967915534973,
    "cross_lang_labse": 0.9981702566146851,
    "cross_lang_mbert": 0.9973878264427185,
    "cross_lang_comet_qe": 0.5040403008460999,
    "backtrans_bleu": 80.88316905963532,
    "backtrans_chrf": 89.53330610178999,
    "backtrans_bertscore": 0.989249587059021,
    "prof_backtrans_bleu": 73.99578369440324,
    "prof_backtrans_chrf": 86.55043202838193,
    "prof_backtrans_bertscore": 0.9750385284423828,
    "prof_backtrans_labse": 0.9910321235656738,
    "prof_backtrans_xlm_roberta": 0.9418838620185852,
    "llm_vs_prof_backtrans_bleu": 79.23573778126263,
    "llm_vs_prof_backtrans_chrf": 88.90408983732847,
    "llm_vs_prof_backtrans_bertscore": 0.9759699106216431,
    "llm_vs_prof_backtrans_labse": 0.9890496134757996
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 39.03320596148023,
    "same_lang_chrf": 70.60552098346685,
    "same_lang_bertscore": 0.8883293271064758,
    "same_lang_comet": 0.9364254474639893,
    "cross_lang_xlm_roberta": 0.9519587159156799,
    "cross_lang_labse": 0.9933683276176453,
    "cross_lang_mbert": 0.988881528377533,
    "cross_lang_comet_qe": 0.5292121171951294,
    "backtrans_bleu": 62.55344758339078,
    "backtrans_chrf": 80.39690967266738,
    "backtrans_bertscore": 0.9764535427093506,
    "prof_backtrans_bleu": 53.56086342957735,
    "prof_backtrans_chrf": 77.11004890281163,
    "prof_backtrans_bertscore": 0.9556028246879578,
    "prof_backtrans_labse": 0.9728401303291321,
    "prof_backtrans_xlm_roberta": 0.9156333208084106,
    "llm_vs_prof_backtrans_bleu": 60.309882304487125,
    "llm_vs_prof_backtrans_chrf": 78.20993339036131,
    "llm_vs_prof_backtrans_bertscore": 0.9587388038635254,
    "llm_vs_prof_backtrans_labse": 0.9791926741600037
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 26.81541980485527,
    "same_lang_chrf": 46.96627685521847,
    "same_lang_bertscore": 0.8984612822532654,
    "same_lang_comet": 0.9352314472198486,
    "cross_lang_xlm_roberta": 0.9666882157325745,
    "cross_lang_labse": 0.9783533215522766,
    "cross_lang_mbert": 0.9910444021224976,
    "cross_lang_comet_qe": 0.5431304574012756,
    "backtrans_bleu": 53.32384119147795,
    "backtrans_chrf": 76.89526709390562,
    "backtrans_bertscore": 0.9663664102554321,
    "prof_backtrans_bleu": 55.76722693495933,
    "prof_backtrans_chrf": 78.22134241199367,
    "prof_backtrans_bertscore": 0.9671114683151245,
    "prof_backtrans_labse": 0.9866206645965576,
    "prof_backtrans_xlm_roberta": 0.9351652264595032,
    "llm_vs_prof_backtrans_bleu": 61.04484309141688,
    "llm_vs_prof_backtrans_chrf": 81.25556579164683,
    "llm_vs_prof_backtrans_bertscore": 0.9639270305633545,
    "llm_vs_prof_backtrans_labse": 0.9778468608856201
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 52.637324984137834,
    "same_lang_chrf": 75.1020728934787,
    "same_lang_bertscore": 0.8921524286270142,
    "same_lang_comet": 0.8299257755279541,
    "cross_lang_xlm_roberta": 0.9910594820976257,
    "cross_lang_labse": 0.9975073337554932,
    "cross_lang_mbert": 0.9946199655532837,
    "cross_lang_comet_qe": 0.5218952894210815,
    "backtrans_bleu": 78.7380395084969,
    "backtrans_chrf": 88.82300197630612,
    "backtrans_bertscore": 0.9850550889968872,
    "prof_backtrans_bleu": 69.52253312939244,
    "prof_backtrans_chrf": 84.17382432226862,
    "prof_backtrans_bertscore": 0.9750869870185852,
    "prof_backtrans_labse": 0.9895660281181335,
    "prof_backtrans_xlm_roberta": 0.8869591951370239,
    "llm_vs_prof_backtrans_bleu": 75.44759146668468,
    "llm_vs_prof_backtrans_chrf": 86.17659526903047,
    "llm_vs_prof_backtrans_bertscore": 0.9734615087509155,
    "llm_vs_prof_backtrans_labse": 0.9877109527587891
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 69.99522933693842,
    "same_lang_chrf": 84.84878913603086,
    "same_lang_bertscore": 0.963482141494751,
    "same_lang_comet": 0.9267817735671997,
    "cross_lang_xlm_roberta": 0.9392811059951782,
    "cross_lang_labse": 0.9926037192344666,
    "cross_lang_mbert": 0.9829205274581909,
    "cross_lang_comet_qe": 0.4939099848270416,
    "backtrans_bleu": 72.76712423428758,
    "backtrans_chrf": 86.18163252837148,
    "backtrans_bertscore": 0.9761569499969482,
    "prof_backtrans_bleu": 65.63685700145315,
    "prof_backtrans_chrf": 82.48456658056426,
    "prof_backtrans_bertscore": 0.9698047637939453,
    "prof_backtrans_labse": 0.9927743077278137,
    "prof_backtrans_xlm_roberta": 0.9744287729263306,
    "llm_vs_prof_backtrans_bleu": 75.46354817540934,
    "llm_vs_prof_backtrans_chrf": 86.2159405426409,
    "llm_vs_prof_backtrans_bertscore": 0.9669362902641296,
    "llm_vs_prof_backtrans_labse": 0.9920104146003723
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 22.901743975512648,
    "same_lang_chrf": 57.445270459028855,
    "same_lang_bertscore": 0.9173030257225037,
    "same_lang_comet": 0.9321300983428955,
    "cross_lang_xlm_roberta": 0.9812139868736267,
    "cross_lang_labse": 0.9832063317298889,
    "cross_lang_mbert": 0.9911066293716431,
    "cross_lang_comet_qe": 0.5326895713806152,
    "backtrans_bleu": 59.3906769102909,
    "backtrans_chrf": 79.79355130679505,
    "backtrans_bertscore": 0.9710366725921631,
    "prof_backtrans_bleu": 41.87263768913038,
    "prof_backtrans_chrf": 70.33252132341109,
    "prof_backtrans_bertscore": 0.954278826713562,
    "prof_backtrans_labse": 0.983784019947052,
    "prof_backtrans_xlm_roberta": 0.9567919969558716,
    "llm_vs_prof_backtrans_bleu": 56.22568082980061,
    "llm_vs_prof_backtrans_chrf": 75.4215810611006,
    "llm_vs_prof_backtrans_bertscore": 0.9629618525505066,
    "llm_vs_prof_backtrans_labse": 0.9847019910812378
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 64.03898651799588,
    "same_lang_chrf": 76.50263268222963,
    "same_lang_bertscore": 0.9370086193084717,
    "same_lang_comet": 0.9320724010467529,
    "cross_lang_xlm_roberta": 0.9580442309379578,
    "cross_lang_labse": 0.9939683675765991,
    "cross_lang_mbert": 0.9807130694389343,
    "cross_lang_comet_qe": 0.48501908779144287,
    "backtrans_bleu": 59.456360435574815,
    "backtrans_chrf": 78.3471097355492,
    "backtrans_bertscore": 0.9698562026023865,
    "prof_backtrans_bleu": 57.777311803677435,
    "prof_backtrans_chrf": 78.94652279652955,
    "prof_backtrans_bertscore": 0.9585152864456177,
    "prof_backtrans_labse": 0.9812853336334229,
    "prof_backtrans_xlm_roberta": 0.9122770428657532,
    "llm_vs_prof_backtrans_bleu": 67.72789461164234,
    "llm_vs_prof_backtrans_chrf": 82.68274576914351,
    "llm_vs_prof_backtrans_bertscore": 0.9613229632377625,
    "llm_vs_prof_backtrans_labse": 0.9822368621826172
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 53.60073032225156,
    "same_lang_chrf": 79.4257313136889,
    "same_lang_bertscore": 0.9003734588623047,
    "same_lang_comet": 0.9274091720581055,
    "cross_lang_xlm_roberta": 0.9557483196258545,
    "cross_lang_labse": 0.992496132850647,
    "cross_lang_mbert": 0.9917181134223938,
    "cross_lang_comet_qe": 0.40251386165618896,
    "backtrans_bleu": 73.08896065327819,
    "backtrans_chrf": 86.21797095736888,
    "backtrans_bertscore": 0.9752095937728882,
    "prof_backtrans_bleu": 73.95045017417561,
    "prof_backtrans_chrf": 87.0529159297969,
    "prof_backtrans_bertscore": 0.969801664352417,
    "prof_backtrans_labse": 0.987761914730072,
    "prof_backtrans_xlm_roberta": 0.9407946467399597,
    "llm_vs_prof_backtrans_bleu": 72.96567439578268,
    "llm_vs_prof_backtrans_chrf": 85.5417541262248,
    "llm_vs_prof_backtrans_bertscore": 0.9715198278427124,
    "llm_vs_prof_backtrans_labse": 0.9872738718986511
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 39.761863938338955,
    "same_lang_chrf": 69.5680562286281,
    "same_lang_bertscore": 0.843265175819397,
    "same_lang_comet": 0.9295427799224854,
    "cross_lang_xlm_roberta": 0.9250966310501099,
    "cross_lang_labse": 0.983653724193573,
    "cross_lang_mbert": 0.9841607213020325,
    "cross_lang_comet_qe": 0.46307432651519775,
    "backtrans_bleu": 51.77274509896403,
    "backtrans_chrf": 75.50911896295234,
    "backtrans_bertscore": 0.960668683052063,
    "prof_backtrans_bleu": 49.68137415861702,
    "prof_backtrans_chrf": 75.53191278382734,
    "prof_backtrans_bertscore": 0.9421809315681458,
    "prof_backtrans_labse": 0.960161030292511,
    "prof_backtrans_xlm_roberta": 0.9063620567321777,
    "llm_vs_prof_backtrans_bleu": 56.439300775419895,
    "llm_vs_prof_backtrans_chrf": 77.29729094879443,
    "llm_vs_prof_backtrans_bertscore": 0.9460287094116211,
    "llm_vs_prof_backtrans_labse": 0.9660975337028503
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 24.889847554940864,
    "same_lang_chrf": 45.8570149620827,
    "same_lang_bertscore": 0.8941931128501892,
    "same_lang_comet": 0.9356048107147217,
    "cross_lang_xlm_roberta": 0.9636341333389282,
    "cross_lang_labse": 0.9418121576309204,
    "cross_lang_mbert": 0.9895091652870178,
    "cross_lang_comet_qe": 0.45881807804107666,
    "backtrans_bleu": 38.09232368633151,
    "backtrans_chrf": 67.83362087042131,
    "backtrans_bertscore": 0.9491307735443115,
    "prof_backtrans_bleu": 40.933904246924484,
    "prof_backtrans_chrf": 71.09185899384852,
    "prof_backtrans_bertscore": 0.9528375864028931,
    "prof_backtrans_labse": 0.9788380265235901,
    "prof_backtrans_xlm_roberta": 0.9231982827186584,
    "llm_vs_prof_backtrans_bleu": 46.29939897227106,
    "llm_vs_prof_backtrans_chrf": 72.98998644626745,
    "llm_vs_prof_backtrans_bertscore": 0.9477983117103577,
    "llm_vs_prof_backtrans_labse": 0.9515358805656433
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 56.824806395486966,
    "same_lang_chrf": 77.14031351045895,
    "same_lang_bertscore": 0.9066535234451294,
    "same_lang_comet": 0.8411957621574402,
    "cross_lang_xlm_roberta": 0.9845628142356873,
    "cross_lang_labse": 0.9956415295600891,
    "cross_lang_mbert": 0.9934766888618469,
    "cross_lang_comet_qe": 0.4771004021167755,
    "backtrans_bleu": 63.98527819887943,
    "backtrans_chrf": 82.7474999490655,
    "backtrans_bertscore": 0.97657310962677,
    "prof_backtrans_bleu": 59.47072661891523,
    "prof_backtrans_chrf": 79.28074005459295,
    "prof_backtrans_bertscore": 0.9596609473228455,
    "prof_backtrans_labse": 0.9841862320899963,
    "prof_backtrans_xlm_roberta": 0.8820345997810364,
    "llm_vs_prof_backtrans_bleu": 64.94394731367423,
    "llm_vs_prof_backtrans_chrf": 80.2063184298897,
    "llm_vs_prof_backtrans_bertscore": 0.9613438844680786,
    "llm_vs_prof_backtrans_labse": 0.9815772771835327
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 64.68811187857129,
    "same_lang_chrf": 82.04804821845751,
    "same_lang_bertscore": 0.940352201461792,
    "same_lang_comet": 0.9250952005386353,
    "cross_lang_xlm_roberta": 0.950555682182312,
    "cross_lang_labse": 0.9896268844604492,
    "cross_lang_mbert": 0.979330837726593,
    "cross_lang_comet_qe": 0.46863624453544617,
    "backtrans_bleu": 66.27948400906459,
    "backtrans_chrf": 82.48437270964547,
    "backtrans_bertscore": 0.9680083990097046,
    "prof_backtrans_bleu": 49.315022483190354,
    "prof_backtrans_chrf": 74.92745811869868,
    "prof_backtrans_bertscore": 0.9474560618400574,
    "prof_backtrans_labse": 0.985214114189148,
    "prof_backtrans_xlm_roberta": 0.9596306085586548,
    "llm_vs_prof_backtrans_bleu": 63.55196387219266,
    "llm_vs_prof_backtrans_chrf": 78.37397495671286,
    "llm_vs_prof_backtrans_bertscore": 0.9599483609199524,
    "llm_vs_prof_backtrans_labse": 0.9811030626296997
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 22.24305671562165,
    "same_lang_chrf": 54.39985695505505,
    "same_lang_bertscore": 0.9225908517837524,
    "same_lang_comet": 0.9305130243301392,
    "cross_lang_xlm_roberta": 0.9409322142601013,
    "cross_lang_labse": 0.9852669835090637,
    "cross_lang_mbert": 0.9792768359184265,
    "cross_lang_comet_qe": 0.513156533241272,
    "backtrans_bleu": 48.94022775928154,
    "backtrans_chrf": 73.4019192625514,
    "backtrans_bertscore": 0.9672507643699646,
    "prof_backtrans_bleu": 47.95560795293864,
    "prof_backtrans_chrf": 72.45232821713029,
    "prof_backtrans_bertscore": 0.962143063545227,
    "prof_backtrans_labse": 0.9884709119796753,
    "prof_backtrans_xlm_roberta": 0.9555149674415588,
    "llm_vs_prof_backtrans_bleu": 61.584435232555805,
    "llm_vs_prof_backtrans_chrf": 80.093711839063,
    "llm_vs_prof_backtrans_bertscore": 0.9664027690887451,
    "llm_vs_prof_backtrans_labse": 0.9856314063072205
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 56.754615001369906,
    "same_lang_chrf": 71.30371193575658,
    "same_lang_bertscore": 0.9079972505569458,
    "same_lang_comet": 0.928817868232727,
    "cross_lang_xlm_roberta": 0.9698916673660278,
    "cross_lang_labse": 0.9820812940597534,
    "cross_lang_mbert": 0.9820510149002075,
    "cross_lang_comet_qe": 0.5322521924972534,
    "backtrans_bleu": 56.10654702508355,
    "backtrans_chrf": 76.20875841571701,
    "backtrans_bertscore": 0.9653790593147278,
    "prof_backtrans_bleu": 51.604276709286815,
    "prof_backtrans_chrf": 75.19256559615152,
    "prof_backtrans_bertscore": 0.9576227068901062,
    "prof_backtrans_labse": 0.9766876697540283,
    "prof_backtrans_xlm_roberta": 0.9187650680541992,
    "llm_vs_prof_backtrans_bleu": 62.66331257948127,
    "llm_vs_prof_backtrans_chrf": 79.64800251755241,
    "llm_vs_prof_backtrans_bertscore": 0.9597516655921936,
    "llm_vs_prof_backtrans_labse": 0.9816337823867798
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 41.41362006387063,
    "same_lang_chrf": 71.26076308761267,
    "same_lang_bertscore": 0.8581104874610901,
    "same_lang_comet": 0.9176511764526367,
    "cross_lang_xlm_roberta": 0.9694401621818542,
    "cross_lang_labse": 0.9936433434486389,
    "cross_lang_mbert": 0.9570993185043335,
    "cross_lang_comet_qe": 0.4474858045578003,
    "backtrans_bleu": 66.39152668117828,
    "backtrans_chrf": 81.85622724165276,
    "backtrans_bertscore": 0.9710450768470764,
    "prof_backtrans_bleu": 62.135106652330045,
    "prof_backtrans_chrf": 79.10902163364325,
    "prof_backtrans_bertscore": 0.9580876231193542,
    "prof_backtrans_labse": 0.9798099398612976,
    "prof_backtrans_xlm_roberta": 0.9257335066795349,
    "llm_vs_prof_backtrans_bleu": 65.83268889891832,
    "llm_vs_prof_backtrans_chrf": 82.43659197795856,
    "llm_vs_prof_backtrans_bertscore": 0.9631394147872925,
    "llm_vs_prof_backtrans_labse": 0.9851375222206116
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 39.91192927030131,
    "same_lang_chrf": 68.26221387115515,
    "same_lang_bertscore": 0.8527853488922119,
    "same_lang_comet": 0.9301986694335938,
    "cross_lang_xlm_roberta": 0.9450226426124573,
    "cross_lang_labse": 0.9913538098335266,
    "cross_lang_mbert": 0.9800674915313721,
    "cross_lang_comet_qe": 0.515178918838501,
    "backtrans_bleu": 52.94407743831405,
    "backtrans_chrf": 74.79945759599738,
    "backtrans_bertscore": 0.972919225692749,
    "prof_backtrans_bleu": 38.92917830559685,
    "prof_backtrans_chrf": 69.49923465260807,
    "prof_backtrans_bertscore": 0.9253261685371399,
    "prof_backtrans_labse": 0.9420335292816162,
    "prof_backtrans_xlm_roberta": 0.9034312963485718,
    "llm_vs_prof_backtrans_bleu": 48.96845055668526,
    "llm_vs_prof_backtrans_chrf": 71.52696333134988,
    "llm_vs_prof_backtrans_bertscore": 0.9292449951171875,
    "llm_vs_prof_backtrans_labse": 0.9504972100257874
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 23.93789786151116,
    "same_lang_chrf": 46.09814894736228,
    "same_lang_bertscore": 0.8754630088806152,
    "same_lang_comet": 0.9348989129066467,
    "cross_lang_xlm_roberta": 0.9649434685707092,
    "cross_lang_labse": 0.9612635970115662,
    "cross_lang_mbert": 0.9712993502616882,
    "cross_lang_comet_qe": 0.523226797580719,
    "backtrans_bleu": 40.65719400027641,
    "backtrans_chrf": 69.17646647758636,
    "backtrans_bertscore": 0.9481614232063293,
    "prof_backtrans_bleu": 45.83058954167174,
    "prof_backtrans_chrf": 72.24393505310167,
    "prof_backtrans_bertscore": 0.960686445236206,
    "prof_backtrans_labse": 0.9816684722900391,
    "prof_backtrans_xlm_roberta": 0.9257835149765015,
    "llm_vs_prof_backtrans_bleu": 47.51369045408076,
    "llm_vs_prof_backtrans_chrf": 73.57437538597262,
    "llm_vs_prof_backtrans_bertscore": 0.9437941908836365,
    "llm_vs_prof_backtrans_labse": 0.9603501558303833
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 45.195558012719495,
    "same_lang_chrf": 71.13154762302604,
    "same_lang_bertscore": 0.8671360611915588,
    "same_lang_comet": 0.836211621761322,
    "cross_lang_xlm_roberta": 0.9882159233093262,
    "cross_lang_labse": 0.9906278252601624,
    "cross_lang_mbert": 0.9848973751068115,
    "cross_lang_comet_qe": 0.2988485097885132,
    "backtrans_bleu": 67.65919771774425,
    "backtrans_chrf": 83.36810630332225,
    "backtrans_bertscore": 0.9649503231048584,
    "prof_backtrans_bleu": 58.016039918188504,
    "prof_backtrans_chrf": 77.89071239499928,
    "prof_backtrans_bertscore": 0.9567840099334717,
    "prof_backtrans_labse": 0.9844179153442383,
    "prof_backtrans_xlm_roberta": 0.8877684473991394,
    "llm_vs_prof_backtrans_bleu": 66.5489757603462,
    "llm_vs_prof_backtrans_chrf": 81.17195048586323,
    "llm_vs_prof_backtrans_bertscore": 0.9647328853607178,
    "llm_vs_prof_backtrans_labse": 0.9810998439788818
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 66.71699828572781,
    "same_lang_chrf": 82.97052324880198,
    "same_lang_bertscore": 0.9411882162094116,
    "same_lang_comet": 0.90986168384552,
    "cross_lang_xlm_roberta": 0.993988573551178,
    "cross_lang_labse": 0.9922645092010498,
    "cross_lang_mbert": 0.9848948121070862,
    "cross_lang_comet_qe": 0.523295521736145,
    "backtrans_bleu": 77.96543747941068,
    "backtrans_chrf": 89.70909926455164,
    "backtrans_bertscore": 0.9821791052818298,
    "prof_backtrans_bleu": 69.5105254816909,
    "prof_backtrans_chrf": 85.10651140809739,
    "prof_backtrans_bertscore": 0.9527022242546082,
    "prof_backtrans_labse": 0.9883884191513062,
    "prof_backtrans_xlm_roberta": 0.9905329346656799,
    "llm_vs_prof_backtrans_bleu": 78.54147853668213,
    "llm_vs_prof_backtrans_chrf": 88.76705887282101,
    "llm_vs_prof_backtrans_bertscore": 0.9597821235656738,
    "llm_vs_prof_backtrans_labse": 0.9909486770629883
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 18.414619670636945,
    "same_lang_chrf": 51.37442599357129,
    "same_lang_bertscore": 0.9125926494598389,
    "same_lang_comet": 0.9237960577011108,
    "cross_lang_xlm_roberta": 0.9905391931533813,
    "cross_lang_labse": 0.9953554272651672,
    "cross_lang_mbert": 0.9881628155708313,
    "cross_lang_comet_qe": 0.5280972719192505,
    "backtrans_bleu": 62.44462644931235,
    "backtrans_chrf": 82.76658061746099,
    "backtrans_bertscore": 0.9724830985069275,
    "prof_backtrans_bleu": 64.30490544782894,
    "prof_backtrans_chrf": 82.6711213373225,
    "prof_backtrans_bertscore": 0.9755290150642395,
    "prof_backtrans_labse": 0.9875305891036987,
    "prof_backtrans_xlm_roberta": 0.9881553053855896,
    "llm_vs_prof_backtrans_bleu": 65.5478493588323,
    "llm_vs_prof_backtrans_chrf": 83.96312626676925,
    "llm_vs_prof_backtrans_bertscore": 0.9730170369148254,
    "llm_vs_prof_backtrans_labse": 0.9864805340766907
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 55.11282940846279,
    "same_lang_chrf": 71.38732653660487,
    "same_lang_bertscore": 0.8939353227615356,
    "same_lang_comet": 0.9033098816871643,
    "cross_lang_xlm_roberta": 0.9912281632423401,
    "cross_lang_labse": 0.9963569641113281,
    "cross_lang_mbert": 0.9787172079086304,
    "cross_lang_comet_qe": 0.49305588006973267,
    "backtrans_bleu": 72.88872199263854,
    "backtrans_chrf": 85.96711192770583,
    "backtrans_bertscore": 0.9755440354347229,
    "prof_backtrans_bleu": 67.54536787416164,
    "prof_backtrans_chrf": 84.33728079829598,
    "prof_backtrans_bertscore": 0.9623260498046875,
    "prof_backtrans_labse": 0.9851470589637756,
    "prof_backtrans_xlm_roberta": 0.9483180642127991,
    "llm_vs_prof_backtrans_bleu": 73.0832687524202,
    "llm_vs_prof_backtrans_chrf": 85.6175735965998,
    "llm_vs_prof_backtrans_bertscore": 0.9687846302986145,
    "llm_vs_prof_backtrans_labse": 0.9869036078453064
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 47.991050404416015,
    "same_lang_chrf": 76.13118131498817,
    "same_lang_bertscore": 0.8967480063438416,
    "same_lang_comet": 0.9009012579917908,
    "cross_lang_xlm_roberta": 0.9917245507240295,
    "cross_lang_labse": 0.9946731925010681,
    "cross_lang_mbert": 0.9902990460395813,
    "cross_lang_comet_qe": 0.47849804162979126,
    "backtrans_bleu": 75.31885382128043,
    "backtrans_chrf": 88.08952821042256,
    "backtrans_bertscore": 0.9735593795776367,
    "prof_backtrans_bleu": 73.26200977521543,
    "prof_backtrans_chrf": 86.9110968838538,
    "prof_backtrans_bertscore": 0.9626548290252686,
    "prof_backtrans_labse": 0.9858807325363159,
    "prof_backtrans_xlm_roberta": 0.9614092707633972,
    "llm_vs_prof_backtrans_bleu": 77.58363202432228,
    "llm_vs_prof_backtrans_chrf": 87.63324091998668,
    "llm_vs_prof_backtrans_bertscore": 0.9663347601890564,
    "llm_vs_prof_backtrans_labse": 0.9896082878112793
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 39.346730533751305,
    "same_lang_chrf": 68.53854353919833,
    "same_lang_bertscore": 0.8823768496513367,
    "same_lang_comet": 0.9252534508705139,
    "cross_lang_xlm_roberta": 0.9867169260978699,
    "cross_lang_labse": 0.9860339164733887,
    "cross_lang_mbert": 0.9733245968818665,
    "cross_lang_comet_qe": 0.5045831203460693,
    "backtrans_bleu": 62.920085934002664,
    "backtrans_chrf": 82.35467010782159,
    "backtrans_bertscore": 0.9660972952842712,
    "prof_backtrans_bleu": 60.83635169241109,
    "prof_backtrans_chrf": 81.62818440138341,
    "prof_backtrans_bertscore": 0.941917359828949,
    "prof_backtrans_labse": 0.9789609909057617,
    "prof_backtrans_xlm_roberta": 0.9680430293083191,
    "llm_vs_prof_backtrans_bleu": 66.01111620723827,
    "llm_vs_prof_backtrans_chrf": 81.72931967813784,
    "llm_vs_prof_backtrans_bertscore": 0.9523860216140747,
    "llm_vs_prof_backtrans_labse": 0.9768589735031128
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 24.3236915225821,
    "same_lang_chrf": 45.88400617103532,
    "same_lang_bertscore": 0.8881362676620483,
    "same_lang_comet": 0.915428638458252,
    "cross_lang_xlm_roberta": 0.9894961714744568,
    "cross_lang_labse": 0.9943398237228394,
    "cross_lang_mbert": 0.9853596091270447,
    "cross_lang_comet_qe": 0.5119181871414185,
    "backtrans_bleu": 58.2888476874641,
    "backtrans_chrf": 81.05691034297533,
    "backtrans_bertscore": 0.9649472236633301,
    "prof_backtrans_bleu": 59.26336338839099,
    "prof_backtrans_chrf": 81.00727422288317,
    "prof_backtrans_bertscore": 0.9430769085884094,
    "prof_backtrans_labse": 0.9863414168357849,
    "prof_backtrans_xlm_roberta": 0.9399323463439941,
    "llm_vs_prof_backtrans_bleu": 58.98548313399592,
    "llm_vs_prof_backtrans_chrf": 80.49782055764913,
    "llm_vs_prof_backtrans_bertscore": 0.9487373232841492,
    "llm_vs_prof_backtrans_labse": 0.9868834614753723
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 50.19492303672626,
    "same_lang_chrf": 73.77061975124137,
    "same_lang_bertscore": 0.8778196573257446,
    "same_lang_comet": 0.8245453238487244,
    "cross_lang_xlm_roberta": 0.9903770685195923,
    "cross_lang_labse": 0.9951986074447632,
    "cross_lang_mbert": 0.9817542433738708,
    "cross_lang_comet_qe": 0.4958253800868988,
    "backtrans_bleu": 71.1487367516647,
    "backtrans_chrf": 86.4867410796185,
    "backtrans_bertscore": 0.9735103845596313,
    "prof_backtrans_bleu": 71.45912746093751,
    "prof_backtrans_chrf": 86.7264911947662,
    "prof_backtrans_bertscore": 0.9484685063362122,
    "prof_backtrans_labse": 0.9843876957893372,
    "prof_backtrans_xlm_roberta": 0.8649681210517883,
    "llm_vs_prof_backtrans_bleu": 73.89723318108175,
    "llm_vs_prof_backtrans_chrf": 86.08529517855395,
    "llm_vs_prof_backtrans_bertscore": 0.9506087899208069,
    "llm_vs_prof_backtrans_labse": 0.9842896461486816
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 67.00093645980658,
    "same_lang_chrf": 84.18047084351292,
    "same_lang_bertscore": 0.9509368538856506,
    "same_lang_comet": 0.9097918272018433,
    "cross_lang_xlm_roberta": 0.9940113425254822,
    "cross_lang_labse": 0.9975607991218567,
    "cross_lang_mbert": 0.9886136054992676,
    "cross_lang_comet_qe": 0.4781768321990967,
    "backtrans_bleu": 81.7116804754863,
    "backtrans_chrf": 90.57123468047517,
    "backtrans_bertscore": 0.9884987473487854,
    "prof_backtrans_bleu": 71.54542434416736,
    "prof_backtrans_chrf": 85.07762105854357,
    "prof_backtrans_bertscore": 0.9732614159584045,
    "prof_backtrans_labse": 0.9936321973800659,
    "prof_backtrans_xlm_roberta": 0.9937477707862854,
    "llm_vs_prof_backtrans_bleu": 79.00421671429933,
    "llm_vs_prof_backtrans_chrf": 89.02492669591956,
    "llm_vs_prof_backtrans_bertscore": 0.9741241335868835,
    "llm_vs_prof_backtrans_labse": 0.9932902455329895
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 15.496859743327505,
    "same_lang_chrf": 56.408898521722065,
    "same_lang_bertscore": 0.9265072345733643,
    "same_lang_comet": 0.9264830350875854,
    "cross_lang_xlm_roberta": 0.9961997270584106,
    "cross_lang_labse": 0.9951745271682739,
    "cross_lang_mbert": 0.9934189915657043,
    "cross_lang_comet_qe": 0.5150975584983826,
    "backtrans_bleu": 68.18847503354087,
    "backtrans_chrf": 84.21779078604602,
    "backtrans_bertscore": 0.9777575135231018,
    "prof_backtrans_bleu": 63.78725967899505,
    "prof_backtrans_chrf": 81.6455240909769,
    "prof_backtrans_bertscore": 0.9795613288879395,
    "prof_backtrans_labse": 0.9940083622932434,
    "prof_backtrans_xlm_roberta": 0.9921340346336365,
    "llm_vs_prof_backtrans_bleu": 70.04554930200665,
    "llm_vs_prof_backtrans_chrf": 84.66270293552466,
    "llm_vs_prof_backtrans_bertscore": 0.9765574336051941,
    "llm_vs_prof_backtrans_labse": 0.9966136813163757
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 50.178807415841284,
    "same_lang_chrf": 66.54324961715463,
    "same_lang_bertscore": 0.8817058205604553,
    "same_lang_comet": 0.904961109161377,
    "cross_lang_xlm_roberta": 0.9882177114486694,
    "cross_lang_labse": 0.9857145547866821,
    "cross_lang_mbert": 0.9563664197921753,
    "cross_lang_comet_qe": 0.48435312509536743,
    "backtrans_bleu": 66.47575155168771,
    "backtrans_chrf": 80.43212582241723,
    "backtrans_bertscore": 0.9776812195777893,
    "prof_backtrans_bleu": 62.44049426581581,
    "prof_backtrans_chrf": 80.82146242028304,
    "prof_backtrans_bertscore": 0.9575457572937012,
    "prof_backtrans_labse": 0.9818311929702759,
    "prof_backtrans_xlm_roberta": 0.9428442120552063,
    "llm_vs_prof_backtrans_bleu": 63.97675923645098,
    "llm_vs_prof_backtrans_chrf": 78.25115361226848,
    "llm_vs_prof_backtrans_bertscore": 0.9578751921653748,
    "llm_vs_prof_backtrans_labse": 0.9749707579612732
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 53.25554203158362,
    "same_lang_chrf": 79.28684454697301,
    "same_lang_bertscore": 0.8797115087509155,
    "same_lang_comet": 0.9018360376358032,
    "cross_lang_xlm_roberta": 0.99323970079422,
    "cross_lang_labse": 0.9966334104537964,
    "cross_lang_mbert": 0.9933291077613831,
    "cross_lang_comet_qe": 0.4971069097518921,
    "backtrans_bleu": 79.06464970978054,
    "backtrans_chrf": 88.9313085688937,
    "backtrans_bertscore": 0.9864956140518188,
    "prof_backtrans_bleu": 71.99334243453963,
    "prof_backtrans_chrf": 86.0893795716681,
    "prof_backtrans_bertscore": 0.9621569514274597,
    "prof_backtrans_labse": 0.9859586954116821,
    "prof_backtrans_xlm_roberta": 0.9620301723480225,
    "llm_vs_prof_backtrans_bleu": 75.47855577449005,
    "llm_vs_prof_backtrans_chrf": 86.76271433037633,
    "llm_vs_prof_backtrans_bertscore": 0.9603756666183472,
    "llm_vs_prof_backtrans_labse": 0.9842466711997986
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 39.992597256910564,
    "same_lang_chrf": 69.86570543861814,
    "same_lang_bertscore": 0.8870259523391724,
    "same_lang_comet": 0.9284213781356812,
    "cross_lang_xlm_roberta": 0.9845482707023621,
    "cross_lang_labse": 0.9928634762763977,
    "cross_lang_mbert": 0.9860838055610657,
    "cross_lang_comet_qe": 0.5420852899551392,
    "backtrans_bleu": 56.798731619721764,
    "backtrans_chrf": 77.56246151595764,
    "backtrans_bertscore": 0.9681202173233032,
    "prof_backtrans_bleu": 57.26653903642229,
    "prof_backtrans_chrf": 79.37305020238789,
    "prof_backtrans_bertscore": 0.9488843083381653,
    "prof_backtrans_labse": 0.9836595058441162,
    "prof_backtrans_xlm_roberta": 0.9675909280776978,
    "llm_vs_prof_backtrans_bleu": 59.96604195544672,
    "llm_vs_prof_backtrans_chrf": 79.38401321294367,
    "llm_vs_prof_backtrans_bertscore": 0.9542570114135742,
    "llm_vs_prof_backtrans_labse": 0.9858375191688538
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 32.87754226711713,
    "same_lang_chrf": 52.64259755869682,
    "same_lang_bertscore": 0.9060822129249573,
    "same_lang_comet": 0.9351866245269775,
    "cross_lang_xlm_roberta": 0.9959121942520142,
    "cross_lang_labse": 0.993782103061676,
    "cross_lang_mbert": 0.9953237771987915,
    "cross_lang_comet_qe": 0.49635884165763855,
    "backtrans_bleu": 59.359432245283415,
    "backtrans_chrf": 79.8769131341639,
    "backtrans_bertscore": 0.968085765838623,
    "prof_backtrans_bleu": 57.84927627286129,
    "prof_backtrans_chrf": 79.20714259888054,
    "prof_backtrans_bertscore": 0.9643690586090088,
    "prof_backtrans_labse": 0.9849360585212708,
    "prof_backtrans_xlm_roberta": 0.9371274709701538,
    "llm_vs_prof_backtrans_bleu": 61.54054981011206,
    "llm_vs_prof_backtrans_chrf": 81.18328613229974,
    "llm_vs_prof_backtrans_bertscore": 0.9590098857879639,
    "llm_vs_prof_backtrans_labse": 0.9847080707550049
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 46.08132542204624,
    "same_lang_chrf": 72.76191628836224,
    "same_lang_bertscore": 0.8891509771347046,
    "same_lang_comet": 0.8048632144927979,
    "cross_lang_xlm_roberta": 0.990764856338501,
    "cross_lang_labse": 0.9933966398239136,
    "cross_lang_mbert": 0.9766658544540405,
    "cross_lang_comet_qe": 0.43603071570396423,
    "backtrans_bleu": 74.93644546234412,
    "backtrans_chrf": 87.82956542035521,
    "backtrans_bertscore": 0.9759351015090942,
    "prof_backtrans_bleu": 64.1190415848612,
    "prof_backtrans_chrf": 82.34414512489167,
    "prof_backtrans_bertscore": 0.9638574719429016,
    "prof_backtrans_labse": 0.982842743396759,
    "prof_backtrans_xlm_roberta": 0.8834360241889954,
    "llm_vs_prof_backtrans_bleu": 71.53741305120901,
    "llm_vs_prof_backtrans_chrf": 84.26163622948248,
    "llm_vs_prof_backtrans_bertscore": 0.9677878618240356,
    "llm_vs_prof_backtrans_labse": 0.9831638336181641
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 69.12467049273305,
    "same_lang_chrf": 85.24559378301915,
    "same_lang_bertscore": 0.9482910633087158,
    "same_lang_comet": 0.9092539548873901,
    "cross_lang_xlm_roberta": 0.9937615990638733,
    "cross_lang_labse": 0.9945371150970459,
    "cross_lang_mbert": 0.9895749092102051,
    "cross_lang_comet_qe": 0.49719321727752686,
    "backtrans_bleu": 79.10593569355996,
    "backtrans_chrf": 89.69489961543037,
    "backtrans_bertscore": 0.9826259613037109,
    "prof_backtrans_bleu": 66.10950019274998,
    "prof_backtrans_chrf": 82.72836365952053,
    "prof_backtrans_bertscore": 0.969643235206604,
    "prof_backtrans_labse": 0.9901296496391296,
    "prof_backtrans_xlm_roberta": 0.9904848337173462,
    "llm_vs_prof_backtrans_bleu": 77.20263505454393,
    "llm_vs_prof_backtrans_chrf": 87.39944516669668,
    "llm_vs_prof_backtrans_bertscore": 0.971716582775116,
    "llm_vs_prof_backtrans_labse": 0.9886661767959595
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 24.89249471565365,
    "same_lang_chrf": 59.82929913031091,
    "same_lang_bertscore": 0.9310507774353027,
    "same_lang_comet": 0.9319533109664917,
    "cross_lang_xlm_roberta": 0.9885497093200684,
    "cross_lang_labse": 0.9932014346122742,
    "cross_lang_mbert": 0.9743407368659973,
    "cross_lang_comet_qe": 0.4898516833782196,
    "backtrans_bleu": 53.459967575955176,
    "backtrans_chrf": 76.81557369369062,
    "backtrans_bertscore": 0.9687929153442383,
    "prof_backtrans_bleu": 51.433556760275714,
    "prof_backtrans_chrf": 75.92988371962842,
    "prof_backtrans_bertscore": 0.9686939716339111,
    "prof_backtrans_labse": 0.9941638708114624,
    "prof_backtrans_xlm_roberta": 0.9902735948562622,
    "llm_vs_prof_backtrans_bleu": 66.50437886889772,
    "llm_vs_prof_backtrans_chrf": 83.23188036934236,
    "llm_vs_prof_backtrans_bertscore": 0.9714585542678833,
    "llm_vs_prof_backtrans_labse": 0.9939364194869995
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 64.85127762917506,
    "same_lang_chrf": 77.36187797936707,
    "same_lang_bertscore": 0.9153581261634827,
    "same_lang_comet": 0.9248335957527161,
    "cross_lang_xlm_roberta": 0.9877930879592896,
    "cross_lang_labse": 0.992790937423706,
    "cross_lang_mbert": 0.9776755571365356,
    "cross_lang_comet_qe": 0.4638705551624298,
    "backtrans_bleu": 60.78625951724368,
    "backtrans_chrf": 79.4694538471281,
    "backtrans_bertscore": 0.968012273311615,
    "prof_backtrans_bleu": 66.75489299936937,
    "prof_backtrans_chrf": 83.69524491684251,
    "prof_backtrans_bertscore": 0.9676909446716309,
    "prof_backtrans_labse": 0.9875963926315308,
    "prof_backtrans_xlm_roberta": 0.9528251886367798,
    "llm_vs_prof_backtrans_bleu": 67.25064756281526,
    "llm_vs_prof_backtrans_chrf": 83.17558709409056,
    "llm_vs_prof_backtrans_bertscore": 0.9618858098983765,
    "llm_vs_prof_backtrans_labse": 0.9850219488143921
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 55.40388046278073,
    "same_lang_chrf": 81.39175756063376,
    "same_lang_bertscore": 0.882927417755127,
    "same_lang_comet": 0.9094107747077942,
    "cross_lang_xlm_roberta": 0.992119550704956,
    "cross_lang_labse": 0.9971872568130493,
    "cross_lang_mbert": 0.9901330471038818,
    "cross_lang_comet_qe": 0.4357909560203552,
    "backtrans_bleu": 76.9167736907108,
    "backtrans_chrf": 87.82536933394157,
    "backtrans_bertscore": 0.982309877872467,
    "prof_backtrans_bleu": 68.35255785332241,
    "prof_backtrans_chrf": 84.43429855495121,
    "prof_backtrans_bertscore": 0.9561997056007385,
    "prof_backtrans_labse": 0.9839907884597778,
    "prof_backtrans_xlm_roberta": 0.9548274278640747,
    "llm_vs_prof_backtrans_bleu": 73.84986040506037,
    "llm_vs_prof_backtrans_chrf": 86.02764468260752,
    "llm_vs_prof_backtrans_bertscore": 0.9549146890640259,
    "llm_vs_prof_backtrans_labse": 0.9841870069503784
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 35.358840355336746,
    "same_lang_chrf": 67.49815961066645,
    "same_lang_bertscore": 0.8851311802864075,
    "same_lang_comet": 0.9292222261428833,
    "cross_lang_xlm_roberta": 0.9812819957733154,
    "cross_lang_labse": 0.9738420248031616,
    "cross_lang_mbert": 0.9441204071044922,
    "cross_lang_comet_qe": 0.5199915766716003,
    "backtrans_bleu": 51.453329705561224,
    "backtrans_chrf": 74.87713889404174,
    "backtrans_bertscore": 0.956569492816925,
    "prof_backtrans_bleu": 45.11520694647382,
    "prof_backtrans_chrf": 74.23480145090696,
    "prof_backtrans_bertscore": 0.926665723323822,
    "prof_backtrans_labse": 0.9670774340629578,
    "prof_backtrans_xlm_roberta": 0.966213583946228,
    "llm_vs_prof_backtrans_bleu": 47.11180401670951,
    "llm_vs_prof_backtrans_chrf": 71.82571299137211,
    "llm_vs_prof_backtrans_bertscore": 0.9326682090759277,
    "llm_vs_prof_backtrans_labse": 0.9654477834701538
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 29.24277918167328,
    "same_lang_chrf": 49.88093806132228,
    "same_lang_bertscore": 0.9123051762580872,
    "same_lang_comet": 0.9255142211914062,
    "cross_lang_xlm_roberta": 0.992819607257843,
    "cross_lang_labse": 0.9878640174865723,
    "cross_lang_mbert": 0.9904302954673767,
    "cross_lang_comet_qe": 0.4739060699939728,
    "backtrans_bleu": 42.81283045395031,
    "backtrans_chrf": 71.19991217487397,
    "backtrans_bertscore": 0.9567816257476807,
    "prof_backtrans_bleu": 54.198114739107936,
    "prof_backtrans_chrf": 77.85707087342826,
    "prof_backtrans_bertscore": 0.9614302515983582,
    "prof_backtrans_labse": 0.9859113693237305,
    "prof_backtrans_xlm_roberta": 0.9426631331443787,
    "llm_vs_prof_backtrans_bleu": 53.40867774880308,
    "llm_vs_prof_backtrans_chrf": 77.42606618814007,
    "llm_vs_prof_backtrans_bertscore": 0.9570042490959167,
    "llm_vs_prof_backtrans_labse": 0.9860233068466187
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 57.97234296247732,
    "same_lang_chrf": 77.72303504794209,
    "same_lang_bertscore": 0.8989505767822266,
    "same_lang_comet": 0.803234875202179,
    "cross_lang_xlm_roberta": 0.98222416639328,
    "cross_lang_labse": 0.9841083288192749,
    "cross_lang_mbert": 0.9885917901992798,
    "cross_lang_comet_qe": 0.4323003888130188,
    "backtrans_bleu": 63.78116882226218,
    "backtrans_chrf": 82.82042614618719,
    "backtrans_bertscore": 0.9723496437072754,
    "prof_backtrans_bleu": 59.615683347666796,
    "prof_backtrans_chrf": 80.12736897594638,
    "prof_backtrans_bertscore": 0.9583204388618469,
    "prof_backtrans_labse": 0.981768786907196,
    "prof_backtrans_xlm_roberta": 0.8731778264045715,
    "llm_vs_prof_backtrans_bleu": 66.46338569994458,
    "llm_vs_prof_backtrans_chrf": 81.43870733626467,
    "llm_vs_prof_backtrans_bertscore": 0.9588426351547241,
    "llm_vs_prof_backtrans_labse": 0.9742215275764465
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 63.66075199744508,
    "same_lang_chrf": 82.52876907016629,
    "same_lang_bertscore": 0.9399499297142029,
    "same_lang_comet": 0.9093968272209167,
    "cross_lang_xlm_roberta": 0.9871034622192383,
    "cross_lang_labse": 0.9855953454971313,
    "cross_lang_mbert": 0.9760390520095825,
    "cross_lang_comet_qe": 0.3982146382331848,
    "backtrans_bleu": 62.6588439436767,
    "backtrans_chrf": 82.00248668496876,
    "backtrans_bertscore": 0.9624126553535461,
    "prof_backtrans_bleu": 48.13751134423205,
    "prof_backtrans_chrf": 74.5639932194361,
    "prof_backtrans_bertscore": 0.9505310654640198,
    "prof_backtrans_labse": 0.9819434881210327,
    "prof_backtrans_xlm_roberta": 0.9872400760650635,
    "llm_vs_prof_backtrans_bleu": 67.19397243911243,
    "llm_vs_prof_backtrans_chrf": 81.62121714116705,
    "llm_vs_prof_backtrans_bertscore": 0.9658339619636536,
    "llm_vs_prof_backtrans_labse": 0.9889583587646484
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 19.24640340626414,
    "same_lang_chrf": 55.977329424211185,
    "same_lang_bertscore": 0.914481520652771,
    "same_lang_comet": 0.9282485246658325,
    "cross_lang_xlm_roberta": 0.9959958791732788,
    "cross_lang_labse": 0.991195559501648,
    "cross_lang_mbert": 0.9945921897888184,
    "cross_lang_comet_qe": 0.4872509837150574,
    "backtrans_bleu": 51.77291386488824,
    "backtrans_chrf": 74.6317734661377,
    "backtrans_bertscore": 0.9681527614593506,
    "prof_backtrans_bleu": 56.19516169275524,
    "prof_backtrans_chrf": 77.38419564624958,
    "prof_backtrans_bertscore": 0.9713155031204224,
    "prof_backtrans_labse": 0.9919238686561584,
    "prof_backtrans_xlm_roberta": 0.9886413216590881,
    "llm_vs_prof_backtrans_bleu": 64.84802992072696,
    "llm_vs_prof_backtrans_chrf": 82.40013392151802,
    "llm_vs_prof_backtrans_bertscore": 0.9731841683387756,
    "llm_vs_prof_backtrans_labse": 0.99219810962677
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 53.82420455627659,
    "same_lang_chrf": 69.71281427322425,
    "same_lang_bertscore": 0.8802688717842102,
    "same_lang_comet": 0.9123789668083191,
    "cross_lang_xlm_roberta": 0.9869296550750732,
    "cross_lang_labse": 0.9840785264968872,
    "cross_lang_mbert": 0.9734312295913696,
    "cross_lang_comet_qe": 0.47972625494003296,
    "backtrans_bleu": 58.08305493136919,
    "backtrans_chrf": 77.68960172675627,
    "backtrans_bertscore": 0.965552568435669,
    "prof_backtrans_bleu": 49.78033369717323,
    "prof_backtrans_chrf": 75.31502862226272,
    "prof_backtrans_bertscore": 0.9527125954627991,
    "prof_backtrans_labse": 0.9835014939308167,
    "prof_backtrans_xlm_roberta": 0.9438716173171997,
    "llm_vs_prof_backtrans_bleu": 59.98592033581148,
    "llm_vs_prof_backtrans_chrf": 79.5166409572253,
    "llm_vs_prof_backtrans_bertscore": 0.9519832134246826,
    "llm_vs_prof_backtrans_labse": 0.98053377866745
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 42.46774501961923,
    "same_lang_chrf": 72.8855626299003,
    "same_lang_bertscore": 0.858080267906189,
    "same_lang_comet": 0.9041060209274292,
    "cross_lang_xlm_roberta": 0.9937700033187866,
    "cross_lang_labse": 0.9920005798339844,
    "cross_lang_mbert": 0.990755021572113,
    "cross_lang_comet_qe": 0.4539588391780853,
    "backtrans_bleu": 67.43714124957359,
    "backtrans_chrf": 83.40655907104406,
    "backtrans_bertscore": 0.975817859172821,
    "prof_backtrans_bleu": 65.60312112201771,
    "prof_backtrans_chrf": 83.08868234345724,
    "prof_backtrans_bertscore": 0.9574486613273621,
    "prof_backtrans_labse": 0.9879517555236816,
    "prof_backtrans_xlm_roberta": 0.9610523581504822,
    "llm_vs_prof_backtrans_bleu": 67.3403560101234,
    "llm_vs_prof_backtrans_chrf": 82.55019105435626,
    "llm_vs_prof_backtrans_bertscore": 0.9578545093536377,
    "llm_vs_prof_backtrans_labse": 0.9825497269630432
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 39.443815588349636,
    "same_lang_chrf": 69.78851414237397,
    "same_lang_bertscore": 0.8677670359611511,
    "same_lang_comet": 0.9248144626617432,
    "cross_lang_xlm_roberta": 0.9711594581604004,
    "cross_lang_labse": 0.9731239080429077,
    "cross_lang_mbert": 0.9232245683670044,
    "cross_lang_comet_qe": 0.43185198307037354,
    "backtrans_bleu": 51.51833621265267,
    "backtrans_chrf": 75.70703169160068,
    "backtrans_bertscore": 0.9425851702690125,
    "prof_backtrans_bleu": 36.62167445093418,
    "prof_backtrans_chrf": 68.89589976958305,
    "prof_backtrans_bertscore": 0.9288269281387329,
    "prof_backtrans_labse": 0.972498893737793,
    "prof_backtrans_xlm_roberta": 0.9722727537155151,
    "llm_vs_prof_backtrans_bleu": 46.844916721205486,
    "llm_vs_prof_backtrans_chrf": 71.04796849474974,
    "llm_vs_prof_backtrans_bertscore": 0.9473198056221008,
    "llm_vs_prof_backtrans_labse": 0.9780635237693787
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 30.246015173706038,
    "same_lang_chrf": 50.62790147477752,
    "same_lang_bertscore": 0.8972633481025696,
    "same_lang_comet": 0.927857518196106,
    "cross_lang_xlm_roberta": 0.9910354018211365,
    "cross_lang_labse": 0.9823166131973267,
    "cross_lang_mbert": 0.9696819186210632,
    "cross_lang_comet_qe": 0.4408690929412842,
    "backtrans_bleu": 43.79291329900226,
    "backtrans_chrf": 70.65870293136622,
    "backtrans_bertscore": 0.9547927975654602,
    "prof_backtrans_bleu": 50.55737163910678,
    "prof_backtrans_chrf": 75.16622396351745,
    "prof_backtrans_bertscore": 0.9629150032997131,
    "prof_backtrans_labse": 0.9854395985603333,
    "prof_backtrans_xlm_roberta": 0.9243046641349792,
    "llm_vs_prof_backtrans_bleu": 51.53193205314342,
    "llm_vs_prof_backtrans_chrf": 76.89370511072441,
    "llm_vs_prof_backtrans_bertscore": 0.95721834897995,
    "llm_vs_prof_backtrans_labse": 0.9811242818832397
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 41.536254932939315,
    "same_lang_chrf": 69.74119961223542,
    "same_lang_bertscore": 0.8731967806816101,
    "same_lang_comet": 0.815780520439148,
    "cross_lang_xlm_roberta": 0.9891025424003601,
    "cross_lang_labse": 0.98359614610672,
    "cross_lang_mbert": 0.9869638681411743,
    "cross_lang_comet_qe": 0.4149319529533386,
    "backtrans_bleu": 62.829095503785126,
    "backtrans_chrf": 81.22242802155179,
    "backtrans_bertscore": 0.9625242352485657,
    "prof_backtrans_bleu": 52.68926811020786,
    "prof_backtrans_chrf": 76.5112547686437,
    "prof_backtrans_bertscore": 0.947117805480957,
    "prof_backtrans_labse": 0.9752417802810669,
    "prof_backtrans_xlm_roberta": 0.8666201829910278,
    "llm_vs_prof_backtrans_bleu": 61.405402907580154,
    "llm_vs_prof_backtrans_chrf": 78.66452026668075,
    "llm_vs_prof_backtrans_bertscore": 0.9472965598106384,
    "llm_vs_prof_backtrans_labse": 0.9685064554214478
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 52.500769265231796,
    "same_lang_chrf": 73.2456927673536,
    "same_lang_bertscore": 0.9022186994552612,
    "same_lang_comet": 0.8983998894691467,
    "cross_lang_xlm_roberta": 0.9976149797439575,
    "cross_lang_labse": 0.9939408302307129,
    "cross_lang_mbert": 0.9894457459449768,
    "cross_lang_comet_qe": 0.40939128398895264,
    "backtrans_bleu": 74.15229334890573,
    "backtrans_chrf": 85.92870236511115,
    "backtrans_bertscore": 0.9793251156806946,
    "prof_backtrans_bleu": 58.77745717401906,
    "prof_backtrans_chrf": 77.52007085614582,
    "prof_backtrans_bertscore": 0.93295818567276,
    "prof_backtrans_labse": 0.968434751033783,
    "prof_backtrans_xlm_roberta": 0.9521312117576599,
    "llm_vs_prof_backtrans_bleu": 63.23443516917639,
    "llm_vs_prof_backtrans_chrf": 76.29194533649486,
    "llm_vs_prof_backtrans_bertscore": 0.9408520460128784,
    "llm_vs_prof_backtrans_labse": 0.9715234041213989
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.8444087123573,
    "same_lang_chrf": 39.70927851657159,
    "same_lang_bertscore": 0.8910343647003174,
    "same_lang_comet": 0.9236497282981873,
    "cross_lang_xlm_roberta": 0.9952610731124878,
    "cross_lang_labse": 0.9816471338272095,
    "cross_lang_mbert": 0.9884642958641052,
    "cross_lang_comet_qe": 0.40408623218536377,
    "backtrans_bleu": 52.66239830594554,
    "backtrans_chrf": 76.91968560431916,
    "backtrans_bertscore": 0.9524682760238647,
    "prof_backtrans_bleu": 54.10030500412257,
    "prof_backtrans_chrf": 75.85503595047221,
    "prof_backtrans_bertscore": 0.9360640645027161,
    "prof_backtrans_labse": 0.9757629632949829,
    "prof_backtrans_xlm_roberta": 0.9127019047737122,
    "llm_vs_prof_backtrans_bleu": 60.47052788076549,
    "llm_vs_prof_backtrans_chrf": 79.096662323342,
    "llm_vs_prof_backtrans_bertscore": 0.947944164276123,
    "llm_vs_prof_backtrans_labse": 0.9840560555458069
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 46.617979499703026,
    "same_lang_chrf": 63.146527312293074,
    "same_lang_bertscore": 0.8615206480026245,
    "same_lang_comet": 0.915642499923706,
    "cross_lang_xlm_roberta": 0.9946077466011047,
    "cross_lang_labse": 0.9852573275566101,
    "cross_lang_mbert": 0.9874329566955566,
    "cross_lang_comet_qe": 0.37790539860725403,
    "backtrans_bleu": 60.72545229885821,
    "backtrans_chrf": 79.36119871750742,
    "backtrans_bertscore": 0.9642333388328552,
    "prof_backtrans_bleu": 53.40907667570812,
    "prof_backtrans_chrf": 74.85158554023752,
    "prof_backtrans_bertscore": 0.9183192253112793,
    "prof_backtrans_labse": 0.9694688320159912,
    "prof_backtrans_xlm_roberta": 0.9377189874649048,
    "llm_vs_prof_backtrans_bleu": 54.60518793710795,
    "llm_vs_prof_backtrans_chrf": 73.49232215858858,
    "llm_vs_prof_backtrans_bertscore": 0.922310471534729,
    "llm_vs_prof_backtrans_labse": 0.9815888404846191
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 36.859645075730604,
    "same_lang_chrf": 66.38971241267902,
    "same_lang_bertscore": 0.8375900387763977,
    "same_lang_comet": 0.9006839990615845,
    "cross_lang_xlm_roberta": 0.9926959872245789,
    "cross_lang_labse": 0.9815984964370728,
    "cross_lang_mbert": 0.9746614098548889,
    "cross_lang_comet_qe": 0.36215120553970337,
    "backtrans_bleu": 69.3573394387768,
    "backtrans_chrf": 84.47707137331363,
    "backtrans_bertscore": 0.9701648950576782,
    "prof_backtrans_bleu": 63.60933927079809,
    "prof_backtrans_chrf": 79.68338337541752,
    "prof_backtrans_bertscore": 0.930678129196167,
    "prof_backtrans_labse": 0.9716708660125732,
    "prof_backtrans_xlm_roberta": 0.943461000919342,
    "llm_vs_prof_backtrans_bleu": 59.2596256078637,
    "llm_vs_prof_backtrans_chrf": 75.21243138777997,
    "llm_vs_prof_backtrans_bertscore": 0.9324002265930176,
    "llm_vs_prof_backtrans_labse": 0.9746100902557373
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 33.914566256631964,
    "same_lang_chrf": 64.32880351649266,
    "same_lang_bertscore": 0.8738732933998108,
    "same_lang_comet": 0.9273793697357178,
    "cross_lang_xlm_roberta": 0.9926081299781799,
    "cross_lang_labse": 0.9885528087615967,
    "cross_lang_mbert": 0.9824715852737427,
    "cross_lang_comet_qe": 0.4111604392528534,
    "backtrans_bleu": 60.6511467468215,
    "backtrans_chrf": 78.01003550405538,
    "backtrans_bertscore": 0.9639870524406433,
    "prof_backtrans_bleu": 36.05944928201337,
    "prof_backtrans_chrf": 65.10588274434609,
    "prof_backtrans_bertscore": 0.9339531064033508,
    "prof_backtrans_labse": 0.961692214012146,
    "prof_backtrans_xlm_roberta": 0.9573408365249634,
    "llm_vs_prof_backtrans_bleu": 45.0620775025582,
    "llm_vs_prof_backtrans_chrf": 68.37382232175817,
    "llm_vs_prof_backtrans_bertscore": 0.9467703700065613,
    "llm_vs_prof_backtrans_labse": 0.9692725539207458
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 17.631381935240356,
    "same_lang_chrf": 35.94633071611073,
    "same_lang_bertscore": 0.8355541825294495,
    "same_lang_comet": 0.9189231395721436,
    "cross_lang_xlm_roberta": 0.9810909628868103,
    "cross_lang_labse": 0.9577106833457947,
    "cross_lang_mbert": 0.9672595262527466,
    "cross_lang_comet_qe": 0.3693303167819977,
    "backtrans_bleu": 46.926900175561,
    "backtrans_chrf": 73.74310888476079,
    "backtrans_bertscore": 0.932515561580658,
    "prof_backtrans_bleu": 62.883233177894084,
    "prof_backtrans_chrf": 82.12250723074433,
    "prof_backtrans_bertscore": 0.9241935610771179,
    "prof_backtrans_labse": 0.965966522693634,
    "prof_backtrans_xlm_roberta": 0.89652019739151,
    "llm_vs_prof_backtrans_bleu": 45.60633586412642,
    "llm_vs_prof_backtrans_chrf": 70.91176087914995,
    "llm_vs_prof_backtrans_bertscore": 0.9280910491943359,
    "llm_vs_prof_backtrans_labse": 0.951245903968811
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 41.12583515359894,
    "same_lang_chrf": 63.50907630443533,
    "same_lang_bertscore": 0.8674487471580505,
    "same_lang_comet": 0.8132469058036804,
    "cross_lang_xlm_roberta": 0.9982017874717712,
    "cross_lang_labse": 0.9729093313217163,
    "cross_lang_mbert": 0.9687090516090393,
    "cross_lang_comet_qe": 0.35995957255363464,
    "backtrans_bleu": 63.50639386101275,
    "backtrans_chrf": 82.34531603698233,
    "backtrans_bertscore": 0.9538944363594055,
    "prof_backtrans_bleu": 55.32802053669507,
    "prof_backtrans_chrf": 77.25042590272982,
    "prof_backtrans_bertscore": 0.9351459741592407,
    "prof_backtrans_labse": 0.9566194415092468,
    "prof_backtrans_xlm_roberta": 0.9016135931015015,
    "llm_vs_prof_backtrans_bleu": 62.87133499050972,
    "llm_vs_prof_backtrans_chrf": 77.41154905183751,
    "llm_vs_prof_backtrans_bertscore": 0.9521120190620422,
    "llm_vs_prof_backtrans_labse": 0.9699572324752808
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 51.78418578335778,
    "same_lang_chrf": 72.09151036433667,
    "same_lang_bertscore": 0.8934183120727539,
    "same_lang_comet": 0.8931655883789062,
    "cross_lang_xlm_roberta": 0.9989477396011353,
    "cross_lang_labse": 0.9987168908119202,
    "cross_lang_mbert": 0.9706560373306274,
    "cross_lang_comet_qe": 0.386851966381073,
    "backtrans_bleu": 83.52265873319763,
    "backtrans_chrf": 90.90138596595006,
    "backtrans_bertscore": 0.988289475440979,
    "prof_backtrans_bleu": 57.27984895598155,
    "prof_backtrans_chrf": 76.48761281514817,
    "prof_backtrans_bertscore": 0.9523154497146606,
    "prof_backtrans_labse": 0.9710243940353394,
    "prof_backtrans_xlm_roberta": 0.9530855417251587,
    "llm_vs_prof_backtrans_bleu": 61.55814505770972,
    "llm_vs_prof_backtrans_chrf": 74.97606317495726,
    "llm_vs_prof_backtrans_bertscore": 0.9567362666130066,
    "llm_vs_prof_backtrans_labse": 0.9667603969573975
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 10.521198504065545,
    "same_lang_chrf": 42.42378957112666,
    "same_lang_bertscore": 0.8892729878425598,
    "same_lang_comet": 0.9233753681182861,
    "cross_lang_xlm_roberta": 0.9803248047828674,
    "cross_lang_labse": 0.9892564415931702,
    "cross_lang_mbert": 0.9207396507263184,
    "cross_lang_comet_qe": 0.3975834846496582,
    "backtrans_bleu": 60.366766874990766,
    "backtrans_chrf": 78.89259527235872,
    "backtrans_bertscore": 0.9668868184089661,
    "prof_backtrans_bleu": 43.32648119817753,
    "prof_backtrans_chrf": 69.00381170852064,
    "prof_backtrans_bertscore": 0.9470605850219727,
    "prof_backtrans_labse": 0.9730690717697144,
    "prof_backtrans_xlm_roberta": 0.9001184105873108,
    "llm_vs_prof_backtrans_bleu": 44.10989074038007,
    "llm_vs_prof_backtrans_chrf": 67.16475053670064,
    "llm_vs_prof_backtrans_bertscore": 0.9471850991249084,
    "llm_vs_prof_backtrans_labse": 0.9713829159736633
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 49.302930960223186,
    "same_lang_chrf": 64.97112789950626,
    "same_lang_bertscore": 0.8701326847076416,
    "same_lang_comet": 0.9143940210342407,
    "cross_lang_xlm_roberta": 0.9960289597511292,
    "cross_lang_labse": 0.9940766096115112,
    "cross_lang_mbert": 0.9834909439086914,
    "cross_lang_comet_qe": 0.3946213126182556,
    "backtrans_bleu": 68.80481214315334,
    "backtrans_chrf": 82.25181838596694,
    "backtrans_bertscore": 0.9756741523742676,
    "prof_backtrans_bleu": 51.325224846644474,
    "prof_backtrans_chrf": 74.19379616999035,
    "prof_backtrans_bertscore": 0.9519858956336975,
    "prof_backtrans_labse": 0.9694876074790955,
    "prof_backtrans_xlm_roberta": 0.938940167427063,
    "llm_vs_prof_backtrans_bleu": 58.6200387356446,
    "llm_vs_prof_backtrans_chrf": 75.76026965890446,
    "llm_vs_prof_backtrans_bertscore": 0.9573684334754944,
    "llm_vs_prof_backtrans_labse": 0.9676633477210999
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 46.621741654990494,
    "same_lang_chrf": 75.53657828974666,
    "same_lang_bertscore": 0.8715497255325317,
    "same_lang_comet": 0.9143592119216919,
    "cross_lang_xlm_roberta": 0.9916176795959473,
    "cross_lang_labse": 0.9940243363380432,
    "cross_lang_mbert": 0.9818927645683289,
    "cross_lang_comet_qe": 0.32979241013526917,
    "backtrans_bleu": 70.65948669098626,
    "backtrans_chrf": 82.70930163539147,
    "backtrans_bertscore": 0.9809905290603638,
    "prof_backtrans_bleu": 71.55095912687644,
    "prof_backtrans_chrf": 85.19746219411046,
    "prof_backtrans_bertscore": 0.9650497436523438,
    "prof_backtrans_labse": 0.9735466241836548,
    "prof_backtrans_xlm_roberta": 0.9465680718421936,
    "llm_vs_prof_backtrans_bleu": 65.49428596697571,
    "llm_vs_prof_backtrans_chrf": 78.4788374103258,
    "llm_vs_prof_backtrans_bertscore": 0.958056628704071,
    "llm_vs_prof_backtrans_labse": 0.9683647751808167
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 35.416602971274756,
    "same_lang_chrf": 67.53524016552241,
    "same_lang_bertscore": 0.8915954828262329,
    "same_lang_comet": 0.9331843852996826,
    "cross_lang_xlm_roberta": 0.981849730014801,
    "cross_lang_labse": 0.9808637499809265,
    "cross_lang_mbert": 0.9291239380836487,
    "cross_lang_comet_qe": 0.4032405912876129,
    "backtrans_bleu": 57.75745830085103,
    "backtrans_chrf": 75.44453563571868,
    "backtrans_bertscore": 0.959546685218811,
    "prof_backtrans_bleu": 37.62760910441701,
    "prof_backtrans_chrf": 65.98918755537511,
    "prof_backtrans_bertscore": 0.9397055506706238,
    "prof_backtrans_labse": 0.9650758504867554,
    "prof_backtrans_xlm_roberta": 0.9598478078842163,
    "llm_vs_prof_backtrans_bleu": 50.55593845049738,
    "llm_vs_prof_backtrans_chrf": 72.84574702006398,
    "llm_vs_prof_backtrans_bertscore": 0.9513056874275208,
    "llm_vs_prof_backtrans_labse": 0.9762782454490662
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 22.223308891503684,
    "same_lang_chrf": 41.12417065791799,
    "same_lang_bertscore": 0.825088620185852,
    "same_lang_comet": 0.9169381856918335,
    "cross_lang_xlm_roberta": 0.9828385710716248,
    "cross_lang_labse": 0.9886984825134277,
    "cross_lang_mbert": 0.923346996307373,
    "cross_lang_comet_qe": 0.3815847933292389,
    "backtrans_bleu": 53.31881304319924,
    "backtrans_chrf": 74.2246456887246,
    "backtrans_bertscore": 0.9593065977096558,
    "prof_backtrans_bleu": 60.02125172485981,
    "prof_backtrans_chrf": 80.65288916199852,
    "prof_backtrans_bertscore": 0.9615221619606018,
    "prof_backtrans_labse": 0.9721115231513977,
    "prof_backtrans_xlm_roberta": 0.902821958065033,
    "llm_vs_prof_backtrans_bleu": 54.80274052633903,
    "llm_vs_prof_backtrans_chrf": 72.45963619118481,
    "llm_vs_prof_backtrans_bertscore": 0.9484050869941711,
    "llm_vs_prof_backtrans_labse": 0.9636868238449097
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 46.32419972038494,
    "same_lang_chrf": 66.27886923210575,
    "same_lang_bertscore": 0.8512842059135437,
    "same_lang_comet": 0.8259378671646118,
    "cross_lang_xlm_roberta": 0.9876872897148132,
    "cross_lang_labse": 0.9821119904518127,
    "cross_lang_mbert": 0.9812994003295898,
    "cross_lang_comet_qe": 0.34488826990127563,
    "backtrans_bleu": 65.57119204948478,
    "backtrans_chrf": 81.59841998169955,
    "backtrans_bertscore": 0.9638857245445251,
    "prof_backtrans_bleu": 53.001363299513145,
    "prof_backtrans_chrf": 75.40066319800113,
    "prof_backtrans_bertscore": 0.9392889738082886,
    "prof_backtrans_labse": 0.9588500261306763,
    "prof_backtrans_xlm_roberta": 0.9016220569610596,
    "llm_vs_prof_backtrans_bleu": 58.82161690900752,
    "llm_vs_prof_backtrans_chrf": 74.44115133088457,
    "llm_vs_prof_backtrans_bertscore": 0.948434591293335,
    "llm_vs_prof_backtrans_labse": 0.9636112451553345
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 53.16622922493446,
    "same_lang_chrf": 73.24857110926717,
    "same_lang_bertscore": 0.8978298902511597,
    "same_lang_comet": 0.9042116403579712,
    "cross_lang_xlm_roberta": 0.9962188601493835,
    "cross_lang_labse": 0.9572815895080566,
    "cross_lang_mbert": 0.9529154300689697,
    "cross_lang_comet_qe": 0.42210739850997925,
    "backtrans_bleu": 72.43549042730262,
    "backtrans_chrf": 86.53039297718558,
    "backtrans_bertscore": 0.9679195284843445,
    "prof_backtrans_bleu": 56.61325737688595,
    "prof_backtrans_chrf": 76.30373342778398,
    "prof_backtrans_bertscore": 0.949525773525238,
    "prof_backtrans_labse": 0.9700663089752197,
    "prof_backtrans_xlm_roberta": 0.9513947367668152,
    "llm_vs_prof_backtrans_bleu": 61.64956847946905,
    "llm_vs_prof_backtrans_chrf": 75.67900185729438,
    "llm_vs_prof_backtrans_bertscore": 0.9547924995422363,
    "llm_vs_prof_backtrans_labse": 0.9440511465072632
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 23.991249032208657,
    "same_lang_chrf": 46.718653182386404,
    "same_lang_bertscore": 0.8907959461212158,
    "same_lang_comet": 0.919894814491272,
    "cross_lang_xlm_roberta": 0.9929925203323364,
    "cross_lang_labse": 0.9946948289871216,
    "cross_lang_mbert": 0.9939472079277039,
    "cross_lang_comet_qe": 0.3776690661907196,
    "backtrans_bleu": 55.90905921679754,
    "backtrans_chrf": 77.13168222283443,
    "backtrans_bertscore": 0.9674170613288879,
    "prof_backtrans_bleu": 40.14797032730509,
    "prof_backtrans_chrf": 68.43304169749814,
    "prof_backtrans_bertscore": 0.9392812848091125,
    "prof_backtrans_labse": 0.9690389037132263,
    "prof_backtrans_xlm_roberta": 0.8958508372306824,
    "llm_vs_prof_backtrans_bleu": 49.31204744914784,
    "llm_vs_prof_backtrans_chrf": 71.4705637914474,
    "llm_vs_prof_backtrans_bertscore": 0.9481868147850037,
    "llm_vs_prof_backtrans_labse": 0.9692710638046265
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 54.131418685216886,
    "same_lang_chrf": 68.46980573600393,
    "same_lang_bertscore": 0.8855558633804321,
    "same_lang_comet": 0.9230377674102783,
    "cross_lang_xlm_roberta": 0.9921644926071167,
    "cross_lang_labse": 0.9911732077598572,
    "cross_lang_mbert": 0.9524017572402954,
    "cross_lang_comet_qe": 0.3532479703426361,
    "backtrans_bleu": 57.89128095499469,
    "backtrans_chrf": 76.22643515100064,
    "backtrans_bertscore": 0.9622182846069336,
    "prof_backtrans_bleu": 44.56331743642633,
    "prof_backtrans_chrf": 70.10193447214277,
    "prof_backtrans_bertscore": 0.9447495937347412,
    "prof_backtrans_labse": 0.9667708277702332,
    "prof_backtrans_xlm_roberta": 0.9357303380966187,
    "llm_vs_prof_backtrans_bleu": 52.79567441495229,
    "llm_vs_prof_backtrans_chrf": 71.42318352745069,
    "llm_vs_prof_backtrans_bertscore": 0.9526264071464539,
    "llm_vs_prof_backtrans_labse": 0.9654983878135681
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 46.99124799308129,
    "same_lang_chrf": 74.045935170062,
    "same_lang_bertscore": 0.8785510063171387,
    "same_lang_comet": 0.9190143346786499,
    "cross_lang_xlm_roberta": 0.986868143081665,
    "cross_lang_labse": 0.9683071970939636,
    "cross_lang_mbert": 0.9401495456695557,
    "cross_lang_comet_qe": 0.3171294033527374,
    "backtrans_bleu": 61.59440700080451,
    "backtrans_chrf": 78.93795053976993,
    "backtrans_bertscore": 0.9601884484291077,
    "prof_backtrans_bleu": 64.82056602879051,
    "prof_backtrans_chrf": 81.5720433468173,
    "prof_backtrans_bertscore": 0.9591332674026489,
    "prof_backtrans_labse": 0.9735491871833801,
    "prof_backtrans_xlm_roberta": 0.9450095891952515,
    "llm_vs_prof_backtrans_bleu": 59.6726661216029,
    "llm_vs_prof_backtrans_chrf": 75.30961860179328,
    "llm_vs_prof_backtrans_bertscore": 0.9607874751091003,
    "llm_vs_prof_backtrans_labse": 0.9628693461418152
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 35.68677160105694,
    "same_lang_chrf": 66.170531804459,
    "same_lang_bertscore": 0.8769192695617676,
    "same_lang_comet": 0.9297102689743042,
    "cross_lang_xlm_roberta": 0.9832607507705688,
    "cross_lang_labse": 0.9648665189743042,
    "cross_lang_mbert": 0.896026074886322,
    "cross_lang_comet_qe": 0.34761708974838257,
    "backtrans_bleu": 45.93197158908729,
    "backtrans_chrf": 68.08788335324222,
    "backtrans_bertscore": 0.9401048421859741,
    "prof_backtrans_bleu": 26.161336795798892,
    "prof_backtrans_chrf": 59.354560530463054,
    "prof_backtrans_bertscore": 0.9096835851669312,
    "prof_backtrans_labse": 0.9168218970298767,
    "prof_backtrans_xlm_roberta": 0.9495586156845093,
    "llm_vs_prof_backtrans_bleu": 38.49869667084898,
    "llm_vs_prof_backtrans_chrf": 63.83588924435851,
    "llm_vs_prof_backtrans_bertscore": 0.9308825731277466,
    "llm_vs_prof_backtrans_labse": 0.9271700382232666
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 19.41587345371784,
    "same_lang_chrf": 38.107868006600796,
    "same_lang_bertscore": 0.8173496127128601,
    "same_lang_comet": 0.9204882979393005,
    "cross_lang_xlm_roberta": 0.9676658511161804,
    "cross_lang_labse": 0.9657019972801208,
    "cross_lang_mbert": 0.9136514067649841,
    "cross_lang_comet_qe": 0.3299546241760254,
    "backtrans_bleu": 41.81390978703163,
    "backtrans_chrf": 67.63920872324304,
    "backtrans_bertscore": 0.9432305693626404,
    "prof_backtrans_bleu": 43.53144782845874,
    "prof_backtrans_chrf": 70.19011954331236,
    "prof_backtrans_bertscore": 0.9322322010993958,
    "prof_backtrans_labse": 0.9643847942352295,
    "prof_backtrans_xlm_roberta": 0.8928945660591125,
    "llm_vs_prof_backtrans_bleu": 37.804545912225684,
    "llm_vs_prof_backtrans_chrf": 63.59909586033384,
    "llm_vs_prof_backtrans_bertscore": 0.9210443496704102,
    "llm_vs_prof_backtrans_labse": 0.950240433216095
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 44.00188599182859,
    "same_lang_chrf": 64.85104804342228,
    "same_lang_bertscore": 0.8483108878135681,
    "same_lang_comet": 0.7974299192428589,
    "cross_lang_xlm_roberta": 0.9948915839195251,
    "cross_lang_labse": 0.9675295948982239,
    "cross_lang_mbert": 0.9888990521430969,
    "cross_lang_comet_qe": 0.24025702476501465,
    "backtrans_bleu": 55.94692705835403,
    "backtrans_chrf": 77.39940815916457,
    "backtrans_bertscore": 0.945879340171814,
    "prof_backtrans_bleu": 46.90719284628843,
    "prof_backtrans_chrf": 72.9606046912293,
    "prof_backtrans_bertscore": 0.9248042702674866,
    "prof_backtrans_labse": 0.951613187789917,
    "prof_backtrans_xlm_roberta": 0.9028793573379517,
    "llm_vs_prof_backtrans_bleu": 54.63363073258491,
    "llm_vs_prof_backtrans_chrf": 73.06151842323433,
    "llm_vs_prof_backtrans_bertscore": 0.940626323223114,
    "llm_vs_prof_backtrans_labse": 0.9657246470451355
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 52.883227818822554,
    "same_lang_chrf": 73.72375681849128,
    "same_lang_bertscore": 0.8971837162971497,
    "same_lang_comet": 0.9033702611923218,
    "cross_lang_xlm_roberta": 0.995717465877533,
    "cross_lang_labse": 0.9911214113235474,
    "cross_lang_mbert": 0.9401182532310486,
    "cross_lang_comet_qe": 0.2801568806171417,
    "backtrans_bleu": 61.27360763672302,
    "backtrans_chrf": 78.23546177066201,
    "backtrans_bertscore": 0.9635732173919678,
    "prof_backtrans_bleu": 44.73029144611179,
    "prof_backtrans_chrf": 69.21228841841773,
    "prof_backtrans_bertscore": 0.9339303374290466,
    "prof_backtrans_labse": 0.9682670831680298,
    "prof_backtrans_xlm_roberta": 0.9513693451881409,
    "llm_vs_prof_backtrans_bleu": 56.3349115342468,
    "llm_vs_prof_backtrans_chrf": 72.71212804366276,
    "llm_vs_prof_backtrans_bertscore": 0.9512929916381836,
    "llm_vs_prof_backtrans_labse": 0.9690670967102051
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 21.239726599231602,
    "same_lang_chrf": 45.70310694439261,
    "same_lang_bertscore": 0.8934333920478821,
    "same_lang_comet": 0.9199308156967163,
    "cross_lang_xlm_roberta": 0.9934734106063843,
    "cross_lang_labse": 0.9928690791130066,
    "cross_lang_mbert": 0.9903751611709595,
    "cross_lang_comet_qe": 0.40188878774642944,
    "backtrans_bleu": 54.94469938590032,
    "backtrans_chrf": 75.69551155840217,
    "backtrans_bertscore": 0.9671381115913391,
    "prof_backtrans_bleu": 32.731250411813754,
    "prof_backtrans_chrf": 64.95614354576342,
    "prof_backtrans_bertscore": 0.940937340259552,
    "prof_backtrans_labse": 0.9730221629142761,
    "prof_backtrans_xlm_roberta": 0.9012677669525146,
    "llm_vs_prof_backtrans_bleu": 40.57677957889056,
    "llm_vs_prof_backtrans_chrf": 68.44099396696029,
    "llm_vs_prof_backtrans_bertscore": 0.9424983859062195,
    "llm_vs_prof_backtrans_labse": 0.9714546799659729
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 47.06502023633517,
    "same_lang_chrf": 64.37386529914936,
    "same_lang_bertscore": 0.8655328750610352,
    "same_lang_comet": 0.9094784259796143,
    "cross_lang_xlm_roberta": 0.9906614422798157,
    "cross_lang_labse": 0.9794125556945801,
    "cross_lang_mbert": 0.9442856311798096,
    "cross_lang_comet_qe": 0.008465230464935303,
    "backtrans_bleu": 36.26651529272925,
    "backtrans_chrf": 65.05913448289247,
    "backtrans_bertscore": 0.939288854598999,
    "prof_backtrans_bleu": 32.82667522994051,
    "prof_backtrans_chrf": 63.25858992408948,
    "prof_backtrans_bertscore": 0.9408086538314819,
    "prof_backtrans_labse": 0.968012809753418,
    "prof_backtrans_xlm_roberta": 0.9355682730674744,
    "llm_vs_prof_backtrans_bleu": 43.08572581281872,
    "llm_vs_prof_backtrans_chrf": 67.07011951901572,
    "llm_vs_prof_backtrans_bertscore": 0.9423388838768005,
    "llm_vs_prof_backtrans_labse": 0.9637865424156189
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 46.00072289940186,
    "same_lang_chrf": 73.65033077134821,
    "same_lang_bertscore": 0.8639255166053772,
    "same_lang_comet": 0.9119895696640015,
    "cross_lang_xlm_roberta": 0.9855785369873047,
    "cross_lang_labse": 0.9943010807037354,
    "cross_lang_mbert": 0.9356569051742554,
    "cross_lang_comet_qe": 0.31387990713119507,
    "backtrans_bleu": 65.51346003817069,
    "backtrans_chrf": 79.10406678449155,
    "backtrans_bertscore": 0.9768635630607605,
    "prof_backtrans_bleu": 59.93495354450471,
    "prof_backtrans_chrf": 77.113209604842,
    "prof_backtrans_bertscore": 0.9483987092971802,
    "prof_backtrans_labse": 0.9677417278289795,
    "prof_backtrans_xlm_roberta": 0.9404168725013733,
    "llm_vs_prof_backtrans_bleu": 64.26874379063541,
    "llm_vs_prof_backtrans_chrf": 79.05510262843204,
    "llm_vs_prof_backtrans_bertscore": 0.9508108496665955,
    "llm_vs_prof_backtrans_labse": 0.9697701930999756
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 33.697213948636254,
    "same_lang_chrf": 66.21791596214959,
    "same_lang_bertscore": 0.8784351944923401,
    "same_lang_comet": 0.9277280569076538,
    "cross_lang_xlm_roberta": 0.9878109097480774,
    "cross_lang_labse": 0.9776453375816345,
    "cross_lang_mbert": 0.9285447597503662,
    "cross_lang_comet_qe": 0.3786922097206116,
    "backtrans_bleu": 41.103542928947924,
    "backtrans_chrf": 66.64054093436695,
    "backtrans_bertscore": 0.9404056668281555,
    "prof_backtrans_bleu": 26.044787537687693,
    "prof_backtrans_chrf": 58.339190688545074,
    "prof_backtrans_bertscore": 0.914514422416687,
    "prof_backtrans_labse": 0.9585504531860352,
    "prof_backtrans_xlm_roberta": 0.9529908299446106,
    "llm_vs_prof_backtrans_bleu": 46.129677815294585,
    "llm_vs_prof_backtrans_chrf": 69.90072711224728,
    "llm_vs_prof_backtrans_bertscore": 0.9355648756027222,
    "llm_vs_prof_backtrans_labse": 0.9698906540870667
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 23.71816931785653,
    "same_lang_chrf": 38.910350571575634,
    "same_lang_bertscore": 0.832610547542572,
    "same_lang_comet": 0.919252872467041,
    "cross_lang_xlm_roberta": 0.9821666479110718,
    "cross_lang_labse": 0.9715292453765869,
    "cross_lang_mbert": 0.9297011494636536,
    "cross_lang_comet_qe": 0.30880674719810486,
    "backtrans_bleu": 43.162805187788,
    "backtrans_chrf": 68.53743919130392,
    "backtrans_bertscore": 0.9457262754440308,
    "prof_backtrans_bleu": 58.64452156936045,
    "prof_backtrans_chrf": 79.21916046022017,
    "prof_backtrans_bertscore": 0.9675534963607788,
    "prof_backtrans_labse": 0.9779396057128906,
    "prof_backtrans_xlm_roberta": 0.8965856432914734,
    "llm_vs_prof_backtrans_bleu": 43.78000713959235,
    "llm_vs_prof_backtrans_chrf": 67.6006300870993,
    "llm_vs_prof_backtrans_bertscore": 0.9462205767631531,
    "llm_vs_prof_backtrans_labse": 0.9738463759422302
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 32.16971914522641,
    "same_lang_chrf": 57.60314672224435,
    "same_lang_bertscore": 0.8284762501716614,
    "same_lang_comet": 0.8111257553100586,
    "cross_lang_xlm_roberta": 0.9957836270332336,
    "cross_lang_labse": 0.9871196746826172,
    "cross_lang_mbert": 0.9424930810928345,
    "cross_lang_comet_qe": 0.31030505895614624,
    "backtrans_bleu": 61.775708283792135,
    "backtrans_chrf": 79.93041243269408,
    "backtrans_bertscore": 0.9671905636787415,
    "prof_backtrans_bleu": 41.106223025697915,
    "prof_backtrans_chrf": 69.31311846362851,
    "prof_backtrans_bertscore": 0.9181822538375854,
    "prof_backtrans_labse": 0.950736403465271,
    "prof_backtrans_xlm_roberta": 0.8995533585548401,
    "llm_vs_prof_backtrans_bleu": 46.94245833361284,
    "llm_vs_prof_backtrans_chrf": 67.84961004487944,
    "llm_vs_prof_backtrans_bertscore": 0.924715518951416,
    "llm_vs_prof_backtrans_labse": 0.952634334564209
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 52.69649427988808,
    "same_lang_chrf": 76.16573946922725,
    "same_lang_bertscore": 0.9005727171897888,
    "same_lang_comet": 0.9064909219741821,
    "cross_lang_xlm_roberta": 0.9949474930763245,
    "cross_lang_labse": 0.9951116442680359,
    "cross_lang_mbert": 0.9934977889060974,
    "cross_lang_comet_qe": 0.5274442434310913,
    "backtrans_bleu": 74.6555479851576,
    "backtrans_chrf": 87.41170030146664,
    "backtrans_bertscore": 0.9807586669921875,
    "prof_backtrans_bleu": 53.93645869677247,
    "prof_backtrans_chrf": 75.72935545365166,
    "prof_backtrans_bertscore": 0.9543161392211914,
    "prof_backtrans_labse": 0.9551346898078918,
    "prof_backtrans_xlm_roberta": 0.9273446798324585,
    "llm_vs_prof_backtrans_bleu": 62.09849564713457,
    "llm_vs_prof_backtrans_chrf": 76.90064171852042,
    "llm_vs_prof_backtrans_bertscore": 0.9571410417556763,
    "llm_vs_prof_backtrans_labse": 0.951342761516571
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 20.01872542885634,
    "same_lang_chrf": 42.572701037953266,
    "same_lang_bertscore": 0.872837483882904,
    "same_lang_comet": 0.9273629784584045,
    "cross_lang_xlm_roberta": 0.9865358471870422,
    "cross_lang_labse": 0.9863167405128479,
    "cross_lang_mbert": 0.9824026823043823,
    "cross_lang_comet_qe": 0.5070494413375854,
    "backtrans_bleu": 59.215700757952824,
    "backtrans_chrf": 78.59065700751427,
    "backtrans_bertscore": 0.9615070819854736,
    "prof_backtrans_bleu": 45.77335478095729,
    "prof_backtrans_chrf": 70.82546547136886,
    "prof_backtrans_bertscore": 0.9423955082893372,
    "prof_backtrans_labse": 0.9533029198646545,
    "prof_backtrans_xlm_roberta": 0.8792503476142883,
    "llm_vs_prof_backtrans_bleu": 51.0309450316248,
    "llm_vs_prof_backtrans_chrf": 70.38847168393163,
    "llm_vs_prof_backtrans_bertscore": 0.9436765313148499,
    "llm_vs_prof_backtrans_labse": 0.9574868083000183
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 51.47859930544604,
    "same_lang_chrf": 69.00059701921997,
    "same_lang_bertscore": 0.8789292573928833,
    "same_lang_comet": 0.9298409223556519,
    "cross_lang_xlm_roberta": 0.9760457873344421,
    "cross_lang_labse": 0.9725104570388794,
    "cross_lang_mbert": 0.9715457558631897,
    "cross_lang_comet_qe": 0.5350704193115234,
    "backtrans_bleu": 57.54355936318109,
    "backtrans_chrf": 77.71884161863049,
    "backtrans_bertscore": 0.9570955634117126,
    "prof_backtrans_bleu": 54.78533329189007,
    "prof_backtrans_chrf": 76.85650182703439,
    "prof_backtrans_bertscore": 0.928342878818512,
    "prof_backtrans_labse": 0.9530588388442993,
    "prof_backtrans_xlm_roberta": 0.9102068543434143,
    "llm_vs_prof_backtrans_bleu": 65.05564369567999,
    "llm_vs_prof_backtrans_chrf": 81.11437622824896,
    "llm_vs_prof_backtrans_bertscore": 0.9271471500396729,
    "llm_vs_prof_backtrans_labse": 0.9685308933258057
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 44.52420793302735,
    "same_lang_chrf": 72.58049528652909,
    "same_lang_bertscore": 0.8606835007667542,
    "same_lang_comet": 0.9182738065719604,
    "cross_lang_xlm_roberta": 0.983366072177887,
    "cross_lang_labse": 0.992064893245697,
    "cross_lang_mbert": 0.9858235716819763,
    "cross_lang_comet_qe": 0.5089073181152344,
    "backtrans_bleu": 70.96301038130517,
    "backtrans_chrf": 84.03572400300908,
    "backtrans_bertscore": 0.9733286499977112,
    "prof_backtrans_bleu": 65.98145089528622,
    "prof_backtrans_chrf": 81.55200053404475,
    "prof_backtrans_bertscore": 0.931675136089325,
    "prof_backtrans_labse": 0.9706304669380188,
    "prof_backtrans_xlm_roberta": 0.9429919123649597,
    "llm_vs_prof_backtrans_bleu": 65.09865272217992,
    "llm_vs_prof_backtrans_chrf": 78.78755246069372,
    "llm_vs_prof_backtrans_bertscore": 0.9370841383934021,
    "llm_vs_prof_backtrans_labse": 0.9732975959777832
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 35.99141604300015,
    "same_lang_chrf": 70.27222845873297,
    "same_lang_bertscore": 0.8885335922241211,
    "same_lang_comet": 0.9421094059944153,
    "cross_lang_xlm_roberta": 0.9715519547462463,
    "cross_lang_labse": 0.9773158431053162,
    "cross_lang_mbert": 0.9791282415390015,
    "cross_lang_comet_qe": 0.5517958998680115,
    "backtrans_bleu": 65.09898637677246,
    "backtrans_chrf": 81.95187820424395,
    "backtrans_bertscore": 0.9653028249740601,
    "prof_backtrans_bleu": 39.99438173036604,
    "prof_backtrans_chrf": 68.37195875723867,
    "prof_backtrans_bertscore": 0.9327164888381958,
    "prof_backtrans_labse": 0.9515329003334045,
    "prof_backtrans_xlm_roberta": 0.9405941963195801,
    "llm_vs_prof_backtrans_bleu": 48.22676302331409,
    "llm_vs_prof_backtrans_chrf": 74.64913861919607,
    "llm_vs_prof_backtrans_bertscore": 0.9472915530204773,
    "llm_vs_prof_backtrans_labse": 0.9591906070709229
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 18.265887796607664,
    "same_lang_chrf": 39.14879506799444,
    "same_lang_bertscore": 0.8472979068756104,
    "same_lang_comet": 0.9270963072776794,
    "cross_lang_xlm_roberta": 0.961153507232666,
    "cross_lang_labse": 0.970498263835907,
    "cross_lang_mbert": 0.9756612181663513,
    "cross_lang_comet_qe": 0.5483356714248657,
    "backtrans_bleu": 50.02628370293887,
    "backtrans_chrf": 74.9774064811269,
    "backtrans_bertscore": 0.9525710940361023,
    "prof_backtrans_bleu": 48.56067007528459,
    "prof_backtrans_chrf": 73.73892351566231,
    "prof_backtrans_bertscore": 0.924659788608551,
    "prof_backtrans_labse": 0.9605534076690674,
    "prof_backtrans_xlm_roberta": 0.8689131736755371,
    "llm_vs_prof_backtrans_bleu": 50.62631168715009,
    "llm_vs_prof_backtrans_chrf": 73.59605153265376,
    "llm_vs_prof_backtrans_bertscore": 0.9189952611923218,
    "llm_vs_prof_backtrans_labse": 0.9684797525405884
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 43.092047653241515,
    "same_lang_chrf": 65.73819659789018,
    "same_lang_bertscore": 0.8371856808662415,
    "same_lang_comet": 0.8287665843963623,
    "cross_lang_xlm_roberta": 0.9967990517616272,
    "cross_lang_labse": 0.9906097054481506,
    "cross_lang_mbert": 0.99566650390625,
    "cross_lang_comet_qe": 0.5004894137382507,
    "backtrans_bleu": 68.31912842133542,
    "backtrans_chrf": 85.02047717197958,
    "backtrans_bertscore": 0.9732397198677063,
    "prof_backtrans_bleu": 49.46869780410016,
    "prof_backtrans_chrf": 75.24580537405454,
    "prof_backtrans_bertscore": 0.9338803887367249,
    "prof_backtrans_labse": 0.9578542113304138,
    "prof_backtrans_xlm_roberta": 0.8904459476470947,
    "llm_vs_prof_backtrans_bleu": 56.902414107991,
    "llm_vs_prof_backtrans_chrf": 74.33143964360127,
    "llm_vs_prof_backtrans_bertscore": 0.9399086833000183,
    "llm_vs_prof_backtrans_labse": 0.9569876194000244
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 52.47321552876004,
    "same_lang_chrf": 76.88224002516297,
    "same_lang_bertscore": 0.8950626850128174,
    "same_lang_comet": 0.9043629765510559,
    "cross_lang_xlm_roberta": 0.9909471869468689,
    "cross_lang_labse": 0.9960160851478577,
    "cross_lang_mbert": 0.9900265336036682,
    "cross_lang_comet_qe": 0.5304744243621826,
    "backtrans_bleu": 76.77178204748064,
    "backtrans_chrf": 87.54836637546622,
    "backtrans_bertscore": 0.9826645255088806,
    "prof_backtrans_bleu": 53.517474939579884,
    "prof_backtrans_chrf": 74.88835732534513,
    "prof_backtrans_bertscore": 0.9517108798027039,
    "prof_backtrans_labse": 0.9559492468833923,
    "prof_backtrans_xlm_roberta": 0.9263926148414612,
    "llm_vs_prof_backtrans_bleu": 60.85318429918509,
    "llm_vs_prof_backtrans_chrf": 76.31269327995994,
    "llm_vs_prof_backtrans_bertscore": 0.9552940726280212,
    "llm_vs_prof_backtrans_labse": 0.9541856050491333
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 16.987803281460987,
    "same_lang_chrf": 43.34828822508807,
    "same_lang_bertscore": 0.8894899487495422,
    "same_lang_comet": 0.9343024492263794,
    "cross_lang_xlm_roberta": 0.9670941829681396,
    "cross_lang_labse": 0.9656296372413635,
    "cross_lang_mbert": 0.9049910306930542,
    "cross_lang_comet_qe": 0.5151766538619995,
    "backtrans_bleu": 50.247878622744224,
    "backtrans_chrf": 73.10961341157706,
    "backtrans_bertscore": 0.9431246519088745,
    "prof_backtrans_bleu": 46.58742270258102,
    "prof_backtrans_chrf": 70.02812540933478,
    "prof_backtrans_bertscore": 0.9353120923042297,
    "prof_backtrans_labse": 0.9577019810676575,
    "prof_backtrans_xlm_roberta": 0.8752906918525696,
    "llm_vs_prof_backtrans_bleu": 56.13520248017789,
    "llm_vs_prof_backtrans_chrf": 76.51888194269286,
    "llm_vs_prof_backtrans_bertscore": 0.9561899900436401,
    "llm_vs_prof_backtrans_labse": 0.9660894870758057
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 58.271137675714144,
    "same_lang_chrf": 72.60176877612679,
    "same_lang_bertscore": 0.8895479440689087,
    "same_lang_comet": 0.9330310821533203,
    "cross_lang_xlm_roberta": 0.97362220287323,
    "cross_lang_labse": 0.9679272770881653,
    "cross_lang_mbert": 0.9684827327728271,
    "cross_lang_comet_qe": 0.5403932332992554,
    "backtrans_bleu": 61.77719076884706,
    "backtrans_chrf": 78.7259544931162,
    "backtrans_bertscore": 0.9588847756385803,
    "prof_backtrans_bleu": 53.42887931161223,
    "prof_backtrans_chrf": 74.74742313904184,
    "prof_backtrans_bertscore": 0.9513733386993408,
    "prof_backtrans_labse": 0.957923173904419,
    "prof_backtrans_xlm_roberta": 0.9098634123802185,
    "llm_vs_prof_backtrans_bleu": 65.69961394318166,
    "llm_vs_prof_backtrans_chrf": 81.38522587278256,
    "llm_vs_prof_backtrans_bertscore": 0.9611754417419434,
    "llm_vs_prof_backtrans_labse": 0.973676860332489
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 47.506810970644,
    "same_lang_chrf": 75.57904927823796,
    "same_lang_bertscore": 0.850828230381012,
    "same_lang_comet": 0.920923113822937,
    "cross_lang_xlm_roberta": 0.9916328191757202,
    "cross_lang_labse": 0.9928556084632874,
    "cross_lang_mbert": 0.9894364476203918,
    "cross_lang_comet_qe": 0.5345010161399841,
    "backtrans_bleu": 79.27608717232914,
    "backtrans_chrf": 89.03581727091117,
    "backtrans_bertscore": 0.9802216291427612,
    "prof_backtrans_bleu": 63.15607191076351,
    "prof_backtrans_chrf": 79.6282183761945,
    "prof_backtrans_bertscore": 0.960966944694519,
    "prof_backtrans_labse": 0.9687156677246094,
    "prof_backtrans_xlm_roberta": 0.9461216330528259,
    "llm_vs_prof_backtrans_bleu": 69.6258133564956,
    "llm_vs_prof_backtrans_chrf": 81.22396123660725,
    "llm_vs_prof_backtrans_bertscore": 0.965391993522644,
    "llm_vs_prof_backtrans_labse": 0.9678972959518433
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 40.759964486339506,
    "same_lang_chrf": 71.88902274551879,
    "same_lang_bertscore": 0.891172468662262,
    "same_lang_comet": 0.945010781288147,
    "cross_lang_xlm_roberta": 0.9705529808998108,
    "cross_lang_labse": 0.9746898412704468,
    "cross_lang_mbert": 0.9814959764480591,
    "cross_lang_comet_qe": 0.5500063896179199,
    "backtrans_bleu": 62.48427156299708,
    "backtrans_chrf": 78.70039436600426,
    "backtrans_bertscore": 0.9594762921333313,
    "prof_backtrans_bleu": 39.79014415840788,
    "prof_backtrans_chrf": 68.45178338249279,
    "prof_backtrans_bertscore": 0.9339115023612976,
    "prof_backtrans_labse": 0.9481611251831055,
    "prof_backtrans_xlm_roberta": 0.9385650157928467,
    "llm_vs_prof_backtrans_bleu": 50.783323033646575,
    "llm_vs_prof_backtrans_chrf": 75.26205605071628,
    "llm_vs_prof_backtrans_bertscore": 0.9555291533470154,
    "llm_vs_prof_backtrans_labse": 0.9761936664581299
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 25.73423673445164,
    "same_lang_chrf": 46.004375732789036,
    "same_lang_bertscore": 0.8672245144844055,
    "same_lang_comet": 0.9422035217285156,
    "cross_lang_xlm_roberta": 0.9638126492500305,
    "cross_lang_labse": 0.9634925127029419,
    "cross_lang_mbert": 0.9148010611534119,
    "cross_lang_comet_qe": 0.5501987338066101,
    "backtrans_bleu": 54.22649627629031,
    "backtrans_chrf": 76.33897569106202,
    "backtrans_bertscore": 0.9571520090103149,
    "prof_backtrans_bleu": 48.06807502582823,
    "prof_backtrans_chrf": 72.84970858004134,
    "prof_backtrans_bertscore": 0.9498605132102966,
    "prof_backtrans_labse": 0.9654369354248047,
    "prof_backtrans_xlm_roberta": 0.8811506032943726,
    "llm_vs_prof_backtrans_bleu": 59.89531266496901,
    "llm_vs_prof_backtrans_chrf": 77.7113079969451,
    "llm_vs_prof_backtrans_bertscore": 0.9599781632423401,
    "llm_vs_prof_backtrans_labse": 0.9732723236083984
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 45.39636933412627,
    "same_lang_chrf": 69.45946452378713,
    "same_lang_bertscore": 0.8602625727653503,
    "same_lang_comet": 0.8360170125961304,
    "cross_lang_xlm_roberta": 0.9854069352149963,
    "cross_lang_labse": 0.9941169023513794,
    "cross_lang_mbert": 0.9880192875862122,
    "cross_lang_comet_qe": 0.5273659229278564,
    "backtrans_bleu": 77.64336792126547,
    "backtrans_chrf": 87.66216748260973,
    "backtrans_bertscore": 0.9804611206054688,
    "prof_backtrans_bleu": 49.76955910661406,
    "prof_backtrans_chrf": 74.74360974931984,
    "prof_backtrans_bertscore": 0.941006600856781,
    "prof_backtrans_labse": 0.9583570957183838,
    "prof_backtrans_xlm_roberta": 0.8963434100151062,
    "llm_vs_prof_backtrans_bleu": 52.97211752428541,
    "llm_vs_prof_backtrans_chrf": 72.17421340471894,
    "llm_vs_prof_backtrans_bertscore": 0.9380589127540588,
    "llm_vs_prof_backtrans_labse": 0.9575899839401245
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 51.900343777154326,
    "same_lang_chrf": 76.16304080733602,
    "same_lang_bertscore": 0.892017662525177,
    "same_lang_comet": 0.9072121381759644,
    "cross_lang_xlm_roberta": 0.9905410408973694,
    "cross_lang_labse": 0.9935930371284485,
    "cross_lang_mbert": 0.9887935519218445,
    "cross_lang_comet_qe": 0.5202653408050537,
    "backtrans_bleu": 68.14640236835208,
    "backtrans_chrf": 81.82030469024286,
    "backtrans_bertscore": 0.976931095123291,
    "prof_backtrans_bleu": 48.24947627138426,
    "prof_backtrans_chrf": 71.96418514687849,
    "prof_backtrans_bertscore": 0.9520326256752014,
    "prof_backtrans_labse": 0.9548142552375793,
    "prof_backtrans_xlm_roberta": 0.9285202026367188,
    "llm_vs_prof_backtrans_bleu": 57.35440640601059,
    "llm_vs_prof_backtrans_chrf": 74.21583510363952,
    "llm_vs_prof_backtrans_bertscore": 0.9588266015052795,
    "llm_vs_prof_backtrans_labse": 0.9536646008491516
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 27.071063771240738,
    "same_lang_chrf": 47.03955335842939,
    "same_lang_bertscore": 0.8911135196685791,
    "same_lang_comet": 0.9358102083206177,
    "cross_lang_xlm_roberta": 0.9727070331573486,
    "cross_lang_labse": 0.9748191833496094,
    "cross_lang_mbert": 0.9581872224807739,
    "cross_lang_comet_qe": 0.5394515991210938,
    "backtrans_bleu": 53.2655198797229,
    "backtrans_chrf": 74.65282987594412,
    "backtrans_bertscore": 0.9476117491722107,
    "prof_backtrans_bleu": 41.244597472695034,
    "prof_backtrans_chrf": 67.78186760526498,
    "prof_backtrans_bertscore": 0.9292431473731995,
    "prof_backtrans_labse": 0.9551166296005249,
    "prof_backtrans_xlm_roberta": 0.872560977935791,
    "llm_vs_prof_backtrans_bleu": 52.161396038909956,
    "llm_vs_prof_backtrans_chrf": 71.63611889314694,
    "llm_vs_prof_backtrans_bertscore": 0.948961079120636,
    "llm_vs_prof_backtrans_labse": 0.9623501896858215
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 61.806310641267466,
    "same_lang_chrf": 75.03562667139924,
    "same_lang_bertscore": 0.8917134404182434,
    "same_lang_comet": 0.9321953058242798,
    "cross_lang_xlm_roberta": 0.9692338705062866,
    "cross_lang_labse": 0.9690479040145874,
    "cross_lang_mbert": 0.9503800272941589,
    "cross_lang_comet_qe": 0.550208568572998,
    "backtrans_bleu": 60.72287950900319,
    "backtrans_chrf": 79.35581434675602,
    "backtrans_bertscore": 0.9549281001091003,
    "prof_backtrans_bleu": 43.37158646181611,
    "prof_backtrans_chrf": 71.35270437314264,
    "prof_backtrans_bertscore": 0.9485913515090942,
    "prof_backtrans_labse": 0.9493256211280823,
    "prof_backtrans_xlm_roberta": 0.9083304405212402,
    "llm_vs_prof_backtrans_bleu": 49.84616754651232,
    "llm_vs_prof_backtrans_chrf": 71.72866640364393,
    "llm_vs_prof_backtrans_bertscore": 0.9447089433670044,
    "llm_vs_prof_backtrans_labse": 0.9680600166320801
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 55.10902265389078,
    "same_lang_chrf": 79.87121221826239,
    "same_lang_bertscore": 0.8746570348739624,
    "same_lang_comet": 0.9289513826370239,
    "cross_lang_xlm_roberta": 0.9893524646759033,
    "cross_lang_labse": 0.9937455058097839,
    "cross_lang_mbert": 0.9919582009315491,
    "cross_lang_comet_qe": 0.5321022272109985,
    "backtrans_bleu": 75.71903740675734,
    "backtrans_chrf": 86.57184213931444,
    "backtrans_bertscore": 0.977074921131134,
    "prof_backtrans_bleu": 61.66631956124583,
    "prof_backtrans_chrf": 79.169724262283,
    "prof_backtrans_bertscore": 0.9594112038612366,
    "prof_backtrans_labse": 0.9709452390670776,
    "prof_backtrans_xlm_roberta": 0.9476479291915894,
    "llm_vs_prof_backtrans_bleu": 65.27546232739874,
    "llm_vs_prof_backtrans_chrf": 79.38829514734378,
    "llm_vs_prof_backtrans_bertscore": 0.9580064415931702,
    "llm_vs_prof_backtrans_labse": 0.9684631824493408
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 36.316558799253286,
    "same_lang_chrf": 71.75617805818231,
    "same_lang_bertscore": 0.8874077200889587,
    "same_lang_comet": 0.9454880952835083,
    "cross_lang_xlm_roberta": 0.9600296020507812,
    "cross_lang_labse": 0.8964887857437134,
    "cross_lang_mbert": 0.931920051574707,
    "cross_lang_comet_qe": 0.563000500202179,
    "backtrans_bleu": 58.17417610370777,
    "backtrans_chrf": 76.35030263265236,
    "backtrans_bertscore": 0.945539116859436,
    "prof_backtrans_bleu": 36.011864656911044,
    "prof_backtrans_chrf": 65.55099976332134,
    "prof_backtrans_bertscore": 0.9298215508460999,
    "prof_backtrans_labse": 0.9452366828918457,
    "prof_backtrans_xlm_roberta": 0.9379936456680298,
    "llm_vs_prof_backtrans_bleu": 46.29628121528734,
    "llm_vs_prof_backtrans_chrf": 71.5151807850687,
    "llm_vs_prof_backtrans_bertscore": 0.9382275342941284,
    "llm_vs_prof_backtrans_labse": 0.903914749622345
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 17.798738015454457,
    "same_lang_chrf": 38.10993455438144,
    "same_lang_bertscore": 0.8507187962532043,
    "same_lang_comet": 0.9309545159339905,
    "cross_lang_xlm_roberta": 0.9519495964050293,
    "cross_lang_labse": 0.934837281703949,
    "cross_lang_mbert": 0.8647580146789551,
    "cross_lang_comet_qe": 0.4831639528274536,
    "backtrans_bleu": 36.827215420192566,
    "backtrans_chrf": 64.43610001319232,
    "backtrans_bertscore": 0.9244992136955261,
    "prof_backtrans_bleu": 37.87769889496832,
    "prof_backtrans_chrf": 69.14193745839201,
    "prof_backtrans_bertscore": 0.9375813603401184,
    "prof_backtrans_labse": 0.954771101474762,
    "prof_backtrans_xlm_roberta": 0.8654612898826599,
    "llm_vs_prof_backtrans_bleu": 36.42098258435769,
    "llm_vs_prof_backtrans_chrf": 63.78512269665073,
    "llm_vs_prof_backtrans_bertscore": 0.9230844378471375,
    "llm_vs_prof_backtrans_labse": 0.9555960893630981
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 38.20649379991168,
    "same_lang_chrf": 63.36009740478602,
    "same_lang_bertscore": 0.8467015027999878,
    "same_lang_comet": 0.8256109952926636,
    "cross_lang_xlm_roberta": 0.9865713119506836,
    "cross_lang_labse": 0.9840525388717651,
    "cross_lang_mbert": 0.9850960969924927,
    "cross_lang_comet_qe": 0.3671189844608307,
    "backtrans_bleu": 57.855978579379304,
    "backtrans_chrf": 79.36541503580837,
    "backtrans_bertscore": 0.9534332156181335,
    "prof_backtrans_bleu": 39.925287901384415,
    "prof_backtrans_chrf": 70.46644857264471,
    "prof_backtrans_bertscore": 0.9222592115402222,
    "prof_backtrans_labse": 0.9554792642593384,
    "prof_backtrans_xlm_roberta": 0.877506673336029,
    "llm_vs_prof_backtrans_bleu": 48.86386071697914,
    "llm_vs_prof_backtrans_chrf": 69.93387591755432,
    "llm_vs_prof_backtrans_bertscore": 0.9332538843154907,
    "llm_vs_prof_backtrans_labse": 0.9609400033950806
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 52.58050982594309,
    "same_lang_chrf": 76.13767194868335,
    "same_lang_bertscore": 0.8958575129508972,
    "same_lang_comet": 0.906047523021698,
    "cross_lang_xlm_roberta": 0.9897110462188721,
    "cross_lang_labse": 0.9923309683799744,
    "cross_lang_mbert": 0.9886820912361145,
    "cross_lang_comet_qe": 0.5342774391174316,
    "backtrans_bleu": 68.18001709563126,
    "backtrans_chrf": 82.6052815087431,
    "backtrans_bertscore": 0.978629469871521,
    "prof_backtrans_bleu": 43.33103690644797,
    "prof_backtrans_chrf": 69.9393822859419,
    "prof_backtrans_bertscore": 0.9486291408538818,
    "prof_backtrans_labse": 0.9544048309326172,
    "prof_backtrans_xlm_roberta": 0.9265816807746887,
    "llm_vs_prof_backtrans_bleu": 58.38249243052999,
    "llm_vs_prof_backtrans_chrf": 74.46006458859972,
    "llm_vs_prof_backtrans_bertscore": 0.9586082100868225,
    "llm_vs_prof_backtrans_labse": 0.9545873999595642
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 17.36216412014557,
    "same_lang_chrf": 43.45692551139461,
    "same_lang_bertscore": 0.8645651936531067,
    "same_lang_comet": 0.9334888458251953,
    "cross_lang_xlm_roberta": 0.9725338220596313,
    "cross_lang_labse": 0.9674422144889832,
    "cross_lang_mbert": 0.9533600211143494,
    "cross_lang_comet_qe": 0.5486368536949158,
    "backtrans_bleu": 50.560825306774774,
    "backtrans_chrf": 73.15652180823858,
    "backtrans_bertscore": 0.9428511261940002,
    "prof_backtrans_bleu": 42.08687050161152,
    "prof_backtrans_chrf": 68.34014753991254,
    "prof_backtrans_bertscore": 0.9278848171234131,
    "prof_backtrans_labse": 0.9531267285346985,
    "prof_backtrans_xlm_roberta": 0.9498869776725769,
    "llm_vs_prof_backtrans_bleu": 56.23518428003589,
    "llm_vs_prof_backtrans_chrf": 77.1938389830532,
    "llm_vs_prof_backtrans_bertscore": 0.9575524926185608,
    "llm_vs_prof_backtrans_labse": 0.975685179233551
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 53.86149471681484,
    "same_lang_chrf": 69.48940861351419,
    "same_lang_bertscore": 0.8722246885299683,
    "same_lang_comet": 0.9204534888267517,
    "cross_lang_xlm_roberta": 0.9722080230712891,
    "cross_lang_labse": 0.9681816697120667,
    "cross_lang_mbert": 0.8826074600219727,
    "cross_lang_comet_qe": 0.44213324785232544,
    "backtrans_bleu": 54.36299170266685,
    "backtrans_chrf": 74.83128648036418,
    "backtrans_bertscore": 0.9472494125366211,
    "prof_backtrans_bleu": 44.4346198922392,
    "prof_backtrans_chrf": 71.02103033447672,
    "prof_backtrans_bertscore": 0.9416154026985168,
    "prof_backtrans_labse": 0.9515230655670166,
    "prof_backtrans_xlm_roberta": 0.9089403748512268,
    "llm_vs_prof_backtrans_bleu": 58.769053964545265,
    "llm_vs_prof_backtrans_chrf": 73.4926079011746,
    "llm_vs_prof_backtrans_bertscore": 0.9432559013366699,
    "llm_vs_prof_backtrans_labse": 0.9558809399604797
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 47.01700869563968,
    "same_lang_chrf": 74.75817482372081,
    "same_lang_bertscore": 0.8619714379310608,
    "same_lang_comet": 0.9198603630065918,
    "cross_lang_xlm_roberta": 0.9880422949790955,
    "cross_lang_labse": 0.9917740821838379,
    "cross_lang_mbert": 0.9898928999900818,
    "cross_lang_comet_qe": 0.44240128993988037,
    "backtrans_bleu": 64.28038403623519,
    "backtrans_chrf": 82.50992132346488,
    "backtrans_bertscore": 0.9686581492424011,
    "prof_backtrans_bleu": 54.82754696412902,
    "prof_backtrans_chrf": 75.96247622445172,
    "prof_backtrans_bertscore": 0.9519781470298767,
    "prof_backtrans_labse": 0.9695368409156799,
    "prof_backtrans_xlm_roberta": 0.9447838068008423,
    "llm_vs_prof_backtrans_bleu": 58.5891526663386,
    "llm_vs_prof_backtrans_chrf": 76.02566067104324,
    "llm_vs_prof_backtrans_bertscore": 0.9556450843811035,
    "llm_vs_prof_backtrans_labse": 0.972961962223053
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 33.690704607512885,
    "same_lang_chrf": 68.87316477622647,
    "same_lang_bertscore": 0.8654728531837463,
    "same_lang_comet": 0.9372581243515015,
    "cross_lang_xlm_roberta": 0.9363347291946411,
    "cross_lang_labse": 0.9680392146110535,
    "cross_lang_mbert": 0.8787699937820435,
    "cross_lang_comet_qe": 0.5127338171005249,
    "backtrans_bleu": 58.72407409551892,
    "backtrans_chrf": 76.69264655881656,
    "backtrans_bertscore": 0.960163950920105,
    "prof_backtrans_bleu": 28.722789599743727,
    "prof_backtrans_chrf": 62.28097768564529,
    "prof_backtrans_bertscore": 0.9231255650520325,
    "prof_backtrans_labse": 0.940653920173645,
    "prof_backtrans_xlm_roberta": 0.9286826252937317,
    "llm_vs_prof_backtrans_bleu": 36.015076443235884,
    "llm_vs_prof_backtrans_chrf": 65.4880130814368,
    "llm_vs_prof_backtrans_bertscore": 0.9321891665458679,
    "llm_vs_prof_backtrans_labse": 0.9626163244247437
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 24.292946685164488,
    "same_lang_chrf": 44.67132186489378,
    "same_lang_bertscore": 0.8575178980827332,
    "same_lang_comet": 0.9381177425384521,
    "cross_lang_xlm_roberta": 0.9605910181999207,
    "cross_lang_labse": 0.9585279822349548,
    "cross_lang_mbert": 0.9371351599693298,
    "cross_lang_comet_qe": 0.5093977451324463,
    "backtrans_bleu": 43.88434346565326,
    "backtrans_chrf": 70.17155910821633,
    "backtrans_bertscore": 0.9417979121208191,
    "prof_backtrans_bleu": 43.37490376125246,
    "prof_backtrans_chrf": 70.75632090652545,
    "prof_backtrans_bertscore": 0.9421385526657104,
    "prof_backtrans_labse": 0.9565594792366028,
    "prof_backtrans_xlm_roberta": 0.8646339774131775,
    "llm_vs_prof_backtrans_bleu": 49.71340463950443,
    "llm_vs_prof_backtrans_chrf": 73.22048198397985,
    "llm_vs_prof_backtrans_bertscore": 0.938184916973114,
    "llm_vs_prof_backtrans_labse": 0.9673102498054504
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 35.365863283536406,
    "same_lang_chrf": 62.927647429008836,
    "same_lang_bertscore": 0.8186401128768921,
    "same_lang_comet": 0.81230628490448,
    "cross_lang_xlm_roberta": 0.9781903028488159,
    "cross_lang_labse": 0.9866729378700256,
    "cross_lang_mbert": 0.9725266098976135,
    "cross_lang_comet_qe": 0.1603521704673767,
    "backtrans_bleu": 47.06943216127913,
    "backtrans_chrf": 73.42971250748228,
    "backtrans_bertscore": 0.9491114616394043,
    "prof_backtrans_bleu": 30.313825190095166,
    "prof_backtrans_chrf": 64.93364792191981,
    "prof_backtrans_bertscore": 0.9107621908187866,
    "prof_backtrans_labse": 0.953631579875946,
    "prof_backtrans_xlm_roberta": 0.8771020174026489,
    "llm_vs_prof_backtrans_bleu": 43.064682408368355,
    "llm_vs_prof_backtrans_chrf": 68.27883118685777,
    "llm_vs_prof_backtrans_bertscore": 0.9256486892700195,
    "llm_vs_prof_backtrans_labse": 0.9595980644226074
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 66.63288751915496,
    "same_lang_chrf": 84.28395095393729,
    "same_lang_bertscore": 0.9296667575836182,
    "same_lang_comet": 0.9059269428253174,
    "cross_lang_xlm_roberta": 0.8963419795036316,
    "cross_lang_labse": 0.9794495701789856,
    "cross_lang_mbert": 0.9619407057762146,
    "cross_lang_comet_qe": 0.34474867582321167,
    "backtrans_bleu": 64.97301868943978,
    "backtrans_chrf": 82.86144153303601,
    "backtrans_bertscore": 0.954232394695282,
    "prof_backtrans_bleu": 64.81443416708058,
    "prof_backtrans_chrf": 82.08227897724592,
    "prof_backtrans_bertscore": 0.9558850526809692,
    "prof_backtrans_labse": 0.9674263596534729,
    "prof_backtrans_xlm_roberta": 0.9177733063697815,
    "llm_vs_prof_backtrans_bleu": 78.16275600160732,
    "llm_vs_prof_backtrans_chrf": 88.80687267443442,
    "llm_vs_prof_backtrans_bertscore": 0.9654171466827393,
    "llm_vs_prof_backtrans_labse": 0.9772207140922546
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.108226488623908,
    "same_lang_chrf": 42.378011715642046,
    "same_lang_bertscore": 0.8844119906425476,
    "same_lang_comet": 0.9173710346221924,
    "cross_lang_xlm_roberta": 0.9659435153007507,
    "cross_lang_labse": 0.9830977320671082,
    "cross_lang_mbert": 0.9745632410049438,
    "cross_lang_comet_qe": 0.43981486558914185,
    "backtrans_bleu": 50.86081456995326,
    "backtrans_chrf": 78.46969830944525,
    "backtrans_bertscore": 0.9483739137649536,
    "prof_backtrans_bleu": 48.723826388957455,
    "prof_backtrans_chrf": 76.4966679711955,
    "prof_backtrans_bertscore": 0.9348040223121643,
    "prof_backtrans_labse": 0.9604530334472656,
    "prof_backtrans_xlm_roberta": 0.8418286442756653,
    "llm_vs_prof_backtrans_bleu": 63.07833584875649,
    "llm_vs_prof_backtrans_chrf": 80.31948321399146,
    "llm_vs_prof_backtrans_bertscore": 0.9517311453819275,
    "llm_vs_prof_backtrans_labse": 0.9756631255149841
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 48.75780735465971,
    "same_lang_chrf": 66.45022919060467,
    "same_lang_bertscore": 0.8563709259033203,
    "same_lang_comet": 0.9071764945983887,
    "cross_lang_xlm_roberta": 0.9189968109130859,
    "cross_lang_labse": 0.9822368025779724,
    "cross_lang_mbert": 0.969508707523346,
    "cross_lang_comet_qe": 0.3865765333175659,
    "backtrans_bleu": 62.05978798867247,
    "backtrans_chrf": 82.61424352540395,
    "backtrans_bertscore": 0.9498952627182007,
    "prof_backtrans_bleu": 59.115561932266864,
    "prof_backtrans_chrf": 80.32668318166,
    "prof_backtrans_bertscore": 0.939708948135376,
    "prof_backtrans_labse": 0.9600802063941956,
    "prof_backtrans_xlm_roberta": 0.9156748056411743,
    "llm_vs_prof_backtrans_bleu": 68.93374856564877,
    "llm_vs_prof_backtrans_chrf": 83.95172689788657,
    "llm_vs_prof_backtrans_bertscore": 0.9597058892250061,
    "llm_vs_prof_backtrans_labse": 0.9746529459953308
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 49.9588499384388,
    "same_lang_chrf": 75.84285531168258,
    "same_lang_bertscore": 0.8870580792427063,
    "same_lang_comet": 0.9134606122970581,
    "cross_lang_xlm_roberta": 0.9221312403678894,
    "cross_lang_labse": 0.9874399900436401,
    "cross_lang_mbert": 0.9774548411369324,
    "cross_lang_comet_qe": 0.353502094745636,
    "backtrans_bleu": 67.4223885363294,
    "backtrans_chrf": 83.93463711484743,
    "backtrans_bertscore": 0.9599801898002625,
    "prof_backtrans_bleu": 65.68053037812636,
    "prof_backtrans_chrf": 83.47807021439083,
    "prof_backtrans_bertscore": 0.9538896679878235,
    "prof_backtrans_labse": 0.9705668091773987,
    "prof_backtrans_xlm_roberta": 0.9030752778053284,
    "llm_vs_prof_backtrans_bleu": 69.59527494350937,
    "llm_vs_prof_backtrans_chrf": 83.34028718032154,
    "llm_vs_prof_backtrans_bertscore": 0.9619007110595703,
    "llm_vs_prof_backtrans_labse": 0.9696334600448608
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 38.07429731184726,
    "same_lang_chrf": 65.27668938780525,
    "same_lang_bertscore": 0.8584649562835693,
    "same_lang_comet": 0.9169408679008484,
    "cross_lang_xlm_roberta": 0.9572438597679138,
    "cross_lang_labse": 0.9805726408958435,
    "cross_lang_mbert": 0.9843096733093262,
    "cross_lang_comet_qe": 0.40461432933807373,
    "backtrans_bleu": 58.06581295451102,
    "backtrans_chrf": 79.60473492580917,
    "backtrans_bertscore": 0.9561368823051453,
    "prof_backtrans_bleu": 46.73973949092329,
    "prof_backtrans_chrf": 72.78543220334322,
    "prof_backtrans_bertscore": 0.942891538143158,
    "prof_backtrans_labse": 0.9604626297950745,
    "prof_backtrans_xlm_roberta": 0.9032484889030457,
    "llm_vs_prof_backtrans_bleu": 55.309704643999964,
    "llm_vs_prof_backtrans_chrf": 75.84386899123844,
    "llm_vs_prof_backtrans_bertscore": 0.9517998695373535,
    "llm_vs_prof_backtrans_labse": 0.9697316884994507
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 16.62247099638892,
    "same_lang_chrf": 36.70283814971957,
    "same_lang_bertscore": 0.8359236121177673,
    "same_lang_comet": 0.9050551652908325,
    "cross_lang_xlm_roberta": 0.9107754826545715,
    "cross_lang_labse": 0.9492002725601196,
    "cross_lang_mbert": 0.9643735885620117,
    "cross_lang_comet_qe": 0.3864997625350952,
    "backtrans_bleu": 46.242575425711216,
    "backtrans_chrf": 76.00065734363469,
    "backtrans_bertscore": 0.9316615462303162,
    "prof_backtrans_bleu": 49.65941581904083,
    "prof_backtrans_chrf": 75.86995814141996,
    "prof_backtrans_bertscore": 0.9392399787902832,
    "prof_backtrans_labse": 0.966748833656311,
    "prof_backtrans_xlm_roberta": 0.8578901290893555,
    "llm_vs_prof_backtrans_bleu": 49.23712966614364,
    "llm_vs_prof_backtrans_chrf": 74.86869393522345,
    "llm_vs_prof_backtrans_bertscore": 0.9284737706184387,
    "llm_vs_prof_backtrans_labse": 0.9469834566116333
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 38.63901134432063,
    "same_lang_chrf": 69.23737574423595,
    "same_lang_bertscore": 0.8683244585990906,
    "same_lang_comet": 0.802518904209137,
    "cross_lang_xlm_roberta": 0.9216309189796448,
    "cross_lang_labse": 0.9740457534790039,
    "cross_lang_mbert": 0.9645748734474182,
    "cross_lang_comet_qe": 0.3767128884792328,
    "backtrans_bleu": 57.63359958750547,
    "backtrans_chrf": 81.18649563113358,
    "backtrans_bertscore": 0.941682755947113,
    "prof_backtrans_bleu": 56.66381634738308,
    "prof_backtrans_chrf": 79.71352510273543,
    "prof_backtrans_bertscore": 0.9414626955986023,
    "prof_backtrans_labse": 0.9659098982810974,
    "prof_backtrans_xlm_roberta": 0.86788409948349,
    "llm_vs_prof_backtrans_bleu": 58.48588896617296,
    "llm_vs_prof_backtrans_chrf": 78.87031148195625,
    "llm_vs_prof_backtrans_bertscore": 0.9562689065933228,
    "llm_vs_prof_backtrans_labse": 0.9675132036209106
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 68.84654468831062,
    "same_lang_chrf": 84.29395766518401,
    "same_lang_bertscore": 0.9305259585380554,
    "same_lang_comet": 0.8868918418884277,
    "cross_lang_xlm_roberta": 0.9762211441993713,
    "cross_lang_labse": 0.9850361347198486,
    "cross_lang_mbert": 0.9857282042503357,
    "cross_lang_comet_qe": 0.42402327060699463,
    "backtrans_bleu": 72.8916146722072,
    "backtrans_chrf": 85.76842200103043,
    "backtrans_bertscore": 0.9752869606018066,
    "prof_backtrans_bleu": 63.37955295636333,
    "prof_backtrans_chrf": 80.17022543160506,
    "prof_backtrans_bertscore": 0.9622339606285095,
    "prof_backtrans_labse": 0.9717174768447876,
    "prof_backtrans_xlm_roberta": 0.9264482855796814,
    "llm_vs_prof_backtrans_bleu": 78.66212738006347,
    "llm_vs_prof_backtrans_chrf": 87.90679056860868,
    "llm_vs_prof_backtrans_bertscore": 0.9727826118469238,
    "llm_vs_prof_backtrans_labse": 0.9703975915908813
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 9.070585002530697,
    "same_lang_chrf": 47.97318509257919,
    "same_lang_bertscore": 0.8822875618934631,
    "same_lang_comet": 0.9226791262626648,
    "cross_lang_xlm_roberta": 0.9498847126960754,
    "cross_lang_labse": 0.9395133256912231,
    "cross_lang_mbert": 0.8844497203826904,
    "cross_lang_comet_qe": 0.43706995248794556,
    "backtrans_bleu": 46.04049930204571,
    "backtrans_chrf": 72.00089581141485,
    "backtrans_bertscore": 0.9409416317939758,
    "prof_backtrans_bleu": 43.606979260584254,
    "prof_backtrans_chrf": 72.27881696446484,
    "prof_backtrans_bertscore": 0.9448923468589783,
    "prof_backtrans_labse": 0.9590888023376465,
    "prof_backtrans_xlm_roberta": 0.8702346086502075,
    "llm_vs_prof_backtrans_bleu": 53.60096384457364,
    "llm_vs_prof_backtrans_chrf": 74.29211682138764,
    "llm_vs_prof_backtrans_bertscore": 0.9320259690284729,
    "llm_vs_prof_backtrans_labse": 0.9422475099563599
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 52.822182397827135,
    "same_lang_chrf": 68.44128941941563,
    "same_lang_bertscore": 0.8699772953987122,
    "same_lang_comet": 0.8963193893432617,
    "cross_lang_xlm_roberta": 0.9844924211502075,
    "cross_lang_labse": 0.9864739775657654,
    "cross_lang_mbert": 0.9824303984642029,
    "cross_lang_comet_qe": 0.4294678270816803,
    "backtrans_bleu": 68.67768014899237,
    "backtrans_chrf": 83.8292994439118,
    "backtrans_bertscore": 0.9745777249336243,
    "prof_backtrans_bleu": 56.94270499578926,
    "prof_backtrans_chrf": 77.91446809738447,
    "prof_backtrans_bertscore": 0.9552644491195679,
    "prof_backtrans_labse": 0.9582428336143494,
    "prof_backtrans_xlm_roberta": 0.9132894277572632,
    "llm_vs_prof_backtrans_bleu": 67.59760939808089,
    "llm_vs_prof_backtrans_chrf": 81.6773925225116,
    "llm_vs_prof_backtrans_bertscore": 0.9622757434844971,
    "llm_vs_prof_backtrans_labse": 0.9729639887809753
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 49.80754455508049,
    "same_lang_chrf": 77.37916624814147,
    "same_lang_bertscore": 0.8965401649475098,
    "same_lang_comet": 0.9202038645744324,
    "cross_lang_xlm_roberta": 0.9235852956771851,
    "cross_lang_labse": 0.9836236238479614,
    "cross_lang_mbert": 0.9795999526977539,
    "cross_lang_comet_qe": 0.4325936734676361,
    "backtrans_bleu": 71.02381629279347,
    "backtrans_chrf": 84.49974220138827,
    "backtrans_bertscore": 0.9740495085716248,
    "prof_backtrans_bleu": 69.5388977261136,
    "prof_backtrans_chrf": 83.6157442801301,
    "prof_backtrans_bertscore": 0.9623128771781921,
    "prof_backtrans_labse": 0.9764816164970398,
    "prof_backtrans_xlm_roberta": 0.9113426804542542,
    "llm_vs_prof_backtrans_bleu": 67.03267664385119,
    "llm_vs_prof_backtrans_chrf": 82.1679018105905,
    "llm_vs_prof_backtrans_bertscore": 0.9535471200942993,
    "llm_vs_prof_backtrans_labse": 0.962867796421051
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 39.47258727303956,
    "same_lang_chrf": 68.98758981104915,
    "same_lang_bertscore": 0.8385721445083618,
    "same_lang_comet": 0.9252206683158875,
    "cross_lang_xlm_roberta": 0.9613142013549805,
    "cross_lang_labse": 0.9508998394012451,
    "cross_lang_mbert": 0.9562686681747437,
    "cross_lang_comet_qe": 0.4110184907913208,
    "backtrans_bleu": 59.00479777424106,
    "backtrans_chrf": 77.48471699265197,
    "backtrans_bertscore": 0.9496270418167114,
    "prof_backtrans_bleu": 45.45841963686084,
    "prof_backtrans_chrf": 71.6352208982627,
    "prof_backtrans_bertscore": 0.9502405524253845,
    "prof_backtrans_labse": 0.9590516090393066,
    "prof_backtrans_xlm_roberta": 0.9151393175125122,
    "llm_vs_prof_backtrans_bleu": 53.30503522164828,
    "llm_vs_prof_backtrans_chrf": 73.33240638937954,
    "llm_vs_prof_backtrans_bertscore": 0.9390642046928406,
    "llm_vs_prof_backtrans_labse": 0.9657312631607056
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 23.291818723755167,
    "same_lang_chrf": 44.67362543683132,
    "same_lang_bertscore": 0.8261049389839172,
    "same_lang_comet": 0.9187585115432739,
    "cross_lang_xlm_roberta": 0.9574509859085083,
    "cross_lang_labse": 0.9532871246337891,
    "cross_lang_mbert": 0.9066184163093567,
    "cross_lang_comet_qe": 0.4343187212944031,
    "backtrans_bleu": 53.19390108198634,
    "backtrans_chrf": 74.17514444644837,
    "backtrans_bertscore": 0.9441928863525391,
    "prof_backtrans_bleu": 52.76646440345038,
    "prof_backtrans_chrf": 75.3341728550035,
    "prof_backtrans_bertscore": 0.9419353008270264,
    "prof_backtrans_labse": 0.9649491310119629,
    "prof_backtrans_xlm_roberta": 0.8473600149154663,
    "llm_vs_prof_backtrans_bleu": 53.7989496349149,
    "llm_vs_prof_backtrans_chrf": 74.74481396184582,
    "llm_vs_prof_backtrans_bertscore": 0.9371238946914673,
    "llm_vs_prof_backtrans_labse": 0.948201060295105
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 44.303811425145646,
    "same_lang_chrf": 69.55117820950802,
    "same_lang_bertscore": 0.8451207280158997,
    "same_lang_comet": 0.7962501645088196,
    "cross_lang_xlm_roberta": 0.975762128829956,
    "cross_lang_labse": 0.9895673990249634,
    "cross_lang_mbert": 0.9921950101852417,
    "cross_lang_comet_qe": 0.39472314715385437,
    "backtrans_bleu": 73.97118746831856,
    "backtrans_chrf": 86.52781679182311,
    "backtrans_bertscore": 0.9774746298789978,
    "prof_backtrans_bleu": 56.060798654139056,
    "prof_backtrans_chrf": 78.5755197851533,
    "prof_backtrans_bertscore": 0.9508220553398132,
    "prof_backtrans_labse": 0.9686598777770996,
    "prof_backtrans_xlm_roberta": 0.8812994360923767,
    "llm_vs_prof_backtrans_bleu": 62.29538331773463,
    "llm_vs_prof_backtrans_chrf": 79.28314325843552,
    "llm_vs_prof_backtrans_bertscore": 0.9559016823768616,
    "llm_vs_prof_backtrans_labse": 0.9684563875198364
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 72.36495085104707,
    "same_lang_chrf": 86.48690933953182,
    "same_lang_bertscore": 0.9326142072677612,
    "same_lang_comet": 0.9028282165527344,
    "cross_lang_xlm_roberta": 0.967197835445404,
    "cross_lang_labse": 0.9889006614685059,
    "cross_lang_mbert": 0.9862814545631409,
    "cross_lang_comet_qe": 0.39140188694000244,
    "backtrans_bleu": 70.50872626473137,
    "backtrans_chrf": 84.91390577324842,
    "backtrans_bertscore": 0.9733939170837402,
    "prof_backtrans_bleu": 60.0863840933723,
    "prof_backtrans_chrf": 78.5919021702202,
    "prof_backtrans_bertscore": 0.958535373210907,
    "prof_backtrans_labse": 0.9711638689041138,
    "prof_backtrans_xlm_roberta": 0.9252369999885559,
    "llm_vs_prof_backtrans_bleu": 76.45493756390682,
    "llm_vs_prof_backtrans_chrf": 86.51516894428177,
    "llm_vs_prof_backtrans_bertscore": 0.9702942371368408,
    "llm_vs_prof_backtrans_labse": 0.9778261780738831
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 18.56957462985545,
    "same_lang_chrf": 51.14252253856808,
    "same_lang_bertscore": 0.9101409912109375,
    "same_lang_comet": 0.9194104671478271,
    "cross_lang_xlm_roberta": 0.964795708656311,
    "cross_lang_labse": 0.9840214848518372,
    "cross_lang_mbert": 0.9092698097229004,
    "cross_lang_comet_qe": 0.43593379855155945,
    "backtrans_bleu": 56.85978973384294,
    "backtrans_chrf": 78.88678445428285,
    "backtrans_bertscore": 0.9698429703712463,
    "prof_backtrans_bleu": 36.08683745779742,
    "prof_backtrans_chrf": 68.45890641562782,
    "prof_backtrans_bertscore": 0.9388275146484375,
    "prof_backtrans_labse": 0.9575653672218323,
    "prof_backtrans_xlm_roberta": 0.8772353529930115,
    "llm_vs_prof_backtrans_bleu": 48.277753920044766,
    "llm_vs_prof_backtrans_chrf": 71.15586534755123,
    "llm_vs_prof_backtrans_bertscore": 0.9461231827735901,
    "llm_vs_prof_backtrans_labse": 0.9709020853042603
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 60.97437721445303,
    "same_lang_chrf": 74.28624809689984,
    "same_lang_bertscore": 0.8766963481903076,
    "same_lang_comet": 0.9074494242668152,
    "cross_lang_xlm_roberta": 0.9865362644195557,
    "cross_lang_labse": 0.9758882522583008,
    "cross_lang_mbert": 0.9464940428733826,
    "cross_lang_comet_qe": 0.4098637104034424,
    "backtrans_bleu": 57.855962854475926,
    "backtrans_chrf": 78.83002142347628,
    "backtrans_bertscore": 0.966709315776825,
    "prof_backtrans_bleu": 43.59475389966539,
    "prof_backtrans_chrf": 72.19314856647617,
    "prof_backtrans_bertscore": 0.934981644153595,
    "prof_backtrans_labse": 0.9498574733734131,
    "prof_backtrans_xlm_roberta": 0.9234997034072876,
    "llm_vs_prof_backtrans_bleu": 59.024108144409766,
    "llm_vs_prof_backtrans_chrf": 76.49532778024944,
    "llm_vs_prof_backtrans_bertscore": 0.9459877014160156,
    "llm_vs_prof_backtrans_labse": 0.9654505252838135
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 53.117293913679084,
    "same_lang_chrf": 77.71201078446754,
    "same_lang_bertscore": 0.8870755434036255,
    "same_lang_comet": 0.9206493496894836,
    "cross_lang_xlm_roberta": 0.9490801095962524,
    "cross_lang_labse": 0.9581867456436157,
    "cross_lang_mbert": 0.9694898724555969,
    "cross_lang_comet_qe": 0.3577362298965454,
    "backtrans_bleu": 64.37779362745975,
    "backtrans_chrf": 82.83650367840623,
    "backtrans_bertscore": 0.967823326587677,
    "prof_backtrans_bleu": 60.50833196174631,
    "prof_backtrans_chrf": 79.58352553743582,
    "prof_backtrans_bertscore": 0.9515421986579895,
    "prof_backtrans_labse": 0.9713189601898193,
    "prof_backtrans_xlm_roberta": 0.9000157117843628,
    "llm_vs_prof_backtrans_bleu": 62.68505528068233,
    "llm_vs_prof_backtrans_chrf": 80.17536083214931,
    "llm_vs_prof_backtrans_bertscore": 0.9473376274108887,
    "llm_vs_prof_backtrans_labse": 0.9448041915893555
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 41.487266571085335,
    "same_lang_chrf": 67.67875496014094,
    "same_lang_bertscore": 0.8513603210449219,
    "same_lang_comet": 0.9239877462387085,
    "cross_lang_xlm_roberta": 0.9679374694824219,
    "cross_lang_labse": 0.9524580240249634,
    "cross_lang_mbert": 0.9694780111312866,
    "cross_lang_comet_qe": 0.35328152775764465,
    "backtrans_bleu": 52.96441665965221,
    "backtrans_chrf": 74.15821932698381,
    "backtrans_bertscore": 0.9566129446029663,
    "prof_backtrans_bleu": 39.31978990487688,
    "prof_backtrans_chrf": 69.03849881856937,
    "prof_backtrans_bertscore": 0.9458795189857483,
    "prof_backtrans_labse": 0.9548425674438477,
    "prof_backtrans_xlm_roberta": 0.9072090983390808,
    "llm_vs_prof_backtrans_bleu": 48.587979385888325,
    "llm_vs_prof_backtrans_chrf": 70.88375370317382,
    "llm_vs_prof_backtrans_bertscore": 0.9463433623313904,
    "llm_vs_prof_backtrans_labse": 0.9614773392677307
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 25.727388621867,
    "same_lang_chrf": 46.27828952673869,
    "same_lang_bertscore": 0.8506603837013245,
    "same_lang_comet": 0.9185696244239807,
    "cross_lang_xlm_roberta": 0.9674205780029297,
    "cross_lang_labse": 0.9520997405052185,
    "cross_lang_mbert": 0.9004107713699341,
    "cross_lang_comet_qe": 0.36361730098724365,
    "backtrans_bleu": 39.56602013963494,
    "backtrans_chrf": 68.42151844652797,
    "backtrans_bertscore": 0.9429383277893066,
    "prof_backtrans_bleu": 49.488495603689174,
    "prof_backtrans_chrf": 74.87553102524542,
    "prof_backtrans_bertscore": 0.9428356885910034,
    "prof_backtrans_labse": 0.9694349765777588,
    "prof_backtrans_xlm_roberta": 0.859079897403717,
    "llm_vs_prof_backtrans_bleu": 49.89453869613089,
    "llm_vs_prof_backtrans_chrf": 74.71562346889596,
    "llm_vs_prof_backtrans_bertscore": 0.932379424571991,
    "llm_vs_prof_backtrans_labse": 0.9593188166618347
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 37.28010048378675,
    "same_lang_chrf": 67.96159437334475,
    "same_lang_bertscore": 0.8571206331253052,
    "same_lang_comet": 0.739404559135437,
    "cross_lang_xlm_roberta": 0.9838688373565674,
    "cross_lang_labse": 0.9791247844696045,
    "cross_lang_mbert": 0.9815293550491333,
    "cross_lang_comet_qe": 0.294217973947525,
    "backtrans_bleu": 56.712889288340506,
    "backtrans_chrf": 79.70098324769253,
    "backtrans_bertscore": 0.9611245393753052,
    "prof_backtrans_bleu": 46.857719817872336,
    "prof_backtrans_chrf": 74.19340027866318,
    "prof_backtrans_bertscore": 0.9386188983917236,
    "prof_backtrans_labse": 0.9707350134849548,
    "prof_backtrans_xlm_roberta": 0.8803322315216064,
    "llm_vs_prof_backtrans_bleu": 54.227481364560944,
    "llm_vs_prof_backtrans_chrf": 75.03101962741253,
    "llm_vs_prof_backtrans_bertscore": 0.9449616074562073,
    "llm_vs_prof_backtrans_labse": 0.9655343294143677
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 67.79785217542603,
    "same_lang_chrf": 82.87250600093614,
    "same_lang_bertscore": 0.9264955520629883,
    "same_lang_comet": 0.8885887861251831,
    "cross_lang_xlm_roberta": 0.9498339295387268,
    "cross_lang_labse": 0.9828014373779297,
    "cross_lang_mbert": 0.9793710708618164,
    "cross_lang_comet_qe": 0.3174355626106262,
    "backtrans_bleu": 59.75698952584144,
    "backtrans_chrf": 78.30576478422968,
    "backtrans_bertscore": 0.9652010798454285,
    "prof_backtrans_bleu": 42.72230289052252,
    "prof_backtrans_chrf": 70.2599402665748,
    "prof_backtrans_bertscore": 0.9363365173339844,
    "prof_backtrans_labse": 0.9632161259651184,
    "prof_backtrans_xlm_roberta": 0.9147655367851257,
    "llm_vs_prof_backtrans_bleu": 61.626212226696026,
    "llm_vs_prof_backtrans_chrf": 76.48705258018742,
    "llm_vs_prof_backtrans_bertscore": 0.9485995769500732,
    "llm_vs_prof_backtrans_labse": 0.9683745503425598
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 20.890016114290283,
    "same_lang_chrf": 52.21314677233272,
    "same_lang_bertscore": 0.9057624936103821,
    "same_lang_comet": 0.9226201176643372,
    "cross_lang_xlm_roberta": 0.9508243799209595,
    "cross_lang_labse": 0.9695491194725037,
    "cross_lang_mbert": 0.9716572165489197,
    "cross_lang_comet_qe": 0.43803906440734863,
    "backtrans_bleu": 43.83970366075061,
    "backtrans_chrf": 70.07083643586202,
    "backtrans_bertscore": 0.9475713968276978,
    "prof_backtrans_bleu": 39.77802254320229,
    "prof_backtrans_chrf": 70.33591522423033,
    "prof_backtrans_bertscore": 0.9425331354141235,
    "prof_backtrans_labse": 0.9646856784820557,
    "prof_backtrans_xlm_roberta": 0.8676259517669678,
    "llm_vs_prof_backtrans_bleu": 50.07588902580261,
    "llm_vs_prof_backtrans_chrf": 74.05533002419166,
    "llm_vs_prof_backtrans_bertscore": 0.936394989490509,
    "llm_vs_prof_backtrans_labse": 0.9786719679832458
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 46.31782202640121,
    "same_lang_chrf": 63.4518609695934,
    "same_lang_bertscore": 0.8429780602455139,
    "same_lang_comet": 0.8864940404891968,
    "cross_lang_xlm_roberta": 0.9814625978469849,
    "cross_lang_labse": 0.97365403175354,
    "cross_lang_mbert": 0.9782500863075256,
    "cross_lang_comet_qe": 0.35427331924438477,
    "backtrans_bleu": 60.23227632032858,
    "backtrans_chrf": 78.4844934392166,
    "backtrans_bertscore": 0.965275764465332,
    "prof_backtrans_bleu": 42.54891796586006,
    "prof_backtrans_chrf": 71.42647820578465,
    "prof_backtrans_bertscore": 0.9302025437355042,
    "prof_backtrans_labse": 0.9413394927978516,
    "prof_backtrans_xlm_roberta": 0.9226861000061035,
    "llm_vs_prof_backtrans_bleu": 49.31658954924529,
    "llm_vs_prof_backtrans_chrf": 71.71445667963295,
    "llm_vs_prof_backtrans_bertscore": 0.9318076372146606,
    "llm_vs_prof_backtrans_labse": 0.9359275102615356
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 43.94109855224108,
    "same_lang_chrf": 73.42653520063195,
    "same_lang_bertscore": 0.8547523021697998,
    "same_lang_comet": 0.9110671281814575,
    "cross_lang_xlm_roberta": 0.9353169798851013,
    "cross_lang_labse": 0.9886679649353027,
    "cross_lang_mbert": 0.9623541831970215,
    "cross_lang_comet_qe": 0.40147414803504944,
    "backtrans_bleu": 62.87965741084674,
    "backtrans_chrf": 80.7196092233241,
    "backtrans_bertscore": 0.9683900475502014,
    "prof_backtrans_bleu": 58.28489045885247,
    "prof_backtrans_chrf": 78.24892500108639,
    "prof_backtrans_bertscore": 0.9594798684120178,
    "prof_backtrans_labse": 0.9687774181365967,
    "prof_backtrans_xlm_roberta": 0.9139062762260437,
    "llm_vs_prof_backtrans_bleu": 58.912329852861696,
    "llm_vs_prof_backtrans_chrf": 78.36226010386946,
    "llm_vs_prof_backtrans_bertscore": 0.9555748105049133,
    "llm_vs_prof_backtrans_labse": 0.970138430595398
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 38.10482470103496,
    "same_lang_chrf": 68.51133648716467,
    "same_lang_bertscore": 0.8632819056510925,
    "same_lang_comet": 0.9175713062286377,
    "cross_lang_xlm_roberta": 0.9681355357170105,
    "cross_lang_labse": 0.9656928181648254,
    "cross_lang_mbert": 0.9756931662559509,
    "cross_lang_comet_qe": 0.3661559820175171,
    "backtrans_bleu": 45.3163480308616,
    "backtrans_chrf": 70.87320655383031,
    "backtrans_bertscore": 0.9527194499969482,
    "prof_backtrans_bleu": 33.93409535755717,
    "prof_backtrans_chrf": 66.01180333594667,
    "prof_backtrans_bertscore": 0.9412001371383667,
    "prof_backtrans_labse": 0.9524456262588501,
    "prof_backtrans_xlm_roberta": 0.9047939777374268,
    "llm_vs_prof_backtrans_bleu": 45.2714378092233,
    "llm_vs_prof_backtrans_chrf": 71.00233400607992,
    "llm_vs_prof_backtrans_bertscore": 0.9474532008171082,
    "llm_vs_prof_backtrans_labse": 0.9638710618019104
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 23.432212199096078,
    "same_lang_chrf": 44.56523387879729,
    "same_lang_bertscore": 0.8370640873908997,
    "same_lang_comet": 0.9068323373794556,
    "cross_lang_xlm_roberta": 0.9696669578552246,
    "cross_lang_labse": 0.9600457549095154,
    "cross_lang_mbert": 0.8951508402824402,
    "cross_lang_comet_qe": 0.2877073884010315,
    "backtrans_bleu": 40.05597354333718,
    "backtrans_chrf": 67.6820778810341,
    "backtrans_bertscore": 0.9459637403488159,
    "prof_backtrans_bleu": 42.10966999340308,
    "prof_backtrans_chrf": 70.16585864609613,
    "prof_backtrans_bertscore": 0.9355707764625549,
    "prof_backtrans_labse": 0.9632300734519958,
    "prof_backtrans_xlm_roberta": 0.8453581929206848,
    "llm_vs_prof_backtrans_bleu": 40.756629579478734,
    "llm_vs_prof_backtrans_chrf": 67.42782021679507,
    "llm_vs_prof_backtrans_bertscore": 0.9373437762260437,
    "llm_vs_prof_backtrans_labse": 0.9561421275138855
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 39.4090925171026,
    "same_lang_chrf": 61.481135885823925,
    "same_lang_bertscore": 0.8366972208023071,
    "same_lang_comet": 0.7887868881225586,
    "cross_lang_xlm_roberta": 0.9627941846847534,
    "cross_lang_labse": 0.9702322483062744,
    "cross_lang_mbert": 0.854642391204834,
    "cross_lang_comet_qe": 0.05642189085483551,
    "backtrans_bleu": 51.96316013630715,
    "backtrans_chrf": 73.20736965042578,
    "backtrans_bertscore": 0.9537861943244934,
    "prof_backtrans_bleu": 47.288772521940196,
    "prof_backtrans_chrf": 73.64140205038935,
    "prof_backtrans_bertscore": 0.943730890750885,
    "prof_backtrans_labse": 0.9730598330497742,
    "prof_backtrans_xlm_roberta": 0.8959143757820129,
    "llm_vs_prof_backtrans_bleu": 47.0231853509201,
    "llm_vs_prof_backtrans_chrf": 69.46355294042863,
    "llm_vs_prof_backtrans_bertscore": 0.9382361769676208,
    "llm_vs_prof_backtrans_labse": 0.9473639726638794
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 51.42903864215323,
    "same_lang_chrf": 73.78471302110303,
    "same_lang_bertscore": 0.9006652235984802,
    "same_lang_comet": 0.9191603064537048,
    "cross_lang_xlm_roberta": 0.9920305609703064,
    "cross_lang_labse": 0.9940319657325745,
    "cross_lang_mbert": 0.9947553277015686,
    "cross_lang_comet_qe": 0.476241797208786,
    "backtrans_bleu": 73.22009830635643,
    "backtrans_chrf": 86.43453260925439,
    "backtrans_bertscore": 0.9815675020217896,
    "prof_backtrans_bleu": 57.626413487306735,
    "prof_backtrans_chrf": 77.92158553322642,
    "prof_backtrans_bertscore": 0.9184795022010803,
    "prof_backtrans_labse": 0.9762609004974365,
    "prof_backtrans_xlm_roberta": 0.941078245639801,
    "llm_vs_prof_backtrans_bleu": 67.51479619114407,
    "llm_vs_prof_backtrans_chrf": 80.58089762415123,
    "llm_vs_prof_backtrans_bertscore": 0.9221614003181458,
    "llm_vs_prof_backtrans_labse": 0.9796686768531799
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 12.626332859727599,
    "same_lang_chrf": 35.9530629866161,
    "same_lang_bertscore": 0.8650690913200378,
    "same_lang_comet": 0.9211826920509338,
    "cross_lang_xlm_roberta": 0.9815037250518799,
    "cross_lang_labse": 0.9850813746452332,
    "cross_lang_mbert": 0.985263466835022,
    "cross_lang_comet_qe": 0.47029876708984375,
    "backtrans_bleu": 46.2127397273889,
    "backtrans_chrf": 74.32719372020345,
    "backtrans_bertscore": 0.9594641327857971,
    "prof_backtrans_bleu": 36.39851786910403,
    "prof_backtrans_chrf": 64.1801651327493,
    "prof_backtrans_bertscore": 0.920933187007904,
    "prof_backtrans_labse": 0.960446834564209,
    "prof_backtrans_xlm_roberta": 0.8253065943717957,
    "llm_vs_prof_backtrans_bleu": 40.40926504464742,
    "llm_vs_prof_backtrans_chrf": 65.38205796516621,
    "llm_vs_prof_backtrans_bertscore": 0.9313763380050659,
    "llm_vs_prof_backtrans_labse": 0.9726429581642151
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 46.59270380862621,
    "same_lang_chrf": 63.89587346123305,
    "same_lang_bertscore": 0.8628044128417969,
    "same_lang_comet": 0.920390248298645,
    "cross_lang_xlm_roberta": 0.9794763922691345,
    "cross_lang_labse": 0.992188036441803,
    "cross_lang_mbert": 0.9849377870559692,
    "cross_lang_comet_qe": 0.4839288890361786,
    "backtrans_bleu": 58.99955244426256,
    "backtrans_chrf": 77.54932480022873,
    "backtrans_bertscore": 0.9724746346473694,
    "prof_backtrans_bleu": 60.17876519931116,
    "prof_backtrans_chrf": 78.55609813423105,
    "prof_backtrans_bertscore": 0.9642385244369507,
    "prof_backtrans_labse": 0.9807680249214172,
    "prof_backtrans_xlm_roberta": 0.9005635380744934,
    "llm_vs_prof_backtrans_bleu": 66.90685194052928,
    "llm_vs_prof_backtrans_chrf": 79.73780004172386,
    "llm_vs_prof_backtrans_bertscore": 0.9684138894081116,
    "llm_vs_prof_backtrans_labse": 0.9818164706230164
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 37.05088397992262,
    "same_lang_chrf": 71.42220806570214,
    "same_lang_bertscore": 0.8286789059638977,
    "same_lang_comet": 0.9190014600753784,
    "cross_lang_xlm_roberta": 0.9849264621734619,
    "cross_lang_labse": 0.9940657019615173,
    "cross_lang_mbert": 0.9865660667419434,
    "cross_lang_comet_qe": 0.4694947898387909,
    "backtrans_bleu": 67.39670666207033,
    "backtrans_chrf": 82.91540041529984,
    "backtrans_bertscore": 0.97658771276474,
    "prof_backtrans_bleu": 68.78737222491267,
    "prof_backtrans_chrf": 82.87082576404067,
    "prof_backtrans_bertscore": 0.9735851287841797,
    "prof_backtrans_labse": 0.9872363805770874,
    "prof_backtrans_xlm_roberta": 0.9223700761795044,
    "llm_vs_prof_backtrans_bleu": 68.96837011726885,
    "llm_vs_prof_backtrans_chrf": 81.80792438996922,
    "llm_vs_prof_backtrans_bertscore": 0.973090648651123,
    "llm_vs_prof_backtrans_labse": 0.9883803725242615
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 33.216928685206646,
    "same_lang_chrf": 64.91291300162624,
    "same_lang_bertscore": 0.873475193977356,
    "same_lang_comet": 0.9392584562301636,
    "cross_lang_xlm_roberta": 0.9879179000854492,
    "cross_lang_labse": 0.9857362508773804,
    "cross_lang_mbert": 0.966168224811554,
    "cross_lang_comet_qe": 0.4519307613372803,
    "backtrans_bleu": 53.747011628205264,
    "backtrans_chrf": 75.29830321391015,
    "backtrans_bertscore": 0.9659651517868042,
    "prof_backtrans_bleu": 35.028206639817846,
    "prof_backtrans_chrf": 63.29945699339247,
    "prof_backtrans_bertscore": 0.9447346925735474,
    "prof_backtrans_labse": 0.9692994952201843,
    "prof_backtrans_xlm_roberta": 0.9046888947486877,
    "llm_vs_prof_backtrans_bleu": 50.2903497865199,
    "llm_vs_prof_backtrans_chrf": 71.82246966234841,
    "llm_vs_prof_backtrans_bertscore": 0.958758533000946,
    "llm_vs_prof_backtrans_labse": 0.970442533493042
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 16.370305475961196,
    "same_lang_chrf": 37.562965879865445,
    "same_lang_bertscore": 0.819776713848114,
    "same_lang_comet": 0.9396607875823975,
    "cross_lang_xlm_roberta": 0.9814398884773254,
    "cross_lang_labse": 0.9826062321662903,
    "cross_lang_mbert": 0.9788633584976196,
    "cross_lang_comet_qe": 0.39163798093795776,
    "backtrans_bleu": 48.36710163816769,
    "backtrans_chrf": 74.98861119175204,
    "backtrans_bertscore": 0.956544041633606,
    "prof_backtrans_bleu": 54.979389459843105,
    "prof_backtrans_chrf": 77.34481805709811,
    "prof_backtrans_bertscore": 0.9311876893043518,
    "prof_backtrans_labse": 0.9765755534172058,
    "prof_backtrans_xlm_roberta": 0.887218713760376,
    "llm_vs_prof_backtrans_bleu": 55.84406849208517,
    "llm_vs_prof_backtrans_chrf": 76.37129537977596,
    "llm_vs_prof_backtrans_bertscore": 0.939511239528656,
    "llm_vs_prof_backtrans_labse": 0.9799357056617737
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 37.389376821722706,
    "same_lang_chrf": 61.89274239785537,
    "same_lang_bertscore": 0.8463425040245056,
    "same_lang_comet": 0.8453319668769836,
    "cross_lang_xlm_roberta": 0.9820749163627625,
    "cross_lang_labse": 0.9867129921913147,
    "cross_lang_mbert": 0.9784636497497559,
    "cross_lang_comet_qe": 0.42860233783721924,
    "backtrans_bleu": 58.465364406272506,
    "backtrans_chrf": 79.30611788301015,
    "backtrans_bertscore": 0.9616270065307617,
    "prof_backtrans_bleu": 47.9564236574545,
    "prof_backtrans_chrf": 74.7548014887062,
    "prof_backtrans_bertscore": 0.907823920249939,
    "prof_backtrans_labse": 0.9454848766326904,
    "prof_backtrans_xlm_roberta": 0.8486196398735046,
    "llm_vs_prof_backtrans_bleu": 49.44909399263949,
    "llm_vs_prof_backtrans_chrf": 70.656858606844,
    "llm_vs_prof_backtrans_bertscore": 0.9100186824798584,
    "llm_vs_prof_backtrans_labse": 0.9644889235496521
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 50.23892742111494,
    "same_lang_chrf": 74.2740743865973,
    "same_lang_bertscore": 0.8987968564033508,
    "same_lang_comet": 0.9185481071472168,
    "cross_lang_xlm_roberta": 0.9984106421470642,
    "cross_lang_labse": 0.9963100552558899,
    "cross_lang_mbert": 0.9977920055389404,
    "cross_lang_comet_qe": 0.47793275117874146,
    "backtrans_bleu": 75.56146111909524,
    "backtrans_chrf": 87.7654122066019,
    "backtrans_bertscore": 0.983565628528595,
    "prof_backtrans_bleu": 56.56059991281456,
    "prof_backtrans_chrf": 77.05642963269732,
    "prof_backtrans_bertscore": 0.95902019739151,
    "prof_backtrans_labse": 0.9823057651519775,
    "prof_backtrans_xlm_roberta": 0.9344038367271423,
    "llm_vs_prof_backtrans_bleu": 65.1018079493734,
    "llm_vs_prof_backtrans_chrf": 78.98394668186187,
    "llm_vs_prof_backtrans_bertscore": 0.9658779501914978,
    "llm_vs_prof_backtrans_labse": 0.9863588809967041
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 13.856114955031416,
    "same_lang_chrf": 37.073015235457135,
    "same_lang_bertscore": 0.8813425898551941,
    "same_lang_comet": 0.9167684316635132,
    "cross_lang_xlm_roberta": 0.9806764125823975,
    "cross_lang_labse": 0.9939427971839905,
    "cross_lang_mbert": 0.9897834062576294,
    "cross_lang_comet_qe": 0.4752129912376404,
    "backtrans_bleu": 58.4496345525619,
    "backtrans_chrf": 77.45735922789497,
    "backtrans_bertscore": 0.9741689562797546,
    "prof_backtrans_bleu": 31.474003753826338,
    "prof_backtrans_chrf": 61.91360799183811,
    "prof_backtrans_bertscore": 0.9406622052192688,
    "prof_backtrans_labse": 0.9709137678146362,
    "prof_backtrans_xlm_roberta": 0.8272857069969177,
    "llm_vs_prof_backtrans_bleu": 39.91472447363036,
    "llm_vs_prof_backtrans_chrf": 64.15339692182347,
    "llm_vs_prof_backtrans_bertscore": 0.9447908997535706,
    "llm_vs_prof_backtrans_labse": 0.9697157740592957
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 52.46465244900748,
    "same_lang_chrf": 67.51218244157864,
    "same_lang_bertscore": 0.8641260862350464,
    "same_lang_comet": 0.9188444018363953,
    "cross_lang_xlm_roberta": 0.9852792024612427,
    "cross_lang_labse": 0.9944292902946472,
    "cross_lang_mbert": 0.9903765320777893,
    "cross_lang_comet_qe": 0.496253103017807,
    "backtrans_bleu": 62.70463662921044,
    "backtrans_chrf": 80.10569208597936,
    "backtrans_bertscore": 0.9745451807975769,
    "prof_backtrans_bleu": 54.0339366338098,
    "prof_backtrans_chrf": 75.43484572031466,
    "prof_backtrans_bertscore": 0.9646844863891602,
    "prof_backtrans_labse": 0.9796772599220276,
    "prof_backtrans_xlm_roberta": 0.9044116139411926,
    "llm_vs_prof_backtrans_bleu": 63.509124775420325,
    "llm_vs_prof_backtrans_chrf": 77.37005483723296,
    "llm_vs_prof_backtrans_bertscore": 0.9625608921051025,
    "llm_vs_prof_backtrans_labse": 0.9767586588859558
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 37.957560245903764,
    "same_lang_chrf": 71.43576028302085,
    "same_lang_bertscore": 0.8234690427780151,
    "same_lang_comet": 0.9078561067581177,
    "cross_lang_xlm_roberta": 0.9934130311012268,
    "cross_lang_labse": 0.9934555292129517,
    "cross_lang_mbert": 0.9938227534294128,
    "cross_lang_comet_qe": 0.47881320118904114,
    "backtrans_bleu": 70.84518296131084,
    "backtrans_chrf": 85.21848926677808,
    "backtrans_bertscore": 0.9793200492858887,
    "prof_backtrans_bleu": 59.87268291073072,
    "prof_backtrans_chrf": 78.41946355340875,
    "prof_backtrans_bertscore": 0.9687076807022095,
    "prof_backtrans_labse": 0.9883498549461365,
    "prof_backtrans_xlm_roberta": 0.9257997274398804,
    "llm_vs_prof_backtrans_bleu": 64.74878527234428,
    "llm_vs_prof_backtrans_chrf": 79.03958856204926,
    "llm_vs_prof_backtrans_bertscore": 0.9631991386413574,
    "llm_vs_prof_backtrans_labse": 0.9848679900169373
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 33.774144627989855,
    "same_lang_chrf": 65.02550874850574,
    "same_lang_bertscore": 0.8666646480560303,
    "same_lang_comet": 0.9430195093154907,
    "cross_lang_xlm_roberta": 0.9779566526412964,
    "cross_lang_labse": 0.9903749823570251,
    "cross_lang_mbert": 0.9570973515510559,
    "cross_lang_comet_qe": 0.4721676707267761,
    "backtrans_bleu": 50.725602869012484,
    "backtrans_chrf": 72.17767312988579,
    "backtrans_bertscore": 0.9659376740455627,
    "prof_backtrans_bleu": 35.7414862233537,
    "prof_backtrans_chrf": 64.34199319260884,
    "prof_backtrans_bertscore": 0.948175847530365,
    "prof_backtrans_labse": 0.9773605465888977,
    "prof_backtrans_xlm_roberta": 0.9201224446296692,
    "llm_vs_prof_backtrans_bleu": 47.645514826832574,
    "llm_vs_prof_backtrans_chrf": 71.4872666570491,
    "llm_vs_prof_backtrans_bertscore": 0.9546471238136292,
    "llm_vs_prof_backtrans_labse": 0.9803683757781982
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 21.396958249748884,
    "same_lang_chrf": 44.52364238556501,
    "same_lang_bertscore": 0.8344610333442688,
    "same_lang_comet": 0.9418267011642456,
    "cross_lang_xlm_roberta": 0.9775792360305786,
    "cross_lang_labse": 0.9872788190841675,
    "cross_lang_mbert": 0.9880979061126709,
    "cross_lang_comet_qe": 0.4455205500125885,
    "backtrans_bleu": 50.48368797385547,
    "backtrans_chrf": 73.5112018052332,
    "backtrans_bertscore": 0.962552011013031,
    "prof_backtrans_bleu": 49.621238074678416,
    "prof_backtrans_chrf": 73.70564704379105,
    "prof_backtrans_bertscore": 0.9559577107429504,
    "prof_backtrans_labse": 0.9817082285881042,
    "prof_backtrans_xlm_roberta": 0.8718571066856384,
    "llm_vs_prof_backtrans_bleu": 54.22984535316696,
    "llm_vs_prof_backtrans_chrf": 72.81157373577518,
    "llm_vs_prof_backtrans_bertscore": 0.9535706639289856,
    "llm_vs_prof_backtrans_labse": 0.9782121777534485
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 39.92807206700772,
    "same_lang_chrf": 62.22570472280209,
    "same_lang_bertscore": 0.8418025970458984,
    "same_lang_comet": 0.8345392942428589,
    "cross_lang_xlm_roberta": 0.9923897981643677,
    "cross_lang_labse": 0.996377170085907,
    "cross_lang_mbert": 0.9958993792533875,
    "cross_lang_comet_qe": 0.43094396591186523,
    "backtrans_bleu": 70.80205224443141,
    "backtrans_chrf": 84.86467257247736,
    "backtrans_bertscore": 0.9814127683639526,
    "prof_backtrans_bleu": 46.16007712229916,
    "prof_backtrans_chrf": 73.81298481120662,
    "prof_backtrans_bertscore": 0.9342324137687683,
    "prof_backtrans_labse": 0.9523930549621582,
    "prof_backtrans_xlm_roberta": 0.8462544083595276,
    "llm_vs_prof_backtrans_bleu": 51.49248622487905,
    "llm_vs_prof_backtrans_chrf": 70.76499351246798,
    "llm_vs_prof_backtrans_bertscore": 0.9374071955680847,
    "llm_vs_prof_backtrans_labse": 0.9535953402519226
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 54.102530341114175,
    "same_lang_chrf": 74.71804675531719,
    "same_lang_bertscore": 0.8915411829948425,
    "same_lang_comet": 0.9176143407821655,
    "cross_lang_xlm_roberta": 0.9863314628601074,
    "cross_lang_labse": 0.9938495755195618,
    "cross_lang_mbert": 0.9922943711280823,
    "cross_lang_comet_qe": 0.4763849973678589,
    "backtrans_bleu": 68.84862862625006,
    "backtrans_chrf": 83.49882070441168,
    "backtrans_bertscore": 0.9803210496902466,
    "prof_backtrans_bleu": 50.49526965429532,
    "prof_backtrans_chrf": 73.9840005681086,
    "prof_backtrans_bertscore": 0.9511750340461731,
    "prof_backtrans_labse": 0.9749953746795654,
    "prof_backtrans_xlm_roberta": 0.9348543286323547,
    "llm_vs_prof_backtrans_bleu": 61.89881438804961,
    "llm_vs_prof_backtrans_chrf": 76.49671954604662,
    "llm_vs_prof_backtrans_bertscore": 0.9556821584701538,
    "llm_vs_prof_backtrans_labse": 0.9787020683288574
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 19.03216479897088,
    "same_lang_chrf": 40.238543474763524,
    "same_lang_bertscore": 0.888853132724762,
    "same_lang_comet": 0.925098180770874,
    "cross_lang_xlm_roberta": 0.9825345873832703,
    "cross_lang_labse": 0.9922050833702087,
    "cross_lang_mbert": 0.9876089096069336,
    "cross_lang_comet_qe": 0.4450923502445221,
    "backtrans_bleu": 48.236774721960394,
    "backtrans_chrf": 71.33893845184437,
    "backtrans_bertscore": 0.965994119644165,
    "prof_backtrans_bleu": 26.689328956470124,
    "prof_backtrans_chrf": 58.497257434602446,
    "prof_backtrans_bertscore": 0.9306821227073669,
    "prof_backtrans_labse": 0.9639281630516052,
    "prof_backtrans_xlm_roberta": 0.8219401240348816,
    "llm_vs_prof_backtrans_bleu": 39.44876635976366,
    "llm_vs_prof_backtrans_chrf": 62.680951994307144,
    "llm_vs_prof_backtrans_bertscore": 0.9355869293212891,
    "llm_vs_prof_backtrans_labse": 0.9718334674835205
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 58.66966033354248,
    "same_lang_chrf": 71.69950033233623,
    "same_lang_bertscore": 0.9064335823059082,
    "same_lang_comet": 0.93808913230896,
    "cross_lang_xlm_roberta": 0.9800200462341309,
    "cross_lang_labse": 0.9931464195251465,
    "cross_lang_mbert": 0.983155369758606,
    "cross_lang_comet_qe": 0.43004125356674194,
    "backtrans_bleu": 52.392738008840006,
    "backtrans_chrf": 73.38395889196818,
    "backtrans_bertscore": 0.9683931469917297,
    "prof_backtrans_bleu": 47.74441093268974,
    "prof_backtrans_chrf": 71.66894720173745,
    "prof_backtrans_bertscore": 0.957650899887085,
    "prof_backtrans_labse": 0.9776294827461243,
    "prof_backtrans_xlm_roberta": 0.8858786225318909,
    "llm_vs_prof_backtrans_bleu": 59.78022726512039,
    "llm_vs_prof_backtrans_chrf": 76.42901048605167,
    "llm_vs_prof_backtrans_bertscore": 0.9625139832496643,
    "llm_vs_prof_backtrans_labse": 0.9803175330162048
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 43.1096092745398,
    "same_lang_chrf": 73.70446851128148,
    "same_lang_bertscore": 0.8534413576126099,
    "same_lang_comet": 0.906630277633667,
    "cross_lang_xlm_roberta": 0.9837418794631958,
    "cross_lang_labse": 0.9967096447944641,
    "cross_lang_mbert": 0.9922131299972534,
    "cross_lang_comet_qe": 0.4079119563102722,
    "backtrans_bleu": 65.88463364226355,
    "backtrans_chrf": 82.3102382261344,
    "backtrans_bertscore": 0.9794668555259705,
    "prof_backtrans_bleu": 55.25877131846674,
    "prof_backtrans_chrf": 76.25257145819069,
    "prof_backtrans_bertscore": 0.9625231623649597,
    "prof_backtrans_labse": 0.9853874444961548,
    "prof_backtrans_xlm_roberta": 0.9122012853622437,
    "llm_vs_prof_backtrans_bleu": 62.78112460508946,
    "llm_vs_prof_backtrans_chrf": 78.16088449408805,
    "llm_vs_prof_backtrans_bertscore": 0.9646857380867004,
    "llm_vs_prof_backtrans_labse": 0.984632670879364
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 36.17702320042627,
    "same_lang_chrf": 68.46026303324481,
    "same_lang_bertscore": 0.8853580355644226,
    "same_lang_comet": 0.9435664415359497,
    "cross_lang_xlm_roberta": 0.9698546528816223,
    "cross_lang_labse": 0.9833247661590576,
    "cross_lang_mbert": 0.9646105170249939,
    "cross_lang_comet_qe": 0.4107198417186737,
    "backtrans_bleu": 43.24727149917014,
    "backtrans_chrf": 67.9696597218453,
    "backtrans_bertscore": 0.951204240322113,
    "prof_backtrans_bleu": 26.759154391197782,
    "prof_backtrans_chrf": 57.87401914843194,
    "prof_backtrans_bertscore": 0.9308041334152222,
    "prof_backtrans_labse": 0.9673805832862854,
    "prof_backtrans_xlm_roberta": 0.8902627825737,
    "llm_vs_prof_backtrans_bleu": 45.56920315275465,
    "llm_vs_prof_backtrans_chrf": 69.76843975000516,
    "llm_vs_prof_backtrans_bertscore": 0.949603796005249,
    "llm_vs_prof_backtrans_labse": 0.9744805693626404
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 19.592124138769712,
    "same_lang_chrf": 44.22619390513719,
    "same_lang_bertscore": 0.8338454961776733,
    "same_lang_comet": 0.9415229558944702,
    "cross_lang_xlm_roberta": 0.9658163189888,
    "cross_lang_labse": 0.9827178716659546,
    "cross_lang_mbert": 0.9762521982192993,
    "cross_lang_comet_qe": 0.3646923303604126,
    "backtrans_bleu": 36.349466732183046,
    "backtrans_chrf": 65.6407546354172,
    "backtrans_bertscore": 0.951255202293396,
    "prof_backtrans_bleu": 34.66636520440901,
    "prof_backtrans_chrf": 65.23697767280896,
    "prof_backtrans_bertscore": 0.9320223331451416,
    "prof_backtrans_labse": 0.9668136239051819,
    "prof_backtrans_xlm_roberta": 0.8573075532913208,
    "llm_vs_prof_backtrans_bleu": 39.584609980246746,
    "llm_vs_prof_backtrans_chrf": 64.374819595611,
    "llm_vs_prof_backtrans_bertscore": 0.9298790693283081,
    "llm_vs_prof_backtrans_labse": 0.9624108076095581
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 37.646515283174395,
    "same_lang_chrf": 61.31884429510788,
    "same_lang_bertscore": 0.8445731997489929,
    "same_lang_comet": 0.8049640655517578,
    "cross_lang_xlm_roberta": 0.9865724444389343,
    "cross_lang_labse": 0.948717474937439,
    "cross_lang_mbert": 0.9784813523292542,
    "cross_lang_comet_qe": 0.3285648822784424,
    "backtrans_bleu": 57.66984097640735,
    "backtrans_chrf": 79.59103417634617,
    "backtrans_bertscore": 0.9571729302406311,
    "prof_backtrans_bleu": 40.59208973343827,
    "prof_backtrans_chrf": 70.36415933254987,
    "prof_backtrans_bertscore": 0.9140322804450989,
    "prof_backtrans_labse": 0.9419952034950256,
    "prof_backtrans_xlm_roberta": 0.8319346308708191,
    "llm_vs_prof_backtrans_bleu": 46.65399786112944,
    "llm_vs_prof_backtrans_chrf": 68.0965469558922,
    "llm_vs_prof_backtrans_bertscore": 0.9208720326423645,
    "llm_vs_prof_backtrans_labse": 0.9174586534500122
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 46.26518076627523,
    "same_lang_chrf": 72.43806301977592,
    "same_lang_bertscore": 0.8855744004249573,
    "same_lang_comet": 0.922035813331604,
    "cross_lang_xlm_roberta": 0.9851167798042297,
    "cross_lang_labse": 0.9885573983192444,
    "cross_lang_mbert": 0.9905694127082825,
    "cross_lang_comet_qe": 0.44652998447418213,
    "backtrans_bleu": 68.33741029597225,
    "backtrans_chrf": 83.68530884101484,
    "backtrans_bertscore": 0.9743019938468933,
    "prof_backtrans_bleu": 42.75774031832745,
    "prof_backtrans_chrf": 71.04901845118802,
    "prof_backtrans_bertscore": 0.9431707859039307,
    "prof_backtrans_labse": 0.9773756265640259,
    "prof_backtrans_xlm_roberta": 0.9332188367843628,
    "llm_vs_prof_backtrans_bleu": 54.50451392874785,
    "llm_vs_prof_backtrans_chrf": 74.12763740101698,
    "llm_vs_prof_backtrans_bertscore": 0.9515343904495239,
    "llm_vs_prof_backtrans_labse": 0.9833461046218872
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 23.706944758043498,
    "same_lang_chrf": 38.738040394501645,
    "same_lang_bertscore": 0.8783812522888184,
    "same_lang_comet": 0.918444037437439,
    "cross_lang_xlm_roberta": 0.9591448903083801,
    "cross_lang_labse": 0.9903409481048584,
    "cross_lang_mbert": 0.9402185082435608,
    "cross_lang_comet_qe": 0.4390220642089844,
    "backtrans_bleu": 45.56427015196494,
    "backtrans_chrf": 71.38783138780666,
    "backtrans_bertscore": 0.9634355306625366,
    "prof_backtrans_bleu": 25.344445859103264,
    "prof_backtrans_chrf": 57.921264767393986,
    "prof_backtrans_bertscore": 0.9280168414115906,
    "prof_backtrans_labse": 0.9633558988571167,
    "prof_backtrans_xlm_roberta": 0.8298572897911072,
    "llm_vs_prof_backtrans_bleu": 37.864286109703976,
    "llm_vs_prof_backtrans_chrf": 64.22272079144972,
    "llm_vs_prof_backtrans_bertscore": 0.9360519051551819,
    "llm_vs_prof_backtrans_labse": 0.9714525938034058
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 49.84148912072019,
    "same_lang_chrf": 64.66301849787457,
    "same_lang_bertscore": 0.8652119636535645,
    "same_lang_comet": 0.9255677461624146,
    "cross_lang_xlm_roberta": 0.9751104712486267,
    "cross_lang_labse": 0.9866586923599243,
    "cross_lang_mbert": 0.9845204949378967,
    "cross_lang_comet_qe": 0.3481420874595642,
    "backtrans_bleu": 46.36204013137278,
    "backtrans_chrf": 70.32717425408688,
    "backtrans_bertscore": 0.9586925506591797,
    "prof_backtrans_bleu": 43.63390053518816,
    "prof_backtrans_chrf": 68.92900924634306,
    "prof_backtrans_bertscore": 0.9542315006256104,
    "prof_backtrans_labse": 0.9793418049812317,
    "prof_backtrans_xlm_roberta": 0.8843298554420471,
    "llm_vs_prof_backtrans_bleu": 58.93492423745936,
    "llm_vs_prof_backtrans_chrf": 76.00204167325153,
    "llm_vs_prof_backtrans_bertscore": 0.9584445357322693,
    "llm_vs_prof_backtrans_labse": 0.975534200668335
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 36.392581298107324,
    "same_lang_chrf": 71.4227716188881,
    "same_lang_bertscore": 0.8318418860435486,
    "same_lang_comet": 0.9023234844207764,
    "cross_lang_xlm_roberta": 0.9600250124931335,
    "cross_lang_labse": 0.9854826331138611,
    "cross_lang_mbert": 0.9493581652641296,
    "cross_lang_comet_qe": 0.25435900688171387,
    "backtrans_bleu": 47.74726322609745,
    "backtrans_chrf": 72.26030989851084,
    "backtrans_bertscore": 0.9613153338432312,
    "prof_backtrans_bleu": 57.72124195587144,
    "prof_backtrans_chrf": 76.93766501431635,
    "prof_backtrans_bertscore": 0.9681926965713501,
    "prof_backtrans_labse": 0.9880278706550598,
    "prof_backtrans_xlm_roberta": 0.9256836771965027,
    "llm_vs_prof_backtrans_bleu": 52.90410918782034,
    "llm_vs_prof_backtrans_chrf": 74.05006279397708,
    "llm_vs_prof_backtrans_bertscore": 0.9563440084457397,
    "llm_vs_prof_backtrans_labse": 0.9816271066665649
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 29.614910910773936,
    "same_lang_chrf": 61.47129560814154,
    "same_lang_bertscore": 0.8576908707618713,
    "same_lang_comet": 0.9339730739593506,
    "cross_lang_xlm_roberta": 0.9769397974014282,
    "cross_lang_labse": 0.9925299286842346,
    "cross_lang_mbert": 0.9831376075744629,
    "cross_lang_comet_qe": 0.4240192174911499,
    "backtrans_bleu": 51.52090124947634,
    "backtrans_chrf": 72.66855536471598,
    "backtrans_bertscore": 0.9675004482269287,
    "prof_backtrans_bleu": 27.3987922473753,
    "prof_backtrans_chrf": 58.24931580489368,
    "prof_backtrans_bertscore": 0.9312629103660583,
    "prof_backtrans_labse": 0.9715819358825684,
    "prof_backtrans_xlm_roberta": 0.9101117253303528,
    "llm_vs_prof_backtrans_bleu": 40.95936541991621,
    "llm_vs_prof_backtrans_chrf": 65.37102975213392,
    "llm_vs_prof_backtrans_bertscore": 0.9410476088523865,
    "llm_vs_prof_backtrans_labse": 0.9761562943458557
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 20.564171172011783,
    "same_lang_chrf": 44.720046792990054,
    "same_lang_bertscore": 0.8288170695304871,
    "same_lang_comet": 0.9373396635055542,
    "cross_lang_xlm_roberta": 0.976188600063324,
    "cross_lang_labse": 0.9820471405982971,
    "cross_lang_mbert": 0.9815801382064819,
    "cross_lang_comet_qe": 0.4572976231575012,
    "backtrans_bleu": 37.46178000577275,
    "backtrans_chrf": 65.83692990356013,
    "backtrans_bertscore": 0.9527987837791443,
    "prof_backtrans_bleu": 42.12195929449022,
    "prof_backtrans_chrf": 69.64846681598404,
    "prof_backtrans_bertscore": 0.9491099119186401,
    "prof_backtrans_labse": 0.9749977588653564,
    "prof_backtrans_xlm_roberta": 0.8691092133522034,
    "llm_vs_prof_backtrans_bleu": 48.48760345237685,
    "llm_vs_prof_backtrans_chrf": 70.61159716559996,
    "llm_vs_prof_backtrans_bertscore": 0.9484622478485107,
    "llm_vs_prof_backtrans_labse": 0.9640309810638428
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 31.769698676673436,
    "same_lang_chrf": 57.51113794319333,
    "same_lang_bertscore": 0.8330951929092407,
    "same_lang_comet": 0.8160660266876221,
    "cross_lang_xlm_roberta": 0.9776055216789246,
    "cross_lang_labse": 0.9890093207359314,
    "cross_lang_mbert": 0.9801314473152161,
    "cross_lang_comet_qe": 0.2014089822769165,
    "backtrans_bleu": 60.751739190111266,
    "backtrans_chrf": 78.46959963836917,
    "backtrans_bertscore": 0.9656758308410645,
    "prof_backtrans_bleu": 41.49321271701007,
    "prof_backtrans_chrf": 70.6931058624378,
    "prof_backtrans_bertscore": 0.924111008644104,
    "prof_backtrans_labse": 0.948962926864624,
    "prof_backtrans_xlm_roberta": 0.8377172946929932,
    "llm_vs_prof_backtrans_bleu": 41.98002607169202,
    "llm_vs_prof_backtrans_chrf": 65.9215385052448,
    "llm_vs_prof_backtrans_bertscore": 0.9222962260246277,
    "llm_vs_prof_backtrans_labse": 0.9528571367263794
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 53.04728162003014,
    "same_lang_chrf": 73.04937781800108,
    "same_lang_bertscore": 0.8758006691932678,
    "same_lang_comet": 0.8975743055343628,
    "cross_lang_xlm_roberta": 0.9988808631896973,
    "cross_lang_labse": 0.997475802898407,
    "cross_lang_mbert": 0.9940583109855652,
    "cross_lang_comet_qe": 0.48426374793052673,
    "backtrans_bleu": 79.18299885040396,
    "backtrans_chrf": 89.15217327197729,
    "backtrans_bertscore": 0.9823169112205505,
    "prof_backtrans_bleu": 56.5081487283398,
    "prof_backtrans_chrf": 78.88613249736687,
    "prof_backtrans_bertscore": 0.9402646422386169,
    "prof_backtrans_labse": 0.9711060523986816,
    "prof_backtrans_xlm_roberta": 0.9533528685569763,
    "llm_vs_prof_backtrans_bleu": 64.42146801333558,
    "llm_vs_prof_backtrans_chrf": 77.56822288705638,
    "llm_vs_prof_backtrans_bertscore": 0.9473479390144348,
    "llm_vs_prof_backtrans_labse": 0.97092205286026
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 18.004784348249363,
    "same_lang_chrf": 40.12715581917682,
    "same_lang_bertscore": 0.8758706450462341,
    "same_lang_comet": 0.9288111925125122,
    "cross_lang_xlm_roberta": 0.9901947975158691,
    "cross_lang_labse": 0.9858180284500122,
    "cross_lang_mbert": 0.9905041456222534,
    "cross_lang_comet_qe": 0.4793773889541626,
    "backtrans_bleu": 52.84778173689372,
    "backtrans_chrf": 78.10845170448067,
    "backtrans_bertscore": 0.9555315375328064,
    "prof_backtrans_bleu": 50.8153884457305,
    "prof_backtrans_chrf": 75.2000386868063,
    "prof_backtrans_bertscore": 0.9594358801841736,
    "prof_backtrans_labse": 0.9874376058578491,
    "prof_backtrans_xlm_roberta": 0.8876351714134216,
    "llm_vs_prof_backtrans_bleu": 55.4403322499673,
    "llm_vs_prof_backtrans_chrf": 76.96325908235823,
    "llm_vs_prof_backtrans_bertscore": 0.9544079899787903,
    "llm_vs_prof_backtrans_labse": 0.9782226085662842
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 50.10785483968077,
    "same_lang_chrf": 67.00128208811276,
    "same_lang_bertscore": 0.8667135238647461,
    "same_lang_comet": 0.9189302921295166,
    "cross_lang_xlm_roberta": 0.9922081232070923,
    "cross_lang_labse": 0.9898692965507507,
    "cross_lang_mbert": 0.9900325536727905,
    "cross_lang_comet_qe": 0.4856661260128021,
    "backtrans_bleu": 69.01776139822574,
    "backtrans_chrf": 85.04582489578306,
    "backtrans_bertscore": 0.9695168137550354,
    "prof_backtrans_bleu": 53.43381117872461,
    "prof_backtrans_chrf": 77.15820409708543,
    "prof_backtrans_bertscore": 0.9438400864601135,
    "prof_backtrans_labse": 0.9716396331787109,
    "prof_backtrans_xlm_roberta": 0.9234374761581421,
    "llm_vs_prof_backtrans_bleu": 59.31434228850029,
    "llm_vs_prof_backtrans_chrf": 77.24195523116784,
    "llm_vs_prof_backtrans_bertscore": 0.9545484781265259,
    "llm_vs_prof_backtrans_labse": 0.9717811346054077
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 41.319211696886875,
    "same_lang_chrf": 72.53383808056144,
    "same_lang_bertscore": 0.8319277167320251,
    "same_lang_comet": 0.9114128351211548,
    "cross_lang_xlm_roberta": 0.994909942150116,
    "cross_lang_labse": 0.9922218322753906,
    "cross_lang_mbert": 0.9767706394195557,
    "cross_lang_comet_qe": 0.4838106334209442,
    "backtrans_bleu": 71.55686208607185,
    "backtrans_chrf": 86.23527558223111,
    "backtrans_bertscore": 0.9730401635169983,
    "prof_backtrans_bleu": 68.81501530489368,
    "prof_backtrans_chrf": 84.3495857577864,
    "prof_backtrans_bertscore": 0.9557276964187622,
    "prof_backtrans_labse": 0.9790586829185486,
    "prof_backtrans_xlm_roberta": 0.9255160093307495,
    "llm_vs_prof_backtrans_bleu": 72.2023015752075,
    "llm_vs_prof_backtrans_chrf": 84.69068253272832,
    "llm_vs_prof_backtrans_bertscore": 0.9578041434288025,
    "llm_vs_prof_backtrans_labse": 0.9787707924842834
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 23.917848547371758,
    "same_lang_chrf": 62.2359820810668,
    "same_lang_bertscore": 0.8480075001716614,
    "same_lang_comet": 0.9142171144485474,
    "cross_lang_xlm_roberta": 0.98420649766922,
    "cross_lang_labse": 0.9929645657539368,
    "cross_lang_mbert": 0.9882827401161194,
    "cross_lang_comet_qe": 0.4856674075126648,
    "backtrans_bleu": 64.6731107485449,
    "backtrans_chrf": 82.0298809147197,
    "backtrans_bertscore": 0.970431387424469,
    "prof_backtrans_bleu": 39.906966720987405,
    "prof_backtrans_chrf": 69.45385372387352,
    "prof_backtrans_bertscore": 0.9249563217163086,
    "prof_backtrans_labse": 0.9401988983154297,
    "prof_backtrans_xlm_roberta": 0.9572447538375854,
    "llm_vs_prof_backtrans_bleu": 48.98408717146421,
    "llm_vs_prof_backtrans_chrf": 71.78565193735909,
    "llm_vs_prof_backtrans_bertscore": 0.9353860020637512,
    "llm_vs_prof_backtrans_labse": 0.9474349021911621
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 19.239140761105332,
    "same_lang_chrf": 42.58404432035724,
    "same_lang_bertscore": 0.8451409935951233,
    "same_lang_comet": 0.921830415725708,
    "cross_lang_xlm_roberta": 0.9794172644615173,
    "cross_lang_labse": 0.9735329151153564,
    "cross_lang_mbert": 0.9765695333480835,
    "cross_lang_comet_qe": 0.4595198929309845,
    "backtrans_bleu": 49.229762851859114,
    "backtrans_chrf": 74.55126953337641,
    "backtrans_bertscore": 0.9506723284721375,
    "prof_backtrans_bleu": 57.48362895192731,
    "prof_backtrans_chrf": 80.00652843079509,
    "prof_backtrans_bertscore": 0.9513702988624573,
    "prof_backtrans_labse": 0.9778167605400085,
    "prof_backtrans_xlm_roberta": 0.8941755890846252,
    "llm_vs_prof_backtrans_bleu": 52.90498954533076,
    "llm_vs_prof_backtrans_chrf": 75.13788195881662,
    "llm_vs_prof_backtrans_bertscore": 0.9504386782646179,
    "llm_vs_prof_backtrans_labse": 0.962020993232727
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 41.68985121352693,
    "same_lang_chrf": 67.41085981775755,
    "same_lang_bertscore": 0.8681514263153076,
    "same_lang_comet": 0.8258723020553589,
    "cross_lang_xlm_roberta": 0.9984123706817627,
    "cross_lang_labse": 0.9907112121582031,
    "cross_lang_mbert": 0.9698255062103271,
    "cross_lang_comet_qe": 0.4038386642932892,
    "backtrans_bleu": 64.57057199518877,
    "backtrans_chrf": 83.164124108232,
    "backtrans_bertscore": 0.9634004831314087,
    "prof_backtrans_bleu": 52.931357659545675,
    "prof_backtrans_chrf": 77.03202291159285,
    "prof_backtrans_bertscore": 0.9490620493888855,
    "prof_backtrans_labse": 0.9758405685424805,
    "prof_backtrans_xlm_roberta": 0.883039116859436,
    "llm_vs_prof_backtrans_bleu": 58.36695308186058,
    "llm_vs_prof_backtrans_chrf": 76.27841454503313,
    "llm_vs_prof_backtrans_bertscore": 0.9575490355491638,
    "llm_vs_prof_backtrans_labse": 0.9735464453697205
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 51.335091348808355,
    "same_lang_chrf": 73.36596052791428,
    "same_lang_bertscore": 0.8797348737716675,
    "same_lang_comet": 0.8989380598068237,
    "cross_lang_xlm_roberta": 0.9986001253128052,
    "cross_lang_labse": 0.9955347180366516,
    "cross_lang_mbert": 0.9604933857917786,
    "cross_lang_comet_qe": 0.46045273542404175,
    "backtrans_bleu": 81.2516842486625,
    "backtrans_chrf": 89.73677731599976,
    "backtrans_bertscore": 0.9857702255249023,
    "prof_backtrans_bleu": 53.0128622213268,
    "prof_backtrans_chrf": 76.38176896685322,
    "prof_backtrans_bertscore": 0.9422135353088379,
    "prof_backtrans_labse": 0.9688906669616699,
    "prof_backtrans_xlm_roberta": 0.948092520236969,
    "llm_vs_prof_backtrans_bleu": 58.212029865864295,
    "llm_vs_prof_backtrans_chrf": 75.24695114290748,
    "llm_vs_prof_backtrans_bertscore": 0.9461042881011963,
    "llm_vs_prof_backtrans_labse": 0.9669546484947205
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 12.614638887936852,
    "same_lang_chrf": 44.62378643812837,
    "same_lang_bertscore": 0.9009387493133545,
    "same_lang_comet": 0.9278740882873535,
    "cross_lang_xlm_roberta": 0.9878774285316467,
    "cross_lang_labse": 0.989495575428009,
    "cross_lang_mbert": 0.9939461350440979,
    "cross_lang_comet_qe": 0.4664415717124939,
    "backtrans_bleu": 62.6073292163235,
    "backtrans_chrf": 79.97788583605009,
    "backtrans_bertscore": 0.9742213487625122,
    "prof_backtrans_bleu": 46.333059493622265,
    "prof_backtrans_chrf": 73.852567553576,
    "prof_backtrans_bertscore": 0.9561218619346619,
    "prof_backtrans_labse": 0.9840251803398132,
    "prof_backtrans_xlm_roberta": 0.8700290322303772,
    "llm_vs_prof_backtrans_bleu": 55.06272883037362,
    "llm_vs_prof_backtrans_chrf": 75.36864616037478,
    "llm_vs_prof_backtrans_bertscore": 0.9615665078163147,
    "llm_vs_prof_backtrans_labse": 0.9804529547691345
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 55.461384029917205,
    "same_lang_chrf": 70.99841949061886,
    "same_lang_bertscore": 0.8818807005882263,
    "same_lang_comet": 0.9177038669586182,
    "cross_lang_xlm_roberta": 0.9862298369407654,
    "cross_lang_labse": 0.991880476474762,
    "cross_lang_mbert": 0.9882727861404419,
    "cross_lang_comet_qe": 0.45978543162345886,
    "backtrans_bleu": 70.36703006122761,
    "backtrans_chrf": 84.9071871026986,
    "backtrans_bertscore": 0.9714058637619019,
    "prof_backtrans_bleu": 57.61497078148028,
    "prof_backtrans_chrf": 78.35546093572047,
    "prof_backtrans_bertscore": 0.9495323896408081,
    "prof_backtrans_labse": 0.9788575172424316,
    "prof_backtrans_xlm_roberta": 0.9293714165687561,
    "llm_vs_prof_backtrans_bleu": 64.84816711670064,
    "llm_vs_prof_backtrans_chrf": 79.45375687090619,
    "llm_vs_prof_backtrans_bertscore": 0.9599151015281677,
    "llm_vs_prof_backtrans_labse": 0.9795785546302795
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 46.962756507277604,
    "same_lang_chrf": 76.78475980773639,
    "same_lang_bertscore": 0.8480228185653687,
    "same_lang_comet": 0.904812216758728,
    "cross_lang_xlm_roberta": 0.993888795375824,
    "cross_lang_labse": 0.9965185523033142,
    "cross_lang_mbert": 0.9967430830001831,
    "cross_lang_comet_qe": 0.4848361611366272,
    "backtrans_bleu": 78.3424390705114,
    "backtrans_chrf": 88.34955757306615,
    "backtrans_bertscore": 0.9863536357879639,
    "prof_backtrans_bleu": 67.98644571006336,
    "prof_backtrans_chrf": 84.0445993974013,
    "prof_backtrans_bertscore": 0.9595513343811035,
    "prof_backtrans_labse": 0.9780832529067993,
    "prof_backtrans_xlm_roberta": 0.9331585168838501,
    "llm_vs_prof_backtrans_bleu": 71.68109276669341,
    "llm_vs_prof_backtrans_chrf": 84.3555074791924,
    "llm_vs_prof_backtrans_bertscore": 0.9638441801071167,
    "llm_vs_prof_backtrans_labse": 0.9811490178108215
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 30.301196282557772,
    "same_lang_chrf": 65.95877238466375,
    "same_lang_bertscore": 0.875088632106781,
    "same_lang_comet": 0.9234076738357544,
    "cross_lang_xlm_roberta": 0.992473840713501,
    "cross_lang_labse": 0.9909073114395142,
    "cross_lang_mbert": 0.93897944688797,
    "cross_lang_comet_qe": 0.46257635951042175,
    "backtrans_bleu": 63.02763105546456,
    "backtrans_chrf": 80.41284026624632,
    "backtrans_bertscore": 0.9719738364219666,
    "prof_backtrans_bleu": 38.68770891865691,
    "prof_backtrans_chrf": 67.39753403385006,
    "prof_backtrans_bertscore": 0.9376305341720581,
    "prof_backtrans_labse": 0.9491574764251709,
    "prof_backtrans_xlm_roberta": 0.95920330286026,
    "llm_vs_prof_backtrans_bleu": 49.22967392729793,
    "llm_vs_prof_backtrans_chrf": 72.507383628973,
    "llm_vs_prof_backtrans_bertscore": 0.9488611817359924,
    "llm_vs_prof_backtrans_labse": 0.9611343741416931
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 23.813933131193362,
    "same_lang_chrf": 45.423664061506656,
    "same_lang_bertscore": 0.833581805229187,
    "same_lang_comet": 0.9208791255950928,
    "cross_lang_xlm_roberta": 0.9662192463874817,
    "cross_lang_labse": 0.9679239392280579,
    "cross_lang_mbert": 0.9827407002449036,
    "cross_lang_comet_qe": 0.4791862666606903,
    "backtrans_bleu": 57.81540729586924,
    "backtrans_chrf": 78.00801954955688,
    "backtrans_bertscore": 0.9588100910186768,
    "prof_backtrans_bleu": 60.17311356434494,
    "prof_backtrans_chrf": 79.86080597001872,
    "prof_backtrans_bertscore": 0.9560996890068054,
    "prof_backtrans_labse": 0.9761404395103455,
    "prof_backtrans_xlm_roberta": 0.8891856074333191,
    "llm_vs_prof_backtrans_bleu": 56.11040167013429,
    "llm_vs_prof_backtrans_chrf": 75.4578610173965,
    "llm_vs_prof_backtrans_bertscore": 0.9389008283615112,
    "llm_vs_prof_backtrans_labse": 0.9487815499305725
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 49.08081483660688,
    "same_lang_chrf": 70.10526705302605,
    "same_lang_bertscore": 0.8852803111076355,
    "same_lang_comet": 0.8366963863372803,
    "cross_lang_xlm_roberta": 0.9959889054298401,
    "cross_lang_labse": 0.9955207109451294,
    "cross_lang_mbert": 0.9709640741348267,
    "cross_lang_comet_qe": 0.4197840094566345,
    "backtrans_bleu": 65.51123634143562,
    "backtrans_chrf": 82.30191191211523,
    "backtrans_bertscore": 0.9759278893470764,
    "prof_backtrans_bleu": 54.182863021224954,
    "prof_backtrans_chrf": 77.15782381740173,
    "prof_backtrans_bertscore": 0.9525090456008911,
    "prof_backtrans_labse": 0.9761659502983093,
    "prof_backtrans_xlm_roberta": 0.883039116859436,
    "llm_vs_prof_backtrans_bleu": 60.46802195611058,
    "llm_vs_prof_backtrans_chrf": 77.05709897960205,
    "llm_vs_prof_backtrans_bertscore": 0.9598019123077393,
    "llm_vs_prof_backtrans_labse": 0.9743267893791199
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 49.57678001683876,
    "same_lang_chrf": 72.25822392672667,
    "same_lang_bertscore": 0.8721110820770264,
    "same_lang_comet": 0.8999651074409485,
    "cross_lang_xlm_roberta": 0.9988598823547363,
    "cross_lang_labse": 0.9942140579223633,
    "cross_lang_mbert": 0.9691269397735596,
    "cross_lang_comet_qe": 0.4674339294433594,
    "backtrans_bleu": 76.4201011404198,
    "backtrans_chrf": 90.3039922686797,
    "backtrans_bertscore": 0.9655122756958008,
    "prof_backtrans_bleu": 51.80744351879954,
    "prof_backtrans_chrf": 75.88353216977532,
    "prof_backtrans_bertscore": 0.9405451416969299,
    "prof_backtrans_labse": 0.969670832157135,
    "prof_backtrans_xlm_roberta": 0.9499602913856506,
    "llm_vs_prof_backtrans_bleu": 56.155099242706314,
    "llm_vs_prof_backtrans_chrf": 72.9936742123335,
    "llm_vs_prof_backtrans_bertscore": 0.9403353333473206,
    "llm_vs_prof_backtrans_labse": 0.9685466885566711
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 30.252080690351626,
    "same_lang_chrf": 44.0963971373316,
    "same_lang_bertscore": 0.8843898177146912,
    "same_lang_comet": 0.9273735284805298,
    "cross_lang_xlm_roberta": 0.9881356358528137,
    "cross_lang_labse": 0.9817138314247131,
    "cross_lang_mbert": 0.993748128414154,
    "cross_lang_comet_qe": 0.49210670590400696,
    "backtrans_bleu": 62.359701513171295,
    "backtrans_chrf": 80.76933483801236,
    "backtrans_bertscore": 0.9693871736526489,
    "prof_backtrans_bleu": 42.91554031387368,
    "prof_backtrans_chrf": 71.29678760238444,
    "prof_backtrans_bertscore": 0.9499212503433228,
    "prof_backtrans_labse": 0.9809930324554443,
    "prof_backtrans_xlm_roberta": 0.8734901547431946,
    "llm_vs_prof_backtrans_bleu": 50.118395783169376,
    "llm_vs_prof_backtrans_chrf": 72.09890211056947,
    "llm_vs_prof_backtrans_bertscore": 0.9467665553092957,
    "llm_vs_prof_backtrans_labse": 0.9697208404541016
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 62.07758373368924,
    "same_lang_chrf": 75.71254079491825,
    "same_lang_bertscore": 0.8796261548995972,
    "same_lang_comet": 0.9291002750396729,
    "cross_lang_xlm_roberta": 0.9929460287094116,
    "cross_lang_labse": 0.9843118190765381,
    "cross_lang_mbert": 0.9889956116676331,
    "cross_lang_comet_qe": 0.4383658170700073,
    "backtrans_bleu": 46.8955809120019,
    "backtrans_chrf": 68.27492926358086,
    "backtrans_bertscore": 0.9557751417160034,
    "prof_backtrans_bleu": 54.17259657253895,
    "prof_backtrans_chrf": 78.2139006931304,
    "prof_backtrans_bertscore": 0.9536415934562683,
    "prof_backtrans_labse": 0.9769130945205688,
    "prof_backtrans_xlm_roberta": 0.929606020450592,
    "llm_vs_prof_backtrans_bleu": 46.29447731618882,
    "llm_vs_prof_backtrans_chrf": 66.3750436491948,
    "llm_vs_prof_backtrans_bertscore": 0.9594438672065735,
    "llm_vs_prof_backtrans_labse": 0.9772974252700806
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 38.086083592440566,
    "same_lang_chrf": 73.74086203605854,
    "same_lang_bertscore": 0.8420467972755432,
    "same_lang_comet": 0.8942415714263916,
    "cross_lang_xlm_roberta": 0.9911470413208008,
    "cross_lang_labse": 0.9095723628997803,
    "cross_lang_mbert": 0.9485933184623718,
    "cross_lang_comet_qe": 0.3418596684932709,
    "backtrans_bleu": 52.191594024423715,
    "backtrans_chrf": 79.83496172017931,
    "backtrans_bertscore": 0.9266018867492676,
    "prof_backtrans_bleu": 64.47514709206408,
    "prof_backtrans_chrf": 82.27176949023229,
    "prof_backtrans_bertscore": 0.9571674466133118,
    "prof_backtrans_labse": 0.9839336276054382,
    "prof_backtrans_xlm_roberta": 0.9320310354232788,
    "llm_vs_prof_backtrans_bleu": 51.30587149283127,
    "llm_vs_prof_backtrans_chrf": 78.18925420971807,
    "llm_vs_prof_backtrans_bertscore": 0.9227562546730042,
    "llm_vs_prof_backtrans_labse": 0.9062962532043457
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 31.08059777248931,
    "same_lang_chrf": 66.92925872671375,
    "same_lang_bertscore": 0.8559073209762573,
    "same_lang_comet": 0.9316762089729309,
    "cross_lang_xlm_roberta": 0.9931482076644897,
    "cross_lang_labse": 0.9557098746299744,
    "cross_lang_mbert": 0.9252736568450928,
    "cross_lang_comet_qe": 0.48253133893013,
    "backtrans_bleu": 50.59329796988424,
    "backtrans_chrf": 74.78340261408884,
    "backtrans_bertscore": 0.9462317228317261,
    "prof_backtrans_bleu": 33.50714581444719,
    "prof_backtrans_chrf": 65.73711372195768,
    "prof_backtrans_bertscore": 0.9242090582847595,
    "prof_backtrans_labse": 0.9459822177886963,
    "prof_backtrans_xlm_roberta": 0.960083544254303,
    "llm_vs_prof_backtrans_bleu": 43.117918904705654,
    "llm_vs_prof_backtrans_chrf": 70.13917863762693,
    "llm_vs_prof_backtrans_bertscore": 0.9339779019355774,
    "llm_vs_prof_backtrans_labse": 0.9357225894927979
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 21.907987242131494,
    "same_lang_chrf": 43.592414593477464,
    "same_lang_bertscore": 0.8374475836753845,
    "same_lang_comet": 0.9172201156616211,
    "cross_lang_xlm_roberta": 0.9908702969551086,
    "cross_lang_labse": 0.9487457871437073,
    "cross_lang_mbert": 0.9777565598487854,
    "cross_lang_comet_qe": 0.47985130548477173,
    "backtrans_bleu": 53.6858984454918,
    "backtrans_chrf": 76.66410808956219,
    "backtrans_bertscore": 0.9496667981147766,
    "prof_backtrans_bleu": 52.71511177635165,
    "prof_backtrans_chrf": 76.66776791595571,
    "prof_backtrans_bertscore": 0.9501868486404419,
    "prof_backtrans_labse": 0.9701672196388245,
    "prof_backtrans_xlm_roberta": 0.8868364095687866,
    "llm_vs_prof_backtrans_bleu": 46.831068487260495,
    "llm_vs_prof_backtrans_chrf": 70.97144286441032,
    "llm_vs_prof_backtrans_bertscore": 0.9349769949913025,
    "llm_vs_prof_backtrans_labse": 0.9268603324890137
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 43.574253929555006,
    "same_lang_chrf": 68.17284623797883,
    "same_lang_bertscore": 0.880585789680481,
    "same_lang_comet": 0.8230211734771729,
    "cross_lang_xlm_roberta": 0.9972405433654785,
    "cross_lang_labse": 0.9910265803337097,
    "cross_lang_mbert": 0.9962567687034607,
    "cross_lang_comet_qe": 0.4433615207672119,
    "backtrans_bleu": 56.54320016930462,
    "backtrans_chrf": 79.19658183673029,
    "backtrans_bertscore": 0.9512564539909363,
    "prof_backtrans_bleu": 47.106204316730754,
    "prof_backtrans_chrf": 73.44700363451473,
    "prof_backtrans_bertscore": 0.9387330412864685,
    "prof_backtrans_labse": 0.9768762588500977,
    "prof_backtrans_xlm_roberta": 0.883039116859436,
    "llm_vs_prof_backtrans_bleu": 52.63952397522344,
    "llm_vs_prof_backtrans_chrf": 71.96149926641851,
    "llm_vs_prof_backtrans_bertscore": 0.9401230216026306,
    "llm_vs_prof_backtrans_labse": 0.9723107814788818
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 50.6242202415637,
    "same_lang_chrf": 72.32796299110964,
    "same_lang_bertscore": 0.8705635666847229,
    "same_lang_comet": 0.893207311630249,
    "cross_lang_xlm_roberta": 0.994775652885437,
    "cross_lang_labse": 0.9933475255966187,
    "cross_lang_mbert": 0.9556180238723755,
    "cross_lang_comet_qe": 0.4188385009765625,
    "backtrans_bleu": 68.41794063650556,
    "backtrans_chrf": 82.75326836215994,
    "backtrans_bertscore": 0.9782590270042419,
    "prof_backtrans_bleu": 49.57174661437778,
    "prof_backtrans_chrf": 74.62088918774397,
    "prof_backtrans_bertscore": 0.937046229839325,
    "prof_backtrans_labse": 0.9684234857559204,
    "prof_backtrans_xlm_roberta": 0.9484847187995911,
    "llm_vs_prof_backtrans_bleu": 55.90982504655213,
    "llm_vs_prof_backtrans_chrf": 74.13776283313108,
    "llm_vs_prof_backtrans_bertscore": 0.943298876285553,
    "llm_vs_prof_backtrans_labse": 0.9690237045288086
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 8.028666380871403,
    "same_lang_chrf": 40.73516284477776,
    "same_lang_bertscore": 0.8936720490455627,
    "same_lang_comet": 0.9236630201339722,
    "cross_lang_xlm_roberta": 0.9865165948867798,
    "cross_lang_labse": 0.9589899778366089,
    "cross_lang_mbert": 0.9683168530464172,
    "cross_lang_comet_qe": 0.3440869152545929,
    "backtrans_bleu": 38.51139370283205,
    "backtrans_chrf": 70.38514872056784,
    "backtrans_bertscore": 0.9410926699638367,
    "prof_backtrans_bleu": 39.47995244873769,
    "prof_backtrans_chrf": 69.21617268328893,
    "prof_backtrans_bertscore": 0.9466843008995056,
    "prof_backtrans_labse": 0.9797024130821228,
    "prof_backtrans_xlm_roberta": 0.880648672580719,
    "llm_vs_prof_backtrans_bleu": 37.37967728583556,
    "llm_vs_prof_backtrans_chrf": 67.23687646686953,
    "llm_vs_prof_backtrans_bertscore": 0.9326886534690857,
    "llm_vs_prof_backtrans_labse": 0.9557901620864868
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 50.18312586787942,
    "same_lang_chrf": 67.49379715881969,
    "same_lang_bertscore": 0.8640264868736267,
    "same_lang_comet": 0.9198185205459595,
    "cross_lang_xlm_roberta": 0.9928781986236572,
    "cross_lang_labse": 0.9828178882598877,
    "cross_lang_mbert": 0.9674252271652222,
    "cross_lang_comet_qe": 0.37866804003715515,
    "backtrans_bleu": 59.431048399595234,
    "backtrans_chrf": 80.16590085535694,
    "backtrans_bertscore": 0.9583830833435059,
    "prof_backtrans_bleu": 47.727103436701455,
    "prof_backtrans_chrf": 74.40252506251842,
    "prof_backtrans_bertscore": 0.9390719532966614,
    "prof_backtrans_labse": 0.9754186272621155,
    "prof_backtrans_xlm_roberta": 0.9220278263092041,
    "llm_vs_prof_backtrans_bleu": 57.767888663485486,
    "llm_vs_prof_backtrans_chrf": 76.36063962390219,
    "llm_vs_prof_backtrans_bertscore": 0.948662281036377,
    "llm_vs_prof_backtrans_labse": 0.9661750793457031
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 47.0360747130934,
    "same_lang_chrf": 76.08339305926961,
    "same_lang_bertscore": 0.8565801978111267,
    "same_lang_comet": 0.9076946973800659,
    "cross_lang_xlm_roberta": 0.9880605936050415,
    "cross_lang_labse": 0.993651270866394,
    "cross_lang_mbert": 0.9538846611976624,
    "cross_lang_comet_qe": 0.4219173789024353,
    "backtrans_bleu": 69.08537314692784,
    "backtrans_chrf": 83.85639502929115,
    "backtrans_bertscore": 0.972353994846344,
    "prof_backtrans_bleu": 60.23228575256974,
    "prof_backtrans_chrf": 79.68211963194521,
    "prof_backtrans_bertscore": 0.9558055996894836,
    "prof_backtrans_labse": 0.9789103269577026,
    "prof_backtrans_xlm_roberta": 0.9310369491577148,
    "llm_vs_prof_backtrans_bleu": 63.982338845288936,
    "llm_vs_prof_backtrans_chrf": 80.55603041741864,
    "llm_vs_prof_backtrans_bertscore": 0.9609661102294922,
    "llm_vs_prof_backtrans_labse": 0.9811360836029053
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 22.642635586290254,
    "same_lang_chrf": 62.3820854637545,
    "same_lang_bertscore": 0.856086254119873,
    "same_lang_comet": 0.9213946461677551,
    "cross_lang_xlm_roberta": 0.9883649945259094,
    "cross_lang_labse": 0.9127979874610901,
    "cross_lang_mbert": 0.8991477489471436,
    "cross_lang_comet_qe": 0.4692824184894562,
    "backtrans_bleu": 51.712375650740654,
    "backtrans_chrf": 76.40000963159149,
    "backtrans_bertscore": 0.9454081654548645,
    "prof_backtrans_bleu": 31.254748198483192,
    "prof_backtrans_chrf": 64.25715435711754,
    "prof_backtrans_bertscore": 0.9249750375747681,
    "prof_backtrans_labse": 0.9462872743606567,
    "prof_backtrans_xlm_roberta": 0.9463046193122864,
    "llm_vs_prof_backtrans_bleu": 38.54223395624793,
    "llm_vs_prof_backtrans_chrf": 69.14579519913511,
    "llm_vs_prof_backtrans_bertscore": 0.934933066368103,
    "llm_vs_prof_backtrans_labse": 0.9004016518592834
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 20.886359632422543,
    "same_lang_chrf": 43.60776469761268,
    "same_lang_bertscore": 0.8386251926422119,
    "same_lang_comet": 0.9171698093414307,
    "cross_lang_xlm_roberta": 0.9852303266525269,
    "cross_lang_labse": 0.9672714471817017,
    "cross_lang_mbert": 0.981002688407898,
    "cross_lang_comet_qe": 0.3823360800743103,
    "backtrans_bleu": 46.74754500212234,
    "backtrans_chrf": 71.54676474054023,
    "backtrans_bertscore": 0.9514032602310181,
    "prof_backtrans_bleu": 50.97412859333047,
    "prof_backtrans_chrf": 74.05212519587879,
    "prof_backtrans_bertscore": 0.9463435411453247,
    "prof_backtrans_labse": 0.9690150618553162,
    "prof_backtrans_xlm_roberta": 0.8789774179458618,
    "llm_vs_prof_backtrans_bleu": 46.980481766346344,
    "llm_vs_prof_backtrans_chrf": 68.64082729033404,
    "llm_vs_prof_backtrans_bertscore": 0.9323471784591675,
    "llm_vs_prof_backtrans_labse": 0.9515294432640076
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 39.08326217109436,
    "same_lang_chrf": 64.04074208843315,
    "same_lang_bertscore": 0.8715066313743591,
    "same_lang_comet": 0.826176106929779,
    "cross_lang_xlm_roberta": 0.9983095526695251,
    "cross_lang_labse": 0.9905909895896912,
    "cross_lang_mbert": 0.9691759347915649,
    "cross_lang_comet_qe": 0.3946409225463867,
    "backtrans_bleu": 56.395378432615054,
    "backtrans_chrf": 77.60070069208228,
    "backtrans_bertscore": 0.9611254930496216,
    "prof_backtrans_bleu": 48.14100995552448,
    "prof_backtrans_chrf": 73.45893905757738,
    "prof_backtrans_bertscore": 0.9404426217079163,
    "prof_backtrans_labse": 0.9796529412269592,
    "prof_backtrans_xlm_roberta": 0.8872028589248657,
    "llm_vs_prof_backtrans_bleu": 54.48931692467019,
    "llm_vs_prof_backtrans_chrf": 73.98646224624153,
    "llm_vs_prof_backtrans_bertscore": 0.9481383562088013,
    "llm_vs_prof_backtrans_labse": 0.9766318202018738
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "spanish",
    "same_lang_bleu": 53.7468586785052,
    "same_lang_chrf": 76.279210825083,
    "same_lang_bertscore": 0.9268239140510559,
    "same_lang_comet": 0.9194161891937256,
    "cross_lang_xlm_roberta": 0.9915198683738708,
    "cross_lang_labse": 0.9943865537643433,
    "cross_lang_mbert": 0.9954813718795776,
    "cross_lang_comet_qe": 0.5748355388641357,
    "backtrans_bleu": 73.54173134906893,
    "backtrans_chrf": 85.72435723433024,
    "backtrans_bertscore": 0.9770458340644836,
    "prof_backtrans_bleu": 52.58158475379302,
    "prof_backtrans_chrf": 76.45141886288988,
    "prof_backtrans_bertscore": 0.9633704423904419,
    "prof_backtrans_labse": 0.9813555479049683,
    "prof_backtrans_xlm_roberta": 0.942975640296936,
    "llm_vs_prof_backtrans_bleu": 62.516104221229256,
    "llm_vs_prof_backtrans_chrf": 76.5937563214346,
    "llm_vs_prof_backtrans_bertscore": 0.9674494862556458,
    "llm_vs_prof_backtrans_labse": 0.981874942779541
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "chinese_simplified",
    "same_lang_bleu": 21.015606481590872,
    "same_lang_chrf": 42.920060123619365,
    "same_lang_bertscore": 0.8917706608772278,
    "same_lang_comet": 0.9383633732795715,
    "cross_lang_xlm_roberta": 0.9806994199752808,
    "cross_lang_labse": 0.991084098815918,
    "cross_lang_mbert": 0.9734683036804199,
    "cross_lang_comet_qe": 0.5204612016677856,
    "backtrans_bleu": 58.289120906809245,
    "backtrans_chrf": 79.17884483528587,
    "backtrans_bertscore": 0.9724247455596924,
    "prof_backtrans_bleu": 42.66394632968098,
    "prof_backtrans_chrf": 70.97433783300353,
    "prof_backtrans_bertscore": 0.9544867277145386,
    "prof_backtrans_labse": 0.9738338589668274,
    "prof_backtrans_xlm_roberta": 0.8777986168861389,
    "llm_vs_prof_backtrans_bleu": 50.206034946566284,
    "llm_vs_prof_backtrans_chrf": 74.97082631270973,
    "llm_vs_prof_backtrans_bertscore": 0.9611236453056335,
    "llm_vs_prof_backtrans_labse": 0.9798645377159119
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "vietnamese",
    "same_lang_bleu": 56.67984131937909,
    "same_lang_chrf": 71.7423521530886,
    "same_lang_bertscore": 0.8880442380905151,
    "same_lang_comet": 0.9343733787536621,
    "cross_lang_xlm_roberta": 0.9831560850143433,
    "cross_lang_labse": 0.9910499453544617,
    "cross_lang_mbert": 0.9783220291137695,
    "cross_lang_comet_qe": 0.570108950138092,
    "backtrans_bleu": 64.45717493193604,
    "backtrans_chrf": 81.55791882069596,
    "backtrans_bertscore": 0.9716319441795349,
    "prof_backtrans_bleu": 53.41320824797551,
    "prof_backtrans_chrf": 76.96380864899298,
    "prof_backtrans_bertscore": 0.9283084869384766,
    "prof_backtrans_labse": 0.9722587466239929,
    "prof_backtrans_xlm_roberta": 0.9326686263084412,
    "llm_vs_prof_backtrans_bleu": 61.408730911290114,
    "llm_vs_prof_backtrans_chrf": 76.31278526585787,
    "llm_vs_prof_backtrans_bertscore": 0.9349988698959351,
    "llm_vs_prof_backtrans_labse": 0.9776848554611206
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "tagalog",
    "same_lang_bleu": 43.91518893364608,
    "same_lang_chrf": 75.02763062499234,
    "same_lang_bertscore": 0.8692769408226013,
    "same_lang_comet": 0.9315667748451233,
    "cross_lang_xlm_roberta": 0.9942735433578491,
    "cross_lang_labse": 0.9926444292068481,
    "cross_lang_mbert": 0.9950879216194153,
    "cross_lang_comet_qe": 0.5615509748458862,
    "backtrans_bleu": 66.62783005975618,
    "backtrans_chrf": 83.2452966142668,
    "backtrans_bertscore": 0.9683674573898315,
    "prof_backtrans_bleu": 67.84744748017765,
    "prof_backtrans_chrf": 82.30169203710724,
    "prof_backtrans_bertscore": 0.967633068561554,
    "prof_backtrans_labse": 0.9730889797210693,
    "prof_backtrans_xlm_roberta": 0.9363836050033569,
    "llm_vs_prof_backtrans_bleu": 75.69648237700163,
    "llm_vs_prof_backtrans_chrf": 87.38622485996407,
    "llm_vs_prof_backtrans_bertscore": 0.9695767760276794,
    "llm_vs_prof_backtrans_labse": 0.9775967001914978
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "russian",
    "same_lang_bleu": 30.437216004370825,
    "same_lang_chrf": 65.28415074200838,
    "same_lang_bertscore": 0.8552572131156921,
    "same_lang_comet": 0.93536376953125,
    "cross_lang_xlm_roberta": 0.9890772104263306,
    "cross_lang_labse": 0.9875637888908386,
    "cross_lang_mbert": 0.9707967042922974,
    "cross_lang_comet_qe": 0.5255724191665649,
    "backtrans_bleu": 58.14646891091517,
    "backtrans_chrf": 77.61822889325228,
    "backtrans_bertscore": 0.9676862359046936,
    "prof_backtrans_bleu": 33.299658687622305,
    "prof_backtrans_chrf": 64.71755656810289,
    "prof_backtrans_bertscore": 0.9247708320617676,
    "prof_backtrans_labse": 0.9653355479240417,
    "prof_backtrans_xlm_roberta": 0.9249776601791382,
    "llm_vs_prof_backtrans_bleu": 48.22302981697467,
    "llm_vs_prof_backtrans_chrf": 72.28945943257256,
    "llm_vs_prof_backtrans_bertscore": 0.934227705001831,
    "llm_vs_prof_backtrans_labse": 0.9678943753242493
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "korean",
    "same_lang_bleu": 19.199516788421708,
    "same_lang_chrf": 42.24809097190255,
    "same_lang_bertscore": 0.8276804685592651,
    "same_lang_comet": 0.9331337213516235,
    "cross_lang_xlm_roberta": 0.9821517467498779,
    "cross_lang_labse": 0.9885942935943604,
    "cross_lang_mbert": 0.9808666110038757,
    "cross_lang_comet_qe": 0.5263174176216125,
    "backtrans_bleu": 54.44881194127529,
    "backtrans_chrf": 76.91604340400097,
    "backtrans_bertscore": 0.9656798243522644,
    "prof_backtrans_bleu": 50.36183680105602,
    "prof_backtrans_chrf": 75.67542784867415,
    "prof_backtrans_bertscore": 0.9119122624397278,
    "prof_backtrans_labse": 0.9835869073867798,
    "prof_backtrans_xlm_roberta": 0.8957545161247253,
    "llm_vs_prof_backtrans_bleu": 61.761810337065114,
    "llm_vs_prof_backtrans_chrf": 78.68300863849666,
    "llm_vs_prof_backtrans_bertscore": 0.9136101007461548,
    "llm_vs_prof_backtrans_labse": 0.9758188724517822
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "haitian_creole",
    "same_lang_bleu": 41.33398083076826,
    "same_lang_chrf": 67.66497487111604,
    "same_lang_bertscore": 0.8778896927833557,
    "same_lang_comet": 0.8605110049247742,
    "cross_lang_xlm_roberta": 0.9885172247886658,
    "cross_lang_labse": 0.9887036681175232,
    "cross_lang_mbert": 0.9637159109115601,
    "cross_lang_comet_qe": 0.5225239992141724,
    "backtrans_bleu": 68.04721627201457,
    "backtrans_chrf": 84.65675856392625,
    "backtrans_bertscore": 0.972297191619873,
    "prof_backtrans_bleu": 55.37467373173341,
    "prof_backtrans_chrf": 78.60074526613877,
    "prof_backtrans_bertscore": 0.9290744066238403,
    "prof_backtrans_labse": 0.9753583669662476,
    "prof_backtrans_xlm_roberta": 0.8787809014320374,
    "llm_vs_prof_backtrans_bleu": 56.294214187661424,
    "llm_vs_prof_backtrans_chrf": 74.78632743902811,
    "llm_vs_prof_backtrans_bertscore": 0.9318602085113525,
    "llm_vs_prof_backtrans_labse": 0.9660147428512573
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "spanish",
    "same_lang_bleu": 52.720511309118045,
    "same_lang_chrf": 75.68464927514403,
    "same_lang_bertscore": 0.911579966545105,
    "same_lang_comet": 0.9174642562866211,
    "cross_lang_xlm_roberta": 0.9981608390808105,
    "cross_lang_labse": 0.9962158203125,
    "cross_lang_mbert": 0.9968807101249695,
    "cross_lang_comet_qe": 0.5619372129440308,
    "backtrans_bleu": 76.51766095949984,
    "backtrans_chrf": 86.84470737845521,
    "backtrans_bertscore": 0.9857192635536194,
    "prof_backtrans_bleu": 54.19918093970049,
    "prof_backtrans_chrf": 77.10600719577711,
    "prof_backtrans_bertscore": 0.9678851366043091,
    "prof_backtrans_labse": 0.985111653804779,
    "prof_backtrans_xlm_roberta": 0.939923107624054,
    "llm_vs_prof_backtrans_bleu": 60.98354777064773,
    "llm_vs_prof_backtrans_chrf": 77.53830059746039,
    "llm_vs_prof_backtrans_bertscore": 0.9717098474502563,
    "llm_vs_prof_backtrans_labse": 0.9821565747261047
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "chinese_simplified",
    "same_lang_bleu": 27.991953134879505,
    "same_lang_chrf": 45.25296735045538,
    "same_lang_bertscore": 0.895473837852478,
    "same_lang_comet": 0.9379177689552307,
    "cross_lang_xlm_roberta": 0.9755163192749023,
    "cross_lang_labse": 0.9833657741546631,
    "cross_lang_mbert": 0.9383338093757629,
    "cross_lang_comet_qe": 0.5667914748191833,
    "backtrans_bleu": 48.524945749623434,
    "backtrans_chrf": 73.4769626429804,
    "backtrans_bertscore": 0.9616581201553345,
    "prof_backtrans_bleu": 38.381758630065,
    "prof_backtrans_chrf": 68.86553233083443,
    "prof_backtrans_bertscore": 0.9519972205162048,
    "prof_backtrans_labse": 0.9735544323921204,
    "prof_backtrans_xlm_roberta": 0.8855671286582947,
    "llm_vs_prof_backtrans_bleu": 51.85870223676949,
    "llm_vs_prof_backtrans_chrf": 73.87064994974409,
    "llm_vs_prof_backtrans_bertscore": 0.9585368633270264,
    "llm_vs_prof_backtrans_labse": 0.9770335555076599
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "vietnamese",
    "same_lang_bleu": 60.888721299620094,
    "same_lang_chrf": 73.37622664834697,
    "same_lang_bertscore": 0.8953551054000854,
    "same_lang_comet": 0.9326604008674622,
    "cross_lang_xlm_roberta": 0.9826433062553406,
    "cross_lang_labse": 0.9845038056373596,
    "cross_lang_mbert": 0.9772762060165405,
    "cross_lang_comet_qe": 0.5613419413566589,
    "backtrans_bleu": 59.7411104855496,
    "backtrans_chrf": 79.06910820833266,
    "backtrans_bertscore": 0.9647071361541748,
    "prof_backtrans_bleu": 55.515461927292854,
    "prof_backtrans_chrf": 76.55439028790478,
    "prof_backtrans_bertscore": 0.9656236171722412,
    "prof_backtrans_labse": 0.9808027744293213,
    "prof_backtrans_xlm_roberta": 0.9335280656814575,
    "llm_vs_prof_backtrans_bleu": 65.86636121842004,
    "llm_vs_prof_backtrans_chrf": 79.1992916659279,
    "llm_vs_prof_backtrans_bertscore": 0.9639266133308411,
    "llm_vs_prof_backtrans_labse": 0.9763397574424744
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "tagalog",
    "same_lang_bleu": 43.09983778089481,
    "same_lang_chrf": 73.33907468596786,
    "same_lang_bertscore": 0.8723914623260498,
    "same_lang_comet": 0.9329156875610352,
    "cross_lang_xlm_roberta": 0.9867948889732361,
    "cross_lang_labse": 0.9944812655448914,
    "cross_lang_mbert": 0.9942854046821594,
    "cross_lang_comet_qe": 0.5482783317565918,
    "backtrans_bleu": 68.01388144895103,
    "backtrans_chrf": 82.85070942570461,
    "backtrans_bertscore": 0.9760614633560181,
    "prof_backtrans_bleu": 67.67405122247592,
    "prof_backtrans_chrf": 82.25242071043324,
    "prof_backtrans_bertscore": 0.9743292331695557,
    "prof_backtrans_labse": 0.9759523272514343,
    "prof_backtrans_xlm_roberta": 0.9370868802070618,
    "llm_vs_prof_backtrans_bleu": 75.2227225927216,
    "llm_vs_prof_backtrans_chrf": 86.5068087063536,
    "llm_vs_prof_backtrans_bertscore": 0.9739121198654175,
    "llm_vs_prof_backtrans_labse": 0.9739794731140137
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "russian",
    "same_lang_bleu": 36.092448589404455,
    "same_lang_chrf": 68.96319578526983,
    "same_lang_bertscore": 0.8493078351020813,
    "same_lang_comet": 0.9378656148910522,
    "cross_lang_xlm_roberta": 0.9782268404960632,
    "cross_lang_labse": 0.9860750436782837,
    "cross_lang_mbert": 0.9415029287338257,
    "cross_lang_comet_qe": 0.5707879662513733,
    "backtrans_bleu": 58.83474015981775,
    "backtrans_chrf": 78.05730859802448,
    "backtrans_bertscore": 0.9642354249954224,
    "prof_backtrans_bleu": 34.000767252241985,
    "prof_backtrans_chrf": 65.32792504185487,
    "prof_backtrans_bertscore": 0.9371328353881836,
    "prof_backtrans_labse": 0.9665648937225342,
    "prof_backtrans_xlm_roberta": 0.9282670021057129,
    "llm_vs_prof_backtrans_bleu": 47.30180553901835,
    "llm_vs_prof_backtrans_chrf": 71.90657335531515,
    "llm_vs_prof_backtrans_bertscore": 0.9527137875556946,
    "llm_vs_prof_backtrans_labse": 0.9787335395812988
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "korean",
    "same_lang_bleu": 18.42967427304004,
    "same_lang_chrf": 41.003222960726944,
    "same_lang_bertscore": 0.8175646066665649,
    "same_lang_comet": 0.9304739236831665,
    "cross_lang_xlm_roberta": 0.9698036313056946,
    "cross_lang_labse": 0.982397735118866,
    "cross_lang_mbert": 0.9782455563545227,
    "cross_lang_comet_qe": 0.5513966679573059,
    "backtrans_bleu": 50.46039552596467,
    "backtrans_chrf": 74.77866792687715,
    "backtrans_bertscore": 0.9566130638122559,
    "prof_backtrans_bleu": 50.0038335345339,
    "prof_backtrans_chrf": 75.01512797693023,
    "prof_backtrans_bertscore": 0.9261936545372009,
    "prof_backtrans_labse": 0.9806497097015381,
    "prof_backtrans_xlm_roberta": 0.8942978978157043,
    "llm_vs_prof_backtrans_bleu": 55.31315484339427,
    "llm_vs_prof_backtrans_chrf": 74.58538718524673,
    "llm_vs_prof_backtrans_bertscore": 0.929215133190155,
    "llm_vs_prof_backtrans_labse": 0.9653832316398621
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "haitian_creole",
    "same_lang_bleu": 41.529677348902986,
    "same_lang_chrf": 67.69915310817434,
    "same_lang_bertscore": 0.8867419362068176,
    "same_lang_comet": 0.8470993638038635,
    "cross_lang_xlm_roberta": 0.9979010224342346,
    "cross_lang_labse": 0.9964210987091064,
    "cross_lang_mbert": 0.9920965433120728,
    "cross_lang_comet_qe": 0.559466540813446,
    "backtrans_bleu": 74.25187417868734,
    "backtrans_chrf": 86.13363216245689,
    "backtrans_bertscore": 0.9808582663536072,
    "prof_backtrans_bleu": 51.509656480668774,
    "prof_backtrans_chrf": 76.51977652869621,
    "prof_backtrans_bertscore": 0.9526349902153015,
    "prof_backtrans_labse": 0.972082793712616,
    "prof_backtrans_xlm_roberta": 0.8770899772644043,
    "llm_vs_prof_backtrans_bleu": 57.196103124139746,
    "llm_vs_prof_backtrans_chrf": 75.0305236101961,
    "llm_vs_prof_backtrans_bertscore": 0.9528685212135315,
    "llm_vs_prof_backtrans_labse": 0.9684203267097473
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "spanish",
    "same_lang_bleu": 55.57761444421414,
    "same_lang_chrf": 77.73841869718612,
    "same_lang_bertscore": 0.9230807423591614,
    "same_lang_comet": 0.9185818433761597,
    "cross_lang_xlm_roberta": 0.9991594552993774,
    "cross_lang_labse": 0.9981130957603455,
    "cross_lang_mbert": 0.9966870546340942,
    "cross_lang_comet_qe": 0.5879638195037842,
    "backtrans_bleu": 77.96377199877313,
    "backtrans_chrf": 88.63758739395237,
    "backtrans_bertscore": 0.9836350679397583,
    "prof_backtrans_bleu": 50.41735626147185,
    "prof_backtrans_chrf": 74.0853581789294,
    "prof_backtrans_bertscore": 0.9573934674263,
    "prof_backtrans_labse": 0.9837451577186584,
    "prof_backtrans_xlm_roberta": 0.9431440234184265,
    "llm_vs_prof_backtrans_bleu": 58.543453144723806,
    "llm_vs_prof_backtrans_chrf": 74.96520708843157,
    "llm_vs_prof_backtrans_bertscore": 0.9613220691680908,
    "llm_vs_prof_backtrans_labse": 0.9829697608947754
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "chinese_simplified",
    "same_lang_bleu": 30.71465941529053,
    "same_lang_chrf": 43.75825985149188,
    "same_lang_bertscore": 0.8885118961334229,
    "same_lang_comet": 0.9396868944168091,
    "cross_lang_xlm_roberta": 0.9696826338768005,
    "cross_lang_labse": 0.9625795483589172,
    "cross_lang_mbert": 0.9554402232170105,
    "cross_lang_comet_qe": 0.5532848834991455,
    "backtrans_bleu": 47.355312496128796,
    "backtrans_chrf": 72.27822539722672,
    "backtrans_bertscore": 0.9584077596664429,
    "prof_backtrans_bleu": 36.750030818166586,
    "prof_backtrans_chrf": 66.80586900055674,
    "prof_backtrans_bertscore": 0.9457404017448425,
    "prof_backtrans_labse": 0.9637662172317505,
    "prof_backtrans_xlm_roberta": 0.8560521602630615,
    "llm_vs_prof_backtrans_bleu": 45.767704539615316,
    "llm_vs_prof_backtrans_chrf": 70.85929794493055,
    "llm_vs_prof_backtrans_bertscore": 0.9476045370101929,
    "llm_vs_prof_backtrans_labse": 0.9593407511711121
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "vietnamese",
    "same_lang_bleu": 69.37744743451248,
    "same_lang_chrf": 79.6189791582668,
    "same_lang_bertscore": 0.9309166073799133,
    "same_lang_comet": 0.9391663074493408,
    "cross_lang_xlm_roberta": 0.9851259589195251,
    "cross_lang_labse": 0.9872219562530518,
    "cross_lang_mbert": 0.9920464754104614,
    "cross_lang_comet_qe": 0.5679488182067871,
    "backtrans_bleu": 61.558902834774,
    "backtrans_chrf": 80.32830125443007,
    "backtrans_bertscore": 0.9697468280792236,
    "prof_backtrans_bleu": 48.92073318368804,
    "prof_backtrans_chrf": 74.84889707860283,
    "prof_backtrans_bertscore": 0.9475852847099304,
    "prof_backtrans_labse": 0.9123051166534424,
    "prof_backtrans_xlm_roberta": 0.9311869144439697,
    "llm_vs_prof_backtrans_bleu": 64.06099785725196,
    "llm_vs_prof_backtrans_chrf": 80.28186605341638,
    "llm_vs_prof_backtrans_bertscore": 0.955624520778656,
    "llm_vs_prof_backtrans_labse": 0.925872266292572
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "tagalog",
    "same_lang_bleu": 50.176701671995296,
    "same_lang_chrf": 78.60405248403617,
    "same_lang_bertscore": 0.8824207186698914,
    "same_lang_comet": 0.9293749332427979,
    "cross_lang_xlm_roberta": 0.9959902167320251,
    "cross_lang_labse": 0.9986134171485901,
    "cross_lang_mbert": 0.9953370690345764,
    "cross_lang_comet_qe": 0.5411282181739807,
    "backtrans_bleu": 73.72112997685416,
    "backtrans_chrf": 86.52317442995103,
    "backtrans_bertscore": 0.983558177947998,
    "prof_backtrans_bleu": 60.59422456555642,
    "prof_backtrans_chrf": 79.20762416532331,
    "prof_backtrans_bertscore": 0.9622197151184082,
    "prof_backtrans_labse": 0.9798488020896912,
    "prof_backtrans_xlm_roberta": 0.9324036240577698,
    "llm_vs_prof_backtrans_bleu": 74.71720553696801,
    "llm_vs_prof_backtrans_chrf": 85.30447723012229,
    "llm_vs_prof_backtrans_bertscore": 0.9669781923294067,
    "llm_vs_prof_backtrans_labse": 0.9779818654060364
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "russian",
    "same_lang_bleu": 32.724778106148236,
    "same_lang_chrf": 68.66692968110961,
    "same_lang_bertscore": 0.8525022268295288,
    "same_lang_comet": 0.9405134916305542,
    "cross_lang_xlm_roberta": 0.9602870345115662,
    "cross_lang_labse": 0.9808926582336426,
    "cross_lang_mbert": 0.9246340990066528,
    "cross_lang_comet_qe": 0.5682092905044556,
    "backtrans_bleu": 47.07441362819935,
    "backtrans_chrf": 71.06387261041704,
    "backtrans_bertscore": 0.9577696919441223,
    "prof_backtrans_bleu": 29.591056358410437,
    "prof_backtrans_chrf": 61.821723657263874,
    "prof_backtrans_bertscore": 0.9346503019332886,
    "prof_backtrans_labse": 0.9529516100883484,
    "prof_backtrans_xlm_roberta": 0.9272550940513611,
    "llm_vs_prof_backtrans_bleu": 45.93882898406923,
    "llm_vs_prof_backtrans_chrf": 73.28209654540673,
    "llm_vs_prof_backtrans_bertscore": 0.9486895799636841,
    "llm_vs_prof_backtrans_labse": 0.9637861251831055
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "korean",
    "same_lang_bleu": 19.19640487957517,
    "same_lang_chrf": 43.935668389092086,
    "same_lang_bertscore": 0.8247591853141785,
    "same_lang_comet": 0.9338874816894531,
    "cross_lang_xlm_roberta": 0.9733384847640991,
    "cross_lang_labse": 0.9840396046638489,
    "cross_lang_mbert": 0.9786697030067444,
    "cross_lang_comet_qe": 0.5773576498031616,
    "backtrans_bleu": 51.7060691916041,
    "backtrans_chrf": 74.1526776413331,
    "backtrans_bertscore": 0.9601354598999023,
    "prof_backtrans_bleu": 49.06293293762961,
    "prof_backtrans_chrf": 73.5107333595619,
    "prof_backtrans_bertscore": 0.9191860556602478,
    "prof_backtrans_labse": 0.9773566722869873,
    "prof_backtrans_xlm_roberta": 0.8946353197097778,
    "llm_vs_prof_backtrans_bleu": 49.483540304881195,
    "llm_vs_prof_backtrans_chrf": 69.56055257337911,
    "llm_vs_prof_backtrans_bertscore": 0.9205505847930908,
    "llm_vs_prof_backtrans_labse": 0.9671579599380493
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "haitian_creole",
    "same_lang_bleu": 41.456111355169334,
    "same_lang_chrf": 67.94944193528946,
    "same_lang_bertscore": 0.8910712003707886,
    "same_lang_comet": 0.8395698666572571,
    "cross_lang_xlm_roberta": 0.9835285544395447,
    "cross_lang_labse": 0.9838649034500122,
    "cross_lang_mbert": 0.9859341979026794,
    "cross_lang_comet_qe": 0.47736620903015137,
    "backtrans_bleu": 62.60852804188091,
    "backtrans_chrf": 80.83525715868825,
    "backtrans_bertscore": 0.9626696705818176,
    "prof_backtrans_bleu": 43.73028903260743,
    "prof_backtrans_chrf": 73.29703156486208,
    "prof_backtrans_bertscore": 0.9411221146583557,
    "prof_backtrans_labse": 0.9756231307983398,
    "prof_backtrans_xlm_roberta": 0.8700676560401917,
    "llm_vs_prof_backtrans_bleu": 53.00944688038379,
    "llm_vs_prof_backtrans_chrf": 73.38037783008818,
    "llm_vs_prof_backtrans_bertscore": 0.9513323307037354,
    "llm_vs_prof_backtrans_labse": 0.971807062625885
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "spanish",
    "same_lang_bleu": 53.175530067880324,
    "same_lang_chrf": 75.61540372883374,
    "same_lang_bertscore": 0.9024172425270081,
    "same_lang_comet": 0.9189352989196777,
    "cross_lang_xlm_roberta": 0.9985324740409851,
    "cross_lang_labse": 0.9933452010154724,
    "cross_lang_mbert": 0.9945066571235657,
    "cross_lang_comet_qe": 0.5792917013168335,
    "backtrans_bleu": 61.64396169144722,
    "backtrans_chrf": 79.98426627142412,
    "backtrans_bertscore": 0.9724119901657104,
    "prof_backtrans_bleu": 47.75446077230731,
    "prof_backtrans_chrf": 73.33381751189843,
    "prof_backtrans_bertscore": 0.9600633978843689,
    "prof_backtrans_labse": 0.983670711517334,
    "prof_backtrans_xlm_roberta": 0.9410253763198853,
    "llm_vs_prof_backtrans_bleu": 58.59263645520512,
    "llm_vs_prof_backtrans_chrf": 76.37770301572623,
    "llm_vs_prof_backtrans_bertscore": 0.9663866758346558,
    "llm_vs_prof_backtrans_labse": 0.9794442057609558
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "chinese_simplified",
    "same_lang_bleu": 24.903015427770548,
    "same_lang_chrf": 45.6176406582218,
    "same_lang_bertscore": 0.8918911218643188,
    "same_lang_comet": 0.9373262524604797,
    "cross_lang_xlm_roberta": 0.9770521521568298,
    "cross_lang_labse": 0.9838179349899292,
    "cross_lang_mbert": 0.952981173992157,
    "cross_lang_comet_qe": 0.5447490215301514,
    "backtrans_bleu": 42.49403689644473,
    "backtrans_chrf": 70.10057552207263,
    "backtrans_bertscore": 0.9524767398834229,
    "prof_backtrans_bleu": 36.32923699305749,
    "prof_backtrans_chrf": 66.29749536024583,
    "prof_backtrans_bertscore": 0.9452775120735168,
    "prof_backtrans_labse": 0.9697952270507812,
    "prof_backtrans_xlm_roberta": 0.862430214881897,
    "llm_vs_prof_backtrans_bleu": 51.017176048837065,
    "llm_vs_prof_backtrans_chrf": 75.3962846858197,
    "llm_vs_prof_backtrans_bertscore": 0.951148509979248,
    "llm_vs_prof_backtrans_labse": 0.9750151634216309
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "vietnamese",
    "same_lang_bleu": 56.97370897179552,
    "same_lang_chrf": 71.08309218667229,
    "same_lang_bertscore": 0.907585859298706,
    "same_lang_comet": 0.9360716938972473,
    "cross_lang_xlm_roberta": 0.9837614297866821,
    "cross_lang_labse": 0.9912286996841431,
    "cross_lang_mbert": 0.9676496982574463,
    "cross_lang_comet_qe": 0.5356940031051636,
    "backtrans_bleu": 58.63496298666856,
    "backtrans_chrf": 77.97297040160261,
    "backtrans_bertscore": 0.9665942192077637,
    "prof_backtrans_bleu": 46.73419353072125,
    "prof_backtrans_chrf": 71.76092353356262,
    "prof_backtrans_bertscore": 0.946862518787384,
    "prof_backtrans_labse": 0.9763050079345703,
    "prof_backtrans_xlm_roberta": 0.9309703707695007,
    "llm_vs_prof_backtrans_bleu": 58.819380954577646,
    "llm_vs_prof_backtrans_chrf": 75.66284503776448,
    "llm_vs_prof_backtrans_bertscore": 0.9557540416717529,
    "llm_vs_prof_backtrans_labse": 0.9805083274841309
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "tagalog",
    "same_lang_bleu": 43.92895604983471,
    "same_lang_chrf": 73.33593978790388,
    "same_lang_bertscore": 0.857630729675293,
    "same_lang_comet": 0.9266406893730164,
    "cross_lang_xlm_roberta": 0.9914672374725342,
    "cross_lang_labse": 0.9915024638175964,
    "cross_lang_mbert": 0.9905298948287964,
    "cross_lang_comet_qe": 0.4953380525112152,
    "backtrans_bleu": 65.32781377062082,
    "backtrans_chrf": 80.9302569524632,
    "backtrans_bertscore": 0.9734096527099609,
    "prof_backtrans_bleu": 60.18720127840878,
    "prof_backtrans_chrf": 78.23400975624955,
    "prof_backtrans_bertscore": 0.9676714539527893,
    "prof_backtrans_labse": 0.9768945574760437,
    "prof_backtrans_xlm_roberta": 0.9228613376617432,
    "llm_vs_prof_backtrans_bleu": 66.31031927254956,
    "llm_vs_prof_backtrans_chrf": 80.72509985918964,
    "llm_vs_prof_backtrans_bertscore": 0.9697564840316772,
    "llm_vs_prof_backtrans_labse": 0.9811102747917175
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "russian",
    "same_lang_bleu": 33.01666759339965,
    "same_lang_chrf": 66.80587370918322,
    "same_lang_bertscore": 0.8485838770866394,
    "same_lang_comet": 0.9377209544181824,
    "cross_lang_xlm_roberta": 0.9736925363540649,
    "cross_lang_labse": 0.9888656139373779,
    "cross_lang_mbert": 0.9433794021606445,
    "cross_lang_comet_qe": 0.5157928466796875,
    "backtrans_bleu": 45.72677783194182,
    "backtrans_chrf": 71.08147387891563,
    "backtrans_bertscore": 0.9562366008758545,
    "prof_backtrans_bleu": 30.10151454095677,
    "prof_backtrans_chrf": 62.134508661497065,
    "prof_backtrans_bertscore": 0.9296664595603943,
    "prof_backtrans_labse": 0.9511072039604187,
    "prof_backtrans_xlm_roberta": 0.9230343103408813,
    "llm_vs_prof_backtrans_bleu": 37.051059807951425,
    "llm_vs_prof_backtrans_chrf": 68.07046835783753,
    "llm_vs_prof_backtrans_bertscore": 0.9376524686813354,
    "llm_vs_prof_backtrans_labse": 0.9542765617370605
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "korean",
    "same_lang_bleu": 17.78248529420905,
    "same_lang_chrf": 41.94660497022287,
    "same_lang_bertscore": 0.8223873972892761,
    "same_lang_comet": 0.9286271333694458,
    "cross_lang_xlm_roberta": 0.9650428891181946,
    "cross_lang_labse": 0.9641339182853699,
    "cross_lang_mbert": 0.9555701017379761,
    "cross_lang_comet_qe": 0.5551151037216187,
    "backtrans_bleu": 43.144323692007184,
    "backtrans_chrf": 70.57946323255632,
    "backtrans_bertscore": 0.9426451325416565,
    "prof_backtrans_bleu": 47.98928293887135,
    "prof_backtrans_chrf": 72.56551769134741,
    "prof_backtrans_bertscore": 0.9152528643608093,
    "prof_backtrans_labse": 0.9822520613670349,
    "prof_backtrans_xlm_roberta": 0.8833514451980591,
    "llm_vs_prof_backtrans_bleu": 47.38115235936916,
    "llm_vs_prof_backtrans_chrf": 68.24103298914095,
    "llm_vs_prof_backtrans_bertscore": 0.9195001721382141,
    "llm_vs_prof_backtrans_labse": 0.9564331769943237
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "haitian_creole",
    "same_lang_bleu": 36.04690720722454,
    "same_lang_chrf": 61.9704480157288,
    "same_lang_bertscore": 0.8717044591903687,
    "same_lang_comet": 0.8267487287521362,
    "cross_lang_xlm_roberta": 0.986209511756897,
    "cross_lang_labse": 0.9715766310691833,
    "cross_lang_mbert": 0.9740254282951355,
    "cross_lang_comet_qe": -0.12733176350593567,
    "backtrans_bleu": 36.66931854575388,
    "backtrans_chrf": 65.94162966781266,
    "backtrans_bertscore": 0.924240231513977,
    "prof_backtrans_bleu": 23.24581428508073,
    "prof_backtrans_chrf": 60.441622893288624,
    "prof_backtrans_bertscore": 0.8935097455978394,
    "prof_backtrans_labse": 0.9637841582298279,
    "prof_backtrans_xlm_roberta": 0.8514321446418762,
    "llm_vs_prof_backtrans_bleu": 29.52520155883377,
    "llm_vs_prof_backtrans_chrf": 61.37910888748659,
    "llm_vs_prof_backtrans_bertscore": 0.9198281764984131,
    "llm_vs_prof_backtrans_labse": 0.9717204570770264
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 35.51521655062336,
    "same_lang_chrf": 67.20140170299493,
    "same_lang_bertscore": 0.8771723508834839,
    "same_lang_comet": 0.900920033454895,
    "cross_lang_xlm_roberta": 0.9888364672660828,
    "cross_lang_labse": 0.9894772171974182,
    "cross_lang_mbert": 0.96623694896698,
    "cross_lang_comet_qe": 0.41821449995040894,
    "backtrans_bleu": 69.28588291380598,
    "backtrans_chrf": 85.98425497147221,
    "backtrans_bertscore": 0.9704340696334839,
    "prof_backtrans_bleu": 55.51818702544034,
    "prof_backtrans_chrf": 80.69172564840682,
    "prof_backtrans_bertscore": 0.9355475306510925,
    "prof_backtrans_labse": 0.9476880431175232,
    "prof_backtrans_xlm_roberta": 0.8077327013015747,
    "llm_vs_prof_backtrans_bleu": 64.82639518371157,
    "llm_vs_prof_backtrans_chrf": 83.72910386066185,
    "llm_vs_prof_backtrans_bertscore": 0.9504775404930115,
    "llm_vs_prof_backtrans_labse": 0.9555138945579529
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 36.614337821232105,
    "same_lang_chrf": 67.20776659129415,
    "same_lang_bertscore": 0.8940797448158264,
    "same_lang_comet": 0.9112056493759155,
    "cross_lang_xlm_roberta": 0.9913733601570129,
    "cross_lang_labse": 0.9901492595672607,
    "cross_lang_mbert": 0.9639903903007507,
    "cross_lang_comet_qe": 0.4320756196975708,
    "backtrans_bleu": 84.36712727239073,
    "backtrans_chrf": 91.64172201876369,
    "backtrans_bertscore": 0.9611086845397949,
    "prof_backtrans_bleu": 59.861896732471706,
    "prof_backtrans_chrf": 81.91558039741074,
    "prof_backtrans_bertscore": 0.9375995397567749,
    "prof_backtrans_labse": 0.9575363397598267,
    "prof_backtrans_xlm_roberta": 0.8211867213249207,
    "llm_vs_prof_backtrans_bleu": 62.74701914570407,
    "llm_vs_prof_backtrans_chrf": 82.13936946293882,
    "llm_vs_prof_backtrans_bertscore": 0.932877779006958,
    "llm_vs_prof_backtrans_labse": 0.9536442756652832
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 39.92403018404444,
    "same_lang_chrf": 69.43927360359923,
    "same_lang_bertscore": 0.856587290763855,
    "same_lang_comet": 0.9150125980377197,
    "cross_lang_xlm_roberta": 0.901968777179718,
    "cross_lang_labse": 0.9585139155387878,
    "cross_lang_mbert": 0.8493378758430481,
    "cross_lang_comet_qe": 0.49855631589889526,
    "backtrans_bleu": 64.33093175810262,
    "backtrans_chrf": 83.91910111036644,
    "backtrans_bertscore": 0.9393083453178406,
    "prof_backtrans_bleu": 42.37285893649178,
    "prof_backtrans_chrf": 75.15515894221303,
    "prof_backtrans_bertscore": 0.9164369702339172,
    "prof_backtrans_labse": 0.929701030254364,
    "prof_backtrans_xlm_roberta": 0.7923409938812256,
    "llm_vs_prof_backtrans_bleu": 49.27608931625142,
    "llm_vs_prof_backtrans_chrf": 74.84424491764213,
    "llm_vs_prof_backtrans_bertscore": 0.9108899235725403,
    "llm_vs_prof_backtrans_labse": 0.9188027381896973
  },
  {
    "doc_id": "immunize/flu_inactive",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 41.43326201615277,
    "same_lang_chrf": 69.2870933552914,
    "same_lang_bertscore": 0.878009021282196,
    "same_lang_comet": 0.9033883810043335,
    "cross_lang_xlm_roberta": 0.9381321668624878,
    "cross_lang_labse": 0.9804139733314514,
    "cross_lang_mbert": 0.9007332921028137,
    "cross_lang_comet_qe": 0.4689948558807373,
    "backtrans_bleu": 55.51093869535475,
    "backtrans_chrf": 79.34620188636589,
    "backtrans_bertscore": 0.9492747783660889,
    "prof_backtrans_bleu": 51.26355602643119,
    "prof_backtrans_chrf": 78.73312309469381,
    "prof_backtrans_bertscore": 0.9305903911590576,
    "prof_backtrans_labse": 0.9499208331108093,
    "prof_backtrans_xlm_roberta": 0.8018542528152466,
    "llm_vs_prof_backtrans_bleu": 59.01362110266434,
    "llm_vs_prof_backtrans_chrf": 79.92926513761759,
    "llm_vs_prof_backtrans_bertscore": 0.943065881729126,
    "llm_vs_prof_backtrans_labse": 0.9600658416748047
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 34.03979666498245,
    "same_lang_chrf": 61.554421095211254,
    "same_lang_bertscore": 0.8611960411071777,
    "same_lang_comet": 0.8930121064186096,
    "cross_lang_xlm_roberta": 0.9935803413391113,
    "cross_lang_labse": 0.9929776191711426,
    "cross_lang_mbert": 0.9753398299217224,
    "cross_lang_comet_qe": 0.5235985517501831,
    "backtrans_bleu": 76.02109093019274,
    "backtrans_chrf": 88.45568397917039,
    "backtrans_bertscore": 0.9752645492553711,
    "prof_backtrans_bleu": 66.99285151286897,
    "prof_backtrans_chrf": 84.31118971693382,
    "prof_backtrans_bertscore": 0.9582058191299438,
    "prof_backtrans_labse": 0.9571290016174316,
    "prof_backtrans_xlm_roberta": 0.8288293480873108,
    "llm_vs_prof_backtrans_bleu": 72.24078603243677,
    "llm_vs_prof_backtrans_chrf": 85.56133323716334,
    "llm_vs_prof_backtrans_bertscore": 0.9600826501846313,
    "llm_vs_prof_backtrans_labse": 0.9657443761825562
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": null,
    "same_lang_chrf": null,
    "same_lang_bertscore": null,
    "same_lang_comet": null,
    "cross_lang_xlm_roberta": null,
    "cross_lang_labse": null,
    "cross_lang_mbert": null,
    "cross_lang_comet_qe": null,
    "backtrans_bleu": null,
    "backtrans_chrf": null,
    "backtrans_bertscore": null,
    "prof_backtrans_bleu": null,
    "prof_backtrans_chrf": null,
    "prof_backtrans_bertscore": null,
    "prof_backtrans_labse": null,
    "prof_backtrans_xlm_roberta": null,
    "llm_vs_prof_backtrans_bleu": null,
    "llm_vs_prof_backtrans_chrf": null,
    "llm_vs_prof_backtrans_bertscore": null,
    "llm_vs_prof_backtrans_labse": null
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 37.14752673114767,
    "same_lang_chrf": 65.10053532199467,
    "same_lang_bertscore": 0.8875980377197266,
    "same_lang_comet": 0.9057254791259766,
    "cross_lang_xlm_roberta": 0.9974482655525208,
    "cross_lang_labse": 0.9939545392990112,
    "cross_lang_mbert": 0.9865132570266724,
    "cross_lang_comet_qe": 0.5063706636428833,
    "backtrans_bleu": 81.52303653462259,
    "backtrans_chrf": 91.51178192406282,
    "backtrans_bertscore": 0.9665851593017578,
    "prof_backtrans_bleu": 83.20442609537119,
    "prof_backtrans_chrf": 90.72331155819977,
    "prof_backtrans_bertscore": 0.9626637101173401,
    "prof_backtrans_labse": 0.9698755741119385,
    "prof_backtrans_xlm_roberta": 0.8406088352203369,
    "llm_vs_prof_backtrans_bleu": 80.75355388940444,
    "llm_vs_prof_backtrans_chrf": 90.67547162039638,
    "llm_vs_prof_backtrans_bertscore": 0.9598615169525146,
    "llm_vs_prof_backtrans_labse": 0.9722834229469299
  },
  {
    "doc_id": "immunize/hepatitis_b",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 37.03856070488627,
    "same_lang_chrf": 64.00843541859415,
    "same_lang_bertscore": 0.8586390018463135,
    "same_lang_comet": 0.8915691375732422,
    "cross_lang_xlm_roberta": 0.9912818670272827,
    "cross_lang_labse": 0.9909735918045044,
    "cross_lang_mbert": 0.9655846953392029,
    "cross_lang_comet_qe": 0.5271079540252686,
    "backtrans_bleu": 67.97956326257278,
    "backtrans_chrf": 86.00113545121587,
    "backtrans_bertscore": 0.9734917879104614,
    "prof_backtrans_bleu": 53.97053649781907,
    "prof_backtrans_chrf": 78.64222039013869,
    "prof_backtrans_bertscore": 0.9517037272453308,
    "prof_backtrans_labse": 0.9624799489974976,
    "prof_backtrans_xlm_roberta": 0.8225048184394836,
    "llm_vs_prof_backtrans_bleu": 57.082530532559446,
    "llm_vs_prof_backtrans_chrf": 78.63874498575572,
    "llm_vs_prof_backtrans_bertscore": 0.9497391581535339,
    "llm_vs_prof_backtrans_labse": 0.9629048109054565
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 42.749885948693304,
    "same_lang_chrf": 67.54871550068208,
    "same_lang_bertscore": 0.9043682813644409,
    "same_lang_comet": 0.902645468711853,
    "cross_lang_xlm_roberta": 0.9829283952713013,
    "cross_lang_labse": 0.9323914051055908,
    "cross_lang_mbert": 0.9385105967521667,
    "cross_lang_comet_qe": 0.40419241786003113,
    "backtrans_bleu": 61.73050024359848,
    "backtrans_chrf": 81.19651308943303,
    "backtrans_bertscore": 0.9231773614883423,
    "prof_backtrans_bleu": 55.30817883935443,
    "prof_backtrans_chrf": 78.671089386373,
    "prof_backtrans_bertscore": 0.9210299253463745,
    "prof_backtrans_labse": 0.9390469789505005,
    "prof_backtrans_xlm_roberta": 0.7893977761268616,
    "llm_vs_prof_backtrans_bleu": 65.3868726816066,
    "llm_vs_prof_backtrans_chrf": 83.1451107504685,
    "llm_vs_prof_backtrans_bertscore": 0.937258243560791,
    "llm_vs_prof_backtrans_labse": 0.9492924213409424
  },
  {
    "doc_id": "immunize/hpv",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 43.554459188947256,
    "same_lang_chrf": 68.37101889273443,
    "same_lang_bertscore": 0.9087560772895813,
    "same_lang_comet": 0.9127583503723145,
    "cross_lang_xlm_roberta": 0.9819273948669434,
    "cross_lang_labse": 0.9845258593559265,
    "cross_lang_mbert": 0.9354153871536255,
    "cross_lang_comet_qe": 0.5015674829483032,
    "backtrans_bleu": 75.04231996509063,
    "backtrans_chrf": 88.60686154331425,
    "backtrans_bertscore": 0.9441366195678711,
    "prof_backtrans_bleu": 64.90230613308707,
    "prof_backtrans_chrf": 83.45738364263616,
    "prof_backtrans_bertscore": 0.9344227910041809,
    "prof_backtrans_labse": 0.951534628868103,
    "prof_backtrans_xlm_roberta": 0.7902613878250122,
    "llm_vs_prof_backtrans_bleu": 71.73901413255219,
    "llm_vs_prof_backtrans_chrf": 85.33151648933533,
    "llm_vs_prof_backtrans_bertscore": 0.9321975111961365,
    "llm_vs_prof_backtrans_labse": 0.9552868008613586
  },
  {
    "doc_id": "immunize/hpv",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 39.46695340698421,
    "same_lang_chrf": 67.70625052996508,
    "same_lang_bertscore": 0.8987870216369629,
    "same_lang_comet": 0.9158108234405518,
    "cross_lang_xlm_roberta": 0.9084436297416687,
    "cross_lang_labse": 0.9320740103721619,
    "cross_lang_mbert": 0.9143295288085938,
    "cross_lang_comet_qe": 0.48340705037117004,
    "backtrans_bleu": 51.13553655718754,
    "backtrans_chrf": 80.07751517319164,
    "backtrans_bertscore": 0.9251949191093445,
    "prof_backtrans_bleu": 49.56612950165646,
    "prof_backtrans_chrf": 76.87896582439998,
    "prof_backtrans_bertscore": 0.9169446229934692,
    "prof_backtrans_labse": 0.9404070377349854,
    "prof_backtrans_xlm_roberta": 0.7814597487449646,
    "llm_vs_prof_backtrans_bleu": 56.91216182496689,
    "llm_vs_prof_backtrans_chrf": 78.58946345843815,
    "llm_vs_prof_backtrans_bertscore": 0.9235771298408508,
    "llm_vs_prof_backtrans_labse": 0.8932991027832031
  },
  {
    "doc_id": "immunize/hpv",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 44.87908357011994,
    "same_lang_chrf": 68.99212252863664,
    "same_lang_bertscore": 0.8848108649253845,
    "same_lang_comet": 0.9011025428771973,
    "cross_lang_xlm_roberta": 0.989692211151123,
    "cross_lang_labse": 0.9831797480583191,
    "cross_lang_mbert": 0.9525749087333679,
    "cross_lang_comet_qe": 0.4772750735282898,
    "backtrans_bleu": 67.16703485334462,
    "backtrans_chrf": 85.26471564568676,
    "backtrans_bertscore": 0.9532942175865173,
    "prof_backtrans_bleu": 47.93492895003876,
    "prof_backtrans_chrf": 75.9068593821234,
    "prof_backtrans_bertscore": 0.9205944538116455,
    "prof_backtrans_labse": 0.940778374671936,
    "prof_backtrans_xlm_roberta": 0.7862956523895264,
    "llm_vs_prof_backtrans_bleu": 60.915669124981015,
    "llm_vs_prof_backtrans_chrf": 79.45017009068775,
    "llm_vs_prof_backtrans_bertscore": 0.9450497031211853,
    "llm_vs_prof_backtrans_labse": 0.958453893661499
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 36.844859989925894,
    "same_lang_chrf": 66.29155118233886,
    "same_lang_bertscore": 0.8781943321228027,
    "same_lang_comet": 0.8974119424819946,
    "cross_lang_xlm_roberta": 0.993229329586029,
    "cross_lang_labse": 0.9942233562469482,
    "cross_lang_mbert": 0.9766351580619812,
    "cross_lang_comet_qe": 0.4767988920211792,
    "backtrans_bleu": 66.05102431483778,
    "backtrans_chrf": 84.95269912524464,
    "backtrans_bertscore": 0.9575388431549072,
    "prof_backtrans_bleu": 64.53535380908409,
    "prof_backtrans_chrf": 83.3838566446197,
    "prof_backtrans_bertscore": 0.950861394405365,
    "prof_backtrans_labse": 0.9613078832626343,
    "prof_backtrans_xlm_roberta": 0.8009569048881531,
    "llm_vs_prof_backtrans_bleu": 62.514057973742524,
    "llm_vs_prof_backtrans_chrf": 82.33919773187264,
    "llm_vs_prof_backtrans_bertscore": 0.9442654848098755,
    "llm_vs_prof_backtrans_labse": 0.9655753970146179
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 40.25408262553618,
    "same_lang_chrf": 68.45531279462728,
    "same_lang_bertscore": 0.8798993229866028,
    "same_lang_comet": 0.9084475040435791,
    "cross_lang_xlm_roberta": 0.9907143712043762,
    "cross_lang_labse": 0.9904841184616089,
    "cross_lang_mbert": 0.9783116579055786,
    "cross_lang_comet_qe": 0.463192343711853,
    "backtrans_bleu": 86.67397412393157,
    "backtrans_chrf": 92.88052853107905,
    "backtrans_bertscore": 0.9810773730278015,
    "prof_backtrans_bleu": 65.07105994694639,
    "prof_backtrans_chrf": 83.99484962709974,
    "prof_backtrans_bertscore": 0.9484753012657166,
    "prof_backtrans_labse": 0.9641474485397339,
    "prof_backtrans_xlm_roberta": 0.8113217353820801,
    "llm_vs_prof_backtrans_bleu": 66.71313019800107,
    "llm_vs_prof_backtrans_chrf": 83.52983410828408,
    "llm_vs_prof_backtrans_bertscore": 0.945672333240509,
    "llm_vs_prof_backtrans_labse": 0.9614697098731995
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 34.660708612868525,
    "same_lang_chrf": 67.38065514599913,
    "same_lang_bertscore": 0.8617821335792542,
    "same_lang_comet": 0.9121425151824951,
    "cross_lang_xlm_roberta": 0.986815333366394,
    "cross_lang_labse": 0.9517256021499634,
    "cross_lang_mbert": 0.9307307004928589,
    "cross_lang_comet_qe": 0.5160249471664429,
    "backtrans_bleu": 72.66319831705736,
    "backtrans_chrf": 90.52165806005492,
    "backtrans_bertscore": 0.9344226121902466,
    "prof_backtrans_bleu": 54.32511163848769,
    "prof_backtrans_chrf": 79.33101232178818,
    "prof_backtrans_bertscore": 0.9416581392288208,
    "prof_backtrans_labse": 0.9613264799118042,
    "prof_backtrans_xlm_roberta": 0.7993810772895813,
    "llm_vs_prof_backtrans_bleu": 51.33862055043457,
    "llm_vs_prof_backtrans_chrf": 74.08618093237604,
    "llm_vs_prof_backtrans_bertscore": 0.9208428263664246,
    "llm_vs_prof_backtrans_labse": 0.9232088327407837
  },
  {
    "doc_id": "immunize/meningococcal_acwy",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 42.685482468933444,
    "same_lang_chrf": 69.0510793268908,
    "same_lang_bertscore": 0.8581855297088623,
    "same_lang_comet": 0.8986597061157227,
    "cross_lang_xlm_roberta": 0.9947924613952637,
    "cross_lang_labse": 0.9936227798461914,
    "cross_lang_mbert": 0.9785128831863403,
    "cross_lang_comet_qe": 0.4327905774116516,
    "backtrans_bleu": 67.99350622266194,
    "backtrans_chrf": 85.20063071687639,
    "backtrans_bertscore": 0.9594149589538574,
    "prof_backtrans_bleu": 49.29775686954407,
    "prof_backtrans_chrf": 77.47338897550506,
    "prof_backtrans_bertscore": 0.9348114132881165,
    "prof_backtrans_labse": 0.9584996700286865,
    "prof_backtrans_xlm_roberta": 0.8064845204353333,
    "llm_vs_prof_backtrans_bleu": 54.54163093411877,
    "llm_vs_prof_backtrans_chrf": 77.00362351326227,
    "llm_vs_prof_backtrans_bertscore": 0.9288731813430786,
    "llm_vs_prof_backtrans_labse": 0.9554744958877563
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 38.01644413409619,
    "same_lang_chrf": 67.16231307853401,
    "same_lang_bertscore": 0.8654248714447021,
    "same_lang_comet": 0.9044694900512695,
    "cross_lang_xlm_roberta": 0.9854410886764526,
    "cross_lang_labse": 0.9883869886398315,
    "cross_lang_mbert": 0.9607623815536499,
    "cross_lang_comet_qe": 0.5177585482597351,
    "backtrans_bleu": 73.29472187268215,
    "backtrans_chrf": 87.22609466814667,
    "backtrans_bertscore": 0.94535231590271,
    "prof_backtrans_bleu": 61.70986902377079,
    "prof_backtrans_chrf": 82.64699161675631,
    "prof_backtrans_bertscore": 0.9429024457931519,
    "prof_backtrans_labse": 0.9341297149658203,
    "prof_backtrans_xlm_roberta": 0.8165879249572754,
    "llm_vs_prof_backtrans_bleu": 64.6197128912054,
    "llm_vs_prof_backtrans_chrf": 82.65747103618625,
    "llm_vs_prof_backtrans_bertscore": 0.9333110451698303,
    "llm_vs_prof_backtrans_labse": 0.9324395656585693
  },
  {
    "doc_id": "immunize/mmr",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 41.06059162227821,
    "same_lang_chrf": 68.23192285187434,
    "same_lang_bertscore": 0.868532657623291,
    "same_lang_comet": 0.9054524898529053,
    "cross_lang_xlm_roberta": 0.9908381104469299,
    "cross_lang_labse": 0.989604651927948,
    "cross_lang_mbert": 0.9718427062034607,
    "cross_lang_comet_qe": 0.518715500831604,
    "backtrans_bleu": 86.51886702892864,
    "backtrans_chrf": 92.23551864814084,
    "backtrans_bertscore": 0.9682282209396362,
    "prof_backtrans_bleu": 82.23116169321426,
    "prof_backtrans_chrf": 91.18639867002464,
    "prof_backtrans_bertscore": 0.9601253271102905,
    "prof_backtrans_labse": 0.9604334831237793,
    "prof_backtrans_xlm_roberta": 0.8288376331329346,
    "llm_vs_prof_backtrans_bleu": 80.74060152731421,
    "llm_vs_prof_backtrans_chrf": 90.60963207432064,
    "llm_vs_prof_backtrans_bertscore": 0.9521884918212891,
    "llm_vs_prof_backtrans_labse": 0.955880880355835
  },
  {
    "doc_id": "immunize/mmr",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 37.53591201870907,
    "same_lang_chrf": 68.55025776866587,
    "same_lang_bertscore": 0.8889548778533936,
    "same_lang_comet": 0.9103100299835205,
    "cross_lang_xlm_roberta": 0.9732431769371033,
    "cross_lang_labse": 0.9306848049163818,
    "cross_lang_mbert": 0.931253969669342,
    "cross_lang_comet_qe": 0.5034044981002808,
    "backtrans_bleu": 64.40614624231966,
    "backtrans_chrf": 86.19360766559805,
    "backtrans_bertscore": 0.9446479082107544,
    "prof_backtrans_bleu": 64.93658843898318,
    "prof_backtrans_chrf": 83.93504575713985,
    "prof_backtrans_bertscore": 0.9515517354011536,
    "prof_backtrans_labse": 0.9470580220222473,
    "prof_backtrans_xlm_roberta": 0.833200216293335,
    "llm_vs_prof_backtrans_bleu": 60.25221639333702,
    "llm_vs_prof_backtrans_chrf": 82.01766339097192,
    "llm_vs_prof_backtrans_bertscore": 0.937022864818573,
    "llm_vs_prof_backtrans_labse": 0.8886995911598206
  },
  {
    "doc_id": "immunize/mmr",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": null,
    "same_lang_chrf": null,
    "same_lang_bertscore": null,
    "same_lang_comet": null,
    "cross_lang_xlm_roberta": null,
    "cross_lang_labse": null,
    "cross_lang_mbert": null,
    "cross_lang_comet_qe": null,
    "backtrans_bleu": null,
    "backtrans_chrf": null,
    "backtrans_bertscore": null,
    "prof_backtrans_bleu": null,
    "prof_backtrans_chrf": null,
    "prof_backtrans_bertscore": null,
    "prof_backtrans_labse": null,
    "prof_backtrans_xlm_roberta": null,
    "llm_vs_prof_backtrans_bleu": null,
    "llm_vs_prof_backtrans_chrf": null,
    "llm_vs_prof_backtrans_bertscore": null,
    "llm_vs_prof_backtrans_labse": null
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 39.47251352996752,
    "same_lang_chrf": 67.11034841405056,
    "same_lang_bertscore": 0.906940221786499,
    "same_lang_comet": 0.9151231050491333,
    "cross_lang_xlm_roberta": 0.994234025478363,
    "cross_lang_labse": 0.9932448863983154,
    "cross_lang_mbert": 0.9301364421844482,
    "cross_lang_comet_qe": 0.528302788734436,
    "backtrans_bleu": 77.90938617813607,
    "backtrans_chrf": 89.74586202713482,
    "backtrans_bertscore": 0.9285285472869873,
    "prof_backtrans_bleu": 63.30080142631512,
    "prof_backtrans_chrf": 83.71423655943275,
    "prof_backtrans_bertscore": 0.9500563740730286,
    "prof_backtrans_labse": 0.950742781162262,
    "prof_backtrans_xlm_roberta": 0.7621644735336304,
    "llm_vs_prof_backtrans_bleu": 65.30041500834282,
    "llm_vs_prof_backtrans_chrf": 83.78053410535802,
    "llm_vs_prof_backtrans_bertscore": 0.9164291620254517,
    "llm_vs_prof_backtrans_labse": 0.9529904127120972
  },
  {
    "doc_id": "immunize/pcv",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 42.21811907789916,
    "same_lang_chrf": 68.27058566664633,
    "same_lang_bertscore": 0.9026139378547668,
    "same_lang_comet": 0.9073868989944458,
    "cross_lang_xlm_roberta": 0.9825139045715332,
    "cross_lang_labse": 0.9936870336532593,
    "cross_lang_mbert": 0.9581395983695984,
    "cross_lang_comet_qe": 0.5291941165924072,
    "backtrans_bleu": 81.96697630451813,
    "backtrans_chrf": 89.77890281003367,
    "backtrans_bertscore": 0.9594128727912903,
    "prof_backtrans_bleu": 66.92489583008536,
    "prof_backtrans_chrf": 85.52902868282713,
    "prof_backtrans_bertscore": 0.9546145796775818,
    "prof_backtrans_labse": 0.9576607346534729,
    "prof_backtrans_xlm_roberta": 0.7801020741462708,
    "llm_vs_prof_backtrans_bleu": 66.18946946912827,
    "llm_vs_prof_backtrans_chrf": 85.14346140582349,
    "llm_vs_prof_backtrans_bertscore": 0.9503892660140991,
    "llm_vs_prof_backtrans_labse": 0.9615082740783691
  },
  {
    "doc_id": "immunize/pcv",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 39.79377263092882,
    "same_lang_chrf": 67.7838396784514,
    "same_lang_bertscore": 0.9097474813461304,
    "same_lang_comet": 0.9231765866279602,
    "cross_lang_xlm_roberta": 0.9728054404258728,
    "cross_lang_labse": 0.9418681263923645,
    "cross_lang_mbert": 0.9416515231132507,
    "cross_lang_comet_qe": 0.5630075931549072,
    "backtrans_bleu": 71.36440291035171,
    "backtrans_chrf": 89.10873504690102,
    "backtrans_bertscore": 0.9347113370895386,
    "prof_backtrans_bleu": 76.93309865406925,
    "prof_backtrans_chrf": 89.24916491536847,
    "prof_backtrans_bertscore": 0.956457257270813,
    "prof_backtrans_labse": 0.9556754231452942,
    "prof_backtrans_xlm_roberta": 0.7631028890609741,
    "llm_vs_prof_backtrans_bleu": 69.50942185068018,
    "llm_vs_prof_backtrans_chrf": 87.65876785090126,
    "llm_vs_prof_backtrans_bertscore": 0.9366080164909363,
    "llm_vs_prof_backtrans_labse": 0.9269751906394958
  },
  {
    "doc_id": "immunize/pcv",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 42.851192953545365,
    "same_lang_chrf": 69.49710256260477,
    "same_lang_bertscore": 0.898746132850647,
    "same_lang_comet": 0.9178712964057922,
    "cross_lang_xlm_roberta": 0.9743705987930298,
    "cross_lang_labse": 0.9795249104499817,
    "cross_lang_mbert": 0.9598134160041809,
    "cross_lang_comet_qe": 0.5426278114318848,
    "backtrans_bleu": 66.70004113698998,
    "backtrans_chrf": 84.59032570808002,
    "backtrans_bertscore": 0.9574240446090698,
    "prof_backtrans_bleu": 53.94455768874896,
    "prof_backtrans_chrf": 79.75520682966544,
    "prof_backtrans_bertscore": 0.9480555653572083,
    "prof_backtrans_labse": 0.9574118256568909,
    "prof_backtrans_xlm_roberta": 0.7709052562713623,
    "llm_vs_prof_backtrans_bleu": 60.68308919534527,
    "llm_vs_prof_backtrans_chrf": 81.24334521556096,
    "llm_vs_prof_backtrans_bertscore": 0.9549926519393921,
    "llm_vs_prof_backtrans_labse": 0.9499790072441101
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 40.484164658805526,
    "same_lang_chrf": 67.08544669897074,
    "same_lang_bertscore": 0.899287223815918,
    "same_lang_comet": 0.9099944829940796,
    "cross_lang_xlm_roberta": 0.986369252204895,
    "cross_lang_labse": 0.9829021096229553,
    "cross_lang_mbert": 0.9763593077659607,
    "cross_lang_comet_qe": 0.4871498644351959,
    "backtrans_bleu": 67.5635288251434,
    "backtrans_chrf": 84.86209327235588,
    "backtrans_bertscore": 0.9655749797821045,
    "prof_backtrans_bleu": 71.49088740480208,
    "prof_backtrans_chrf": 86.65713871028677,
    "prof_backtrans_bertscore": 0.9549116492271423,
    "prof_backtrans_labse": 0.9620815515518188,
    "prof_backtrans_xlm_roberta": 0.8184163570404053,
    "llm_vs_prof_backtrans_bleu": 63.74329282063678,
    "llm_vs_prof_backtrans_chrf": 84.67224149069324,
    "llm_vs_prof_backtrans_bertscore": 0.9363476634025574,
    "llm_vs_prof_backtrans_labse": 0.9477170705795288
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 48.044168147343896,
    "same_lang_chrf": 71.6169974191452,
    "same_lang_bertscore": 0.9124137759208679,
    "same_lang_comet": 0.9143376350402832,
    "cross_lang_xlm_roberta": 0.9969559907913208,
    "cross_lang_labse": 0.9914053082466125,
    "cross_lang_mbert": 0.9847856163978577,
    "cross_lang_comet_qe": 0.4827337861061096,
    "backtrans_bleu": 80.5039119268419,
    "backtrans_chrf": 90.55236479199141,
    "backtrans_bertscore": 0.9817886352539062,
    "prof_backtrans_bleu": 70.13675724923587,
    "prof_backtrans_chrf": 86.31646166043082,
    "prof_backtrans_bertscore": 0.9572404026985168,
    "prof_backtrans_labse": 0.9675894379615784,
    "prof_backtrans_xlm_roberta": 0.816343367099762,
    "llm_vs_prof_backtrans_bleu": 74.86783140065211,
    "llm_vs_prof_backtrans_chrf": 88.925438433238,
    "llm_vs_prof_backtrans_bertscore": 0.9629889130592346,
    "llm_vs_prof_backtrans_labse": 0.9715272188186646
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 47.1397257871016,
    "same_lang_chrf": 71.93292017750865,
    "same_lang_bertscore": 0.9087976217269897,
    "same_lang_comet": 0.9124003648757935,
    "cross_lang_xlm_roberta": 0.9711103439331055,
    "cross_lang_labse": 0.9784731268882751,
    "cross_lang_mbert": 0.9739561676979065,
    "cross_lang_comet_qe": 0.4885864853858948,
    "backtrans_bleu": 72.33181816153238,
    "backtrans_chrf": 88.11313048584778,
    "backtrans_bertscore": 0.9474292397499084,
    "prof_backtrans_bleu": 56.00610412356323,
    "prof_backtrans_chrf": 80.58351437090077,
    "prof_backtrans_bertscore": 0.9527299404144287,
    "prof_backtrans_labse": 0.9655994176864624,
    "prof_backtrans_xlm_roberta": 0.8112369775772095,
    "llm_vs_prof_backtrans_bleu": 61.346761774899804,
    "llm_vs_prof_backtrans_chrf": 81.05048702855774,
    "llm_vs_prof_backtrans_bertscore": 0.9298680424690247,
    "llm_vs_prof_backtrans_labse": 0.9582058191299438
  },
  {
    "doc_id": "immunize/polio_ipv",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 46.04570248886823,
    "same_lang_chrf": 70.3302568788956,
    "same_lang_bertscore": 0.9034382700920105,
    "same_lang_comet": 0.9160763621330261,
    "cross_lang_xlm_roberta": 0.9877821803092957,
    "cross_lang_labse": 0.9815664291381836,
    "cross_lang_mbert": 0.9782249331474304,
    "cross_lang_comet_qe": 0.47508567571640015,
    "backtrans_bleu": 66.64215218070976,
    "backtrans_chrf": 85.89337482900629,
    "backtrans_bertscore": 0.9504119753837585,
    "prof_backtrans_bleu": 55.900529951058914,
    "prof_backtrans_chrf": 80.92626436179438,
    "prof_backtrans_bertscore": 0.9484922289848328,
    "prof_backtrans_labse": 0.96416175365448,
    "prof_backtrans_xlm_roberta": 0.8057403564453125,
    "llm_vs_prof_backtrans_bleu": 63.98978079379489,
    "llm_vs_prof_backtrans_chrf": 81.68504595110899,
    "llm_vs_prof_backtrans_bertscore": 0.9333550930023193,
    "llm_vs_prof_backtrans_labse": 0.9549203515052795
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 40.38144303984657,
    "same_lang_chrf": 67.56709102104804,
    "same_lang_bertscore": 0.9101778268814087,
    "same_lang_comet": 0.909919261932373,
    "cross_lang_xlm_roberta": 0.9851385354995728,
    "cross_lang_labse": 0.9931503534317017,
    "cross_lang_mbert": 0.9590115547180176,
    "cross_lang_comet_qe": 0.4496883153915405,
    "backtrans_bleu": 75.84234000631987,
    "backtrans_chrf": 88.77275596718648,
    "backtrans_bertscore": 0.9319668412208557,
    "prof_backtrans_bleu": 61.634033259511824,
    "prof_backtrans_chrf": 81.95389673789992,
    "prof_backtrans_bertscore": 0.9529417753219604,
    "prof_backtrans_labse": 0.957111120223999,
    "prof_backtrans_xlm_roberta": 0.7884973883628845,
    "llm_vs_prof_backtrans_bleu": 66.33183677703285,
    "llm_vs_prof_backtrans_chrf": 83.99758941093829,
    "llm_vs_prof_backtrans_bertscore": 0.9216718077659607,
    "llm_vs_prof_backtrans_labse": 0.9607329964637756
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 43.9296124782722,
    "same_lang_chrf": 68.88180751244619,
    "same_lang_bertscore": 0.9133179187774658,
    "same_lang_comet": 0.9157145023345947,
    "cross_lang_xlm_roberta": 0.9748979210853577,
    "cross_lang_labse": 0.9946754574775696,
    "cross_lang_mbert": 0.9411042332649231,
    "cross_lang_comet_qe": 0.517825722694397,
    "backtrans_bleu": 81.73125337012762,
    "backtrans_chrf": 89.66988684562162,
    "backtrans_bertscore": 0.9671934843063354,
    "prof_backtrans_bleu": 66.84383459974073,
    "prof_backtrans_chrf": 84.97958772142472,
    "prof_backtrans_bertscore": 0.9580622911453247,
    "prof_backtrans_labse": 0.9627504944801331,
    "prof_backtrans_xlm_roberta": 0.7929415702819824,
    "llm_vs_prof_backtrans_bleu": 69.09492793422817,
    "llm_vs_prof_backtrans_chrf": 86.50645098241301,
    "llm_vs_prof_backtrans_bertscore": 0.9565906524658203,
    "llm_vs_prof_backtrans_labse": 0.9687596559524536
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 40.94901412414816,
    "same_lang_chrf": 67.89238500112607,
    "same_lang_bertscore": 0.894515335559845,
    "same_lang_comet": 0.9192526936531067,
    "cross_lang_xlm_roberta": 0.927435040473938,
    "cross_lang_labse": 0.9232963919639587,
    "cross_lang_mbert": 0.8975039720535278,
    "cross_lang_comet_qe": 0.5137302279472351,
    "backtrans_bleu": 69.94698177233624,
    "backtrans_chrf": 87.19281506319149,
    "backtrans_bertscore": 0.9315390586853027,
    "prof_backtrans_bleu": 50.84022321822592,
    "prof_backtrans_chrf": 78.06518495131633,
    "prof_backtrans_bertscore": 0.940407395362854,
    "prof_backtrans_labse": 0.9541485905647278,
    "prof_backtrans_xlm_roberta": 0.7711127400398254,
    "llm_vs_prof_backtrans_bleu": 49.47438292642117,
    "llm_vs_prof_backtrans_chrf": 73.9916238779972,
    "llm_vs_prof_backtrans_bertscore": 0.9248149991035461,
    "llm_vs_prof_backtrans_labse": 0.9011610746383667
  },
  {
    "doc_id": "immunize/ppsv",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 41.38197082158514,
    "same_lang_chrf": 67.43237821036236,
    "same_lang_bertscore": 0.9104256629943848,
    "same_lang_comet": 0.9215916395187378,
    "cross_lang_xlm_roberta": 0.975300133228302,
    "cross_lang_labse": 0.98316490650177,
    "cross_lang_mbert": 0.9105771780014038,
    "cross_lang_comet_qe": 0.5120673179626465,
    "backtrans_bleu": 65.27871339525417,
    "backtrans_chrf": 84.19229177394125,
    "backtrans_bertscore": 0.9578714370727539,
    "prof_backtrans_bleu": 53.883539188661636,
    "prof_backtrans_chrf": 79.70936950779125,
    "prof_backtrans_bertscore": 0.9482371211051941,
    "prof_backtrans_labse": 0.9577426314353943,
    "prof_backtrans_xlm_roberta": 0.7802319526672363,
    "llm_vs_prof_backtrans_bleu": 60.59613455136495,
    "llm_vs_prof_backtrans_chrf": 81.28668182476605,
    "llm_vs_prof_backtrans_bertscore": 0.9494989514350891,
    "llm_vs_prof_backtrans_labse": 0.9584858417510986
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 38.497685145751895,
    "same_lang_chrf": 65.39255646468904,
    "same_lang_bertscore": 0.8788202404975891,
    "same_lang_comet": 0.8994506597518921,
    "cross_lang_xlm_roberta": 0.9783364534378052,
    "cross_lang_labse": 0.9863042831420898,
    "cross_lang_mbert": 0.9590193033218384,
    "cross_lang_comet_qe": 0.47867491841316223,
    "backtrans_bleu": 64.29271622438323,
    "backtrans_chrf": 83.44542201279468,
    "backtrans_bertscore": 0.9372473955154419,
    "prof_backtrans_bleu": 60.81281913002919,
    "prof_backtrans_chrf": 82.21257960667138,
    "prof_backtrans_bertscore": 0.9313616752624512,
    "prof_backtrans_labse": 0.9417363405227661,
    "prof_backtrans_xlm_roberta": 0.7965795397758484,
    "llm_vs_prof_backtrans_bleu": 68.59563950133395,
    "llm_vs_prof_backtrans_chrf": 85.60853461264561,
    "llm_vs_prof_backtrans_bertscore": 0.9350537657737732,
    "llm_vs_prof_backtrans_labse": 0.9479215741157532
  },
  {
    "doc_id": "immunize/tdap",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 41.081050202813884,
    "same_lang_chrf": 67.53559137396569,
    "same_lang_bertscore": 0.8830528855323792,
    "same_lang_comet": 0.8981616497039795,
    "cross_lang_xlm_roberta": 0.996338963508606,
    "cross_lang_labse": 0.9919549226760864,
    "cross_lang_mbert": 0.9731675386428833,
    "cross_lang_comet_qe": 0.47198355197906494,
    "backtrans_bleu": 85.89915223914332,
    "backtrans_chrf": 92.15867829521177,
    "backtrans_bertscore": 0.9649633169174194,
    "prof_backtrans_bleu": 68.84973339031694,
    "prof_backtrans_chrf": 85.46892996903074,
    "prof_backtrans_bertscore": 0.9497206807136536,
    "prof_backtrans_labse": 0.964369535446167,
    "prof_backtrans_xlm_roberta": 0.8019771575927734,
    "llm_vs_prof_backtrans_bleu": 69.69678427398793,
    "llm_vs_prof_backtrans_chrf": 85.21188086882596,
    "llm_vs_prof_backtrans_bertscore": 0.9376514554023743,
    "llm_vs_prof_backtrans_labse": 0.9647411108016968
  },
  {
    "doc_id": "immunize/tdap",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 38.04024837753016,
    "same_lang_chrf": 67.34118006081147,
    "same_lang_bertscore": 0.8853981494903564,
    "same_lang_comet": 0.9027557373046875,
    "cross_lang_xlm_roberta": 0.9734559059143066,
    "cross_lang_labse": 0.9163316488265991,
    "cross_lang_mbert": 0.9299609065055847,
    "cross_lang_comet_qe": 0.43956658244132996,
    "backtrans_bleu": 63.00958112881876,
    "backtrans_chrf": 85.45930233229389,
    "backtrans_bertscore": 0.9372272491455078,
    "prof_backtrans_bleu": 69.74854828637211,
    "prof_backtrans_chrf": 85.27718714604615,
    "prof_backtrans_bertscore": 0.9420583844184875,
    "prof_backtrans_labse": 0.9538489580154419,
    "prof_backtrans_xlm_roberta": 0.7747340798377991,
    "llm_vs_prof_backtrans_bleu": 61.58685766702945,
    "llm_vs_prof_backtrans_chrf": 84.27538935513682,
    "llm_vs_prof_backtrans_bertscore": 0.9317513704299927,
    "llm_vs_prof_backtrans_labse": 0.902280330657959
  },
  {
    "doc_id": "immunize/tdap",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 41.457282342746275,
    "same_lang_chrf": 68.2230775311417,
    "same_lang_bertscore": 0.8796269297599792,
    "same_lang_comet": 0.8990896940231323,
    "cross_lang_xlm_roberta": 0.9945988059043884,
    "cross_lang_labse": 0.9905726909637451,
    "cross_lang_mbert": 0.9758334755897522,
    "cross_lang_comet_qe": 0.48829805850982666,
    "backtrans_bleu": 67.7585410748415,
    "backtrans_chrf": 84.3178160047515,
    "backtrans_bertscore": 0.9628324508666992,
    "prof_backtrans_bleu": 49.73685660249917,
    "prof_backtrans_chrf": 77.20022082393994,
    "prof_backtrans_bertscore": 0.9278548955917358,
    "prof_backtrans_labse": 0.9390748739242554,
    "prof_backtrans_xlm_roberta": 0.7848104238510132,
    "llm_vs_prof_backtrans_bleu": 56.014880930460976,
    "llm_vs_prof_backtrans_chrf": 77.06701652660539,
    "llm_vs_prof_backtrans_bertscore": 0.9244863390922546,
    "llm_vs_prof_backtrans_labse": 0.9371832609176636
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 37.44404103955851,
    "same_lang_chrf": 66.35581289839381,
    "same_lang_bertscore": 0.8452087044715881,
    "same_lang_comet": 0.9006991386413574,
    "cross_lang_xlm_roberta": 0.9980459809303284,
    "cross_lang_labse": 0.9899595975875854,
    "cross_lang_mbert": 0.9799955487251282,
    "cross_lang_comet_qe": 0.4950653910636902,
    "backtrans_bleu": 72.23594917710852,
    "backtrans_chrf": 87.14251440865446,
    "backtrans_bertscore": 0.9695569276809692,
    "prof_backtrans_bleu": 62.865184965638036,
    "prof_backtrans_chrf": 83.259790506183,
    "prof_backtrans_bertscore": 0.9524264931678772,
    "prof_backtrans_labse": 0.9538443684577942,
    "prof_backtrans_xlm_roberta": 0.8431777954101562,
    "llm_vs_prof_backtrans_bleu": 70.51153130878828,
    "llm_vs_prof_backtrans_chrf": 85.93336050037082,
    "llm_vs_prof_backtrans_bertscore": 0.9551687836647034,
    "llm_vs_prof_backtrans_labse": 0.956559419631958
  },
  {
    "doc_id": "immunize/varicella",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 34.70512497710593,
    "same_lang_chrf": 64.67249391594501,
    "same_lang_bertscore": 0.8256880640983582,
    "same_lang_comet": 0.8979513645172119,
    "cross_lang_xlm_roberta": 0.9945054650306702,
    "cross_lang_labse": 0.9909747242927551,
    "cross_lang_mbert": 0.9710652232170105,
    "cross_lang_comet_qe": 0.4184369444847107,
    "backtrans_bleu": 85.00279001163075,
    "backtrans_chrf": 92.25615074121966,
    "backtrans_bertscore": 0.9713989496231079,
    "prof_backtrans_bleu": 65.9048619259127,
    "prof_backtrans_chrf": 84.2985679622978,
    "prof_backtrans_bertscore": 0.9538872241973877,
    "prof_backtrans_labse": 0.9506171345710754,
    "prof_backtrans_xlm_roberta": 0.8549555540084839,
    "llm_vs_prof_backtrans_bleu": 70.44607864134217,
    "llm_vs_prof_backtrans_chrf": 86.02594257623262,
    "llm_vs_prof_backtrans_bertscore": 0.9521689414978027,
    "llm_vs_prof_backtrans_labse": 0.9583625793457031
  },
  {
    "doc_id": "immunize/varicella",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 34.65365964756346,
    "same_lang_chrf": 66.22371106757546,
    "same_lang_bertscore": 0.833112895488739,
    "same_lang_comet": 0.9092617034912109,
    "cross_lang_xlm_roberta": 0.9721142649650574,
    "cross_lang_labse": 0.9310473799705505,
    "cross_lang_mbert": 0.8844506144523621,
    "cross_lang_comet_qe": 0.4887380301952362,
    "backtrans_bleu": 69.96019424541869,
    "backtrans_chrf": 89.63101436022355,
    "backtrans_bertscore": 0.9343299269676208,
    "prof_backtrans_bleu": 82.14869748359843,
    "prof_backtrans_chrf": 91.67472565199643,
    "prof_backtrans_bertscore": 0.9572876691818237,
    "prof_backtrans_labse": 0.9587013125419617,
    "prof_backtrans_xlm_roberta": 0.8504406213760376,
    "llm_vs_prof_backtrans_bleu": 69.66371170224281,
    "llm_vs_prof_backtrans_chrf": 88.75472216598159,
    "llm_vs_prof_backtrans_bertscore": 0.9417791366577148,
    "llm_vs_prof_backtrans_labse": 0.9216145277023315
  },
  {
    "doc_id": "immunize/varicella",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 38.270968695857675,
    "same_lang_chrf": 66.30511703436245,
    "same_lang_bertscore": 0.8538694977760315,
    "same_lang_comet": 0.90614914894104,
    "cross_lang_xlm_roberta": 0.9613747596740723,
    "cross_lang_labse": 0.9764853119850159,
    "cross_lang_mbert": 0.8929343223571777,
    "cross_lang_comet_qe": 0.47576022148132324,
    "backtrans_bleu": 60.74518197478135,
    "backtrans_chrf": 81.54864055850277,
    "backtrans_bertscore": 0.9365856051445007,
    "prof_backtrans_bleu": 49.90149943022692,
    "prof_backtrans_chrf": 78.10426245691858,
    "prof_backtrans_bertscore": 0.9475570321083069,
    "prof_backtrans_labse": 0.9481706023216248,
    "prof_backtrans_xlm_roberta": 0.844441294670105,
    "llm_vs_prof_backtrans_bleu": 54.90145724102709,
    "llm_vs_prof_backtrans_chrf": 76.7336339982236,
    "llm_vs_prof_backtrans_bertscore": 0.9324601292610168,
    "llm_vs_prof_backtrans_labse": 0.9585390090942383
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 30.899558664523866,
    "same_lang_chrf": 60.82257842075697,
    "same_lang_bertscore": 0.8481630086898804,
    "same_lang_comet": 0.889034628868103,
    "cross_lang_xlm_roberta": 0.9889142513275146,
    "cross_lang_labse": 0.9871894121170044,
    "cross_lang_mbert": 0.9586878418922424,
    "cross_lang_comet_qe": 0.44719505310058594,
    "backtrans_bleu": 72.3106202714796,
    "backtrans_chrf": 88.18004207682716,
    "backtrans_bertscore": 0.9619277715682983,
    "prof_backtrans_bleu": 78.84511118672809,
    "prof_backtrans_chrf": 86.85146909257867,
    "prof_backtrans_bertscore": 0.9415264129638672,
    "prof_backtrans_labse": 0.9667048454284668,
    "prof_backtrans_xlm_roberta": 0.8917462229728699,
    "llm_vs_prof_backtrans_bleu": 66.52737064704903,
    "llm_vs_prof_backtrans_chrf": 84.71048003359208,
    "llm_vs_prof_backtrans_bertscore": 0.9204076528549194,
    "llm_vs_prof_backtrans_labse": 0.9545495510101318
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 30.283854313139027,
    "same_lang_chrf": 61.05984255755777,
    "same_lang_bertscore": 0.844351589679718,
    "same_lang_comet": 0.8941159248352051,
    "cross_lang_xlm_roberta": 0.9720879793167114,
    "cross_lang_labse": 0.9815132021903992,
    "cross_lang_mbert": 0.9167441129684448,
    "cross_lang_comet_qe": 0.5290229320526123,
    "backtrans_bleu": 80.11718347687744,
    "backtrans_chrf": 89.54271980282365,
    "backtrans_bertscore": 0.9633575081825256,
    "prof_backtrans_bleu": 71.9958803105069,
    "prof_backtrans_chrf": 86.10408301124556,
    "prof_backtrans_bertscore": 0.9394327998161316,
    "prof_backtrans_labse": 0.9652678966522217,
    "prof_backtrans_xlm_roberta": 0.8804724216461182,
    "llm_vs_prof_backtrans_bleu": 73.04584856844929,
    "llm_vs_prof_backtrans_chrf": 88.4892369123315,
    "llm_vs_prof_backtrans_bertscore": 0.9363136291503906,
    "llm_vs_prof_backtrans_labse": 0.9528396725654602
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 27.937985727671176,
    "same_lang_chrf": 61.58572872818101,
    "same_lang_bertscore": 0.8584753274917603,
    "same_lang_comet": 0.9070510864257812,
    "cross_lang_xlm_roberta": 0.970104992389679,
    "cross_lang_labse": 0.9222828149795532,
    "cross_lang_mbert": 0.8839786648750305,
    "cross_lang_comet_qe": 0.5295887589454651,
    "backtrans_bleu": 60.60062779805676,
    "backtrans_chrf": 84.68346286018014,
    "backtrans_bertscore": 0.9317413568496704,
    "prof_backtrans_bleu": 83.54471883622097,
    "prof_backtrans_chrf": 90.4280063821603,
    "prof_backtrans_bertscore": 0.9419946670532227,
    "prof_backtrans_labse": 0.9660665392875671,
    "prof_backtrans_xlm_roberta": 0.8844807147979736,
    "llm_vs_prof_backtrans_bleu": 59.694340126652364,
    "llm_vs_prof_backtrans_chrf": 86.65516751077546,
    "llm_vs_prof_backtrans_bertscore": 0.9344016313552856,
    "llm_vs_prof_backtrans_labse": 0.8827328085899353
  },
  {
    "doc_id": "immunize/zoster_recombinant",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 29.679605147968264,
    "same_lang_chrf": 61.0907393857793,
    "same_lang_bertscore": 0.8205820918083191,
    "same_lang_comet": 0.88177889585495,
    "cross_lang_xlm_roberta": 0.9743008613586426,
    "cross_lang_labse": 0.9773445129394531,
    "cross_lang_mbert": 0.8896324634552002,
    "cross_lang_comet_qe": 0.5221682786941528,
    "backtrans_bleu": 64.38792965223371,
    "backtrans_chrf": 83.61263598629705,
    "backtrans_bertscore": 0.9427676796913147,
    "prof_backtrans_bleu": 46.874228861310165,
    "prof_backtrans_chrf": 72.95416177669807,
    "prof_backtrans_bertscore": 0.917058527469635,
    "prof_backtrans_labse": 0.9389354586601257,
    "prof_backtrans_xlm_roberta": 0.8828384876251221,
    "llm_vs_prof_backtrans_bleu": 48.455516586660345,
    "llm_vs_prof_backtrans_chrf": 72.6942614937759,
    "llm_vs_prof_backtrans_bertscore": 0.9182954430580139,
    "llm_vs_prof_backtrans_labse": 0.9238417744636536
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 39.22010894940331,
    "same_lang_chrf": 69.19208570236968,
    "same_lang_bertscore": 0.8575485348701477,
    "same_lang_comet": 0.908614993095398,
    "cross_lang_xlm_roberta": 0.9916386604309082,
    "cross_lang_labse": 0.9945821762084961,
    "cross_lang_mbert": 0.9935368895530701,
    "cross_lang_comet_qe": 0.5328207015991211,
    "backtrans_bleu": 69.95414725589812,
    "backtrans_chrf": 86.2859914493334,
    "backtrans_bertscore": 0.9736196398735046,
    "prof_backtrans_bleu": 70.98995823076464,
    "prof_backtrans_chrf": 86.70787708129815,
    "prof_backtrans_bertscore": 0.964978814125061,
    "prof_backtrans_labse": 0.9873825311660767,
    "prof_backtrans_xlm_roberta": 0.9700822830200195,
    "llm_vs_prof_backtrans_bleu": 74.81740421467991,
    "llm_vs_prof_backtrans_chrf": 87.28120692901018,
    "llm_vs_prof_backtrans_bertscore": 0.9647029042243958,
    "llm_vs_prof_backtrans_labse": 0.9840913414955139
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 41.00281043636654,
    "same_lang_chrf": 69.6239227491693,
    "same_lang_bertscore": 0.9065508246421814,
    "same_lang_comet": 0.9277236461639404,
    "cross_lang_xlm_roberta": 0.9978893995285034,
    "cross_lang_labse": 0.9929280877113342,
    "cross_lang_mbert": 0.9971385598182678,
    "cross_lang_comet_qe": 0.5251147747039795,
    "backtrans_bleu": 68.8844939115143,
    "backtrans_chrf": 82.82503092345635,
    "backtrans_bertscore": 0.9852920174598694,
    "prof_backtrans_bleu": 71.66058931459898,
    "prof_backtrans_chrf": 86.83098704532249,
    "prof_backtrans_bertscore": 0.9693256616592407,
    "prof_backtrans_labse": 0.9810045957565308,
    "prof_backtrans_xlm_roberta": 0.9652724266052246,
    "llm_vs_prof_backtrans_bleu": 71.29015416855476,
    "llm_vs_prof_backtrans_chrf": 82.58891776891107,
    "llm_vs_prof_backtrans_bertscore": 0.9698073267936707,
    "llm_vs_prof_backtrans_labse": 0.9757326245307922
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 50.24264453287859,
    "same_lang_chrf": 76.89041709350644,
    "same_lang_bertscore": 0.9032281041145325,
    "same_lang_comet": 0.9253786206245422,
    "cross_lang_xlm_roberta": 0.9939415454864502,
    "cross_lang_labse": 0.9884557127952576,
    "cross_lang_mbert": 0.9928629398345947,
    "cross_lang_comet_qe": 0.4907340109348297,
    "backtrans_bleu": 75.49077907652081,
    "backtrans_chrf": 88.33447629731708,
    "backtrans_bertscore": 0.9789424538612366,
    "prof_backtrans_bleu": 71.59452993738087,
    "prof_backtrans_chrf": 86.82589982480701,
    "prof_backtrans_bertscore": 0.9688751101493835,
    "prof_backtrans_labse": 0.9845027923583984,
    "prof_backtrans_xlm_roberta": 0.9632120132446289,
    "llm_vs_prof_backtrans_bleu": 78.34430998290068,
    "llm_vs_prof_backtrans_chrf": 88.50040731875251,
    "llm_vs_prof_backtrans_bertscore": 0.9743124842643738,
    "llm_vs_prof_backtrans_labse": 0.9730437397956848
  },
  {
    "doc_id": "cancer/after-a-breast-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 41.283326017590674,
    "same_lang_chrf": 70.0785430836163,
    "same_lang_bertscore": 0.8696568608283997,
    "same_lang_comet": 0.92047518491745,
    "cross_lang_xlm_roberta": 0.9798587560653687,
    "cross_lang_labse": 0.9883069396018982,
    "cross_lang_mbert": 0.9726829528808594,
    "cross_lang_comet_qe": 0.5115393400192261,
    "backtrans_bleu": 52.63855632089145,
    "backtrans_chrf": 76.86248326443905,
    "backtrans_bertscore": 0.9585679173469543,
    "prof_backtrans_bleu": 53.62081672741602,
    "prof_backtrans_chrf": 78.49294201312573,
    "prof_backtrans_bertscore": 0.9490421414375305,
    "prof_backtrans_labse": 0.9736766815185547,
    "prof_backtrans_xlm_roberta": 0.9562926292419434,
    "llm_vs_prof_backtrans_bleu": 58.85324659725294,
    "llm_vs_prof_backtrans_chrf": 79.28070819138507,
    "llm_vs_prof_backtrans_bertscore": 0.9557387828826904,
    "llm_vs_prof_backtrans_labse": 0.9780098795890808
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 44.16003409127153,
    "same_lang_chrf": 72.21573622185335,
    "same_lang_bertscore": 0.9046838283538818,
    "same_lang_comet": 0.9203017950057983,
    "cross_lang_xlm_roberta": 0.9957240223884583,
    "cross_lang_labse": 0.9958537817001343,
    "cross_lang_mbert": 0.9809683561325073,
    "cross_lang_comet_qe": 0.5504733920097351,
    "backtrans_bleu": 65.30338318680984,
    "backtrans_chrf": 83.55360487825145,
    "backtrans_bertscore": 0.981339156627655,
    "prof_backtrans_bleu": 70.2817871044116,
    "prof_backtrans_chrf": 86.27133641256549,
    "prof_backtrans_bertscore": 0.9710122346878052,
    "prof_backtrans_labse": 0.980798065662384,
    "prof_backtrans_xlm_roberta": 0.9701846837997437,
    "llm_vs_prof_backtrans_bleu": 73.42984448935172,
    "llm_vs_prof_backtrans_chrf": 86.10846728482565,
    "llm_vs_prof_backtrans_bertscore": 0.9685455560684204,
    "llm_vs_prof_backtrans_labse": 0.972398579120636
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 42.945909423575,
    "same_lang_chrf": 69.97697756582785,
    "same_lang_bertscore": 0.9060962796211243,
    "same_lang_comet": 0.9075642824172974,
    "cross_lang_xlm_roberta": 0.9953785538673401,
    "cross_lang_labse": 0.9978066086769104,
    "cross_lang_mbert": 0.9933444261550903,
    "cross_lang_comet_qe": 0.48724454641342163,
    "backtrans_bleu": 67.88289170261717,
    "backtrans_chrf": 82.20350780588461,
    "backtrans_bertscore": 0.9843164682388306,
    "prof_backtrans_bleu": 70.17607053769916,
    "prof_backtrans_chrf": 85.82948035236075,
    "prof_backtrans_bertscore": 0.9732108116149902,
    "prof_backtrans_labse": 0.985457181930542,
    "prof_backtrans_xlm_roberta": 0.9697597622871399,
    "llm_vs_prof_backtrans_bleu": 71.79742520068665,
    "llm_vs_prof_backtrans_chrf": 83.29199265350138,
    "llm_vs_prof_backtrans_bertscore": 0.9748088717460632,
    "llm_vs_prof_backtrans_labse": 0.9832833409309387
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 47.31829749501436,
    "same_lang_chrf": 75.29289391766932,
    "same_lang_bertscore": 0.9229732155799866,
    "same_lang_comet": 0.9292421340942383,
    "cross_lang_xlm_roberta": 0.9882340431213379,
    "cross_lang_labse": 0.9885203242301941,
    "cross_lang_mbert": 0.9798388481140137,
    "cross_lang_comet_qe": 0.5238971710205078,
    "backtrans_bleu": 66.12594060115937,
    "backtrans_chrf": 84.2376549169036,
    "backtrans_bertscore": 0.9626163244247437,
    "prof_backtrans_bleu": 71.68189800474143,
    "prof_backtrans_chrf": 86.83935062923331,
    "prof_backtrans_bertscore": 0.9701746106147766,
    "prof_backtrans_labse": 0.9793832898139954,
    "prof_backtrans_xlm_roberta": 0.9678142666816711,
    "llm_vs_prof_backtrans_bleu": 73.41620483450922,
    "llm_vs_prof_backtrans_chrf": 86.58653745837401,
    "llm_vs_prof_backtrans_bertscore": 0.9675472378730774,
    "llm_vs_prof_backtrans_labse": 0.9815061688423157
  },
  {
    "doc_id": "cancer/after-a-cervical-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 45.767743855564476,
    "same_lang_chrf": 73.58211034107126,
    "same_lang_bertscore": 0.9096603393554688,
    "same_lang_comet": 0.9218613505363464,
    "cross_lang_xlm_roberta": 0.9874229431152344,
    "cross_lang_labse": 0.9829673767089844,
    "cross_lang_mbert": 0.9665242433547974,
    "cross_lang_comet_qe": 0.5215284824371338,
    "backtrans_bleu": 56.303164309671295,
    "backtrans_chrf": 78.59718024119015,
    "backtrans_bertscore": 0.9643606543540955,
    "prof_backtrans_bleu": 53.82724525513525,
    "prof_backtrans_chrf": 78.46568388841408,
    "prof_backtrans_bertscore": 0.9581024646759033,
    "prof_backtrans_labse": 0.9789292812347412,
    "prof_backtrans_xlm_roberta": 0.9732794165611267,
    "llm_vs_prof_backtrans_bleu": 65.2273023441197,
    "llm_vs_prof_backtrans_chrf": 82.56088832177772,
    "llm_vs_prof_backtrans_bertscore": 0.9650979042053223,
    "llm_vs_prof_backtrans_labse": 0.9744540452957153
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 40.646923532188204,
    "same_lang_chrf": 71.82022874575223,
    "same_lang_bertscore": 0.9045759439468384,
    "same_lang_comet": 0.897783637046814,
    "cross_lang_xlm_roberta": 0.9896623492240906,
    "cross_lang_labse": 0.9883033633232117,
    "cross_lang_mbert": 0.9869470596313477,
    "cross_lang_comet_qe": 0.5415123105049133,
    "backtrans_bleu": 67.87541109667485,
    "backtrans_chrf": 85.31029867248814,
    "backtrans_bertscore": 0.9751383662223816,
    "prof_backtrans_bleu": 69.27729525924974,
    "prof_backtrans_chrf": 86.17415197928463,
    "prof_backtrans_bertscore": 0.9526291489601135,
    "prof_backtrans_labse": 0.9859873652458191,
    "prof_backtrans_xlm_roberta": 0.9743771553039551,
    "llm_vs_prof_backtrans_bleu": 74.73258199131129,
    "llm_vs_prof_backtrans_chrf": 87.694105121574,
    "llm_vs_prof_backtrans_bertscore": 0.956043541431427,
    "llm_vs_prof_backtrans_labse": 0.9837714433670044
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 47.16845501321118,
    "same_lang_chrf": 73.33529598558754,
    "same_lang_bertscore": 0.9094910025596619,
    "same_lang_comet": 0.9108659625053406,
    "cross_lang_xlm_roberta": 0.991810142993927,
    "cross_lang_labse": 0.9969691634178162,
    "cross_lang_mbert": 0.9952412247657776,
    "cross_lang_comet_qe": 0.5153346061706543,
    "backtrans_bleu": 70.85750756287595,
    "backtrans_chrf": 85.06843357123324,
    "backtrans_bertscore": 0.9797387719154358,
    "prof_backtrans_bleu": 68.35977893523678,
    "prof_backtrans_chrf": 85.3412834617784,
    "prof_backtrans_bertscore": 0.9761879444122314,
    "prof_backtrans_labse": 0.9869617819786072,
    "prof_backtrans_xlm_roberta": 0.976438045501709,
    "llm_vs_prof_backtrans_bleu": 75.33721358108299,
    "llm_vs_prof_backtrans_chrf": 86.46821426464393,
    "llm_vs_prof_backtrans_bertscore": 0.9721689820289612,
    "llm_vs_prof_backtrans_labse": 0.989677906036377
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 47.94655504077868,
    "same_lang_chrf": 75.84028687311366,
    "same_lang_bertscore": 0.9206505417823792,
    "same_lang_comet": 0.9112786054611206,
    "cross_lang_xlm_roberta": 0.990218460559845,
    "cross_lang_labse": 0.9585144519805908,
    "cross_lang_mbert": 0.9861500859260559,
    "cross_lang_comet_qe": 0.569778561592102,
    "backtrans_bleu": 64.10932166152574,
    "backtrans_chrf": 83.52368566622962,
    "backtrans_bertscore": 0.9664696455001831,
    "prof_backtrans_bleu": 70.25636346293257,
    "prof_backtrans_chrf": 86.4832898480218,
    "prof_backtrans_bertscore": 0.9691473245620728,
    "prof_backtrans_labse": 0.9836658835411072,
    "prof_backtrans_xlm_roberta": 0.9740990996360779,
    "llm_vs_prof_backtrans_bleu": 69.8139918469898,
    "llm_vs_prof_backtrans_chrf": 84.32254700789767,
    "llm_vs_prof_backtrans_bertscore": 0.9653043746948242,
    "llm_vs_prof_backtrans_labse": 0.9582940936088562
  },
  {
    "doc_id": "cancer/after-a-colorectal-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 44.911116909727895,
    "same_lang_chrf": 73.21171451161379,
    "same_lang_bertscore": 0.9010444283485413,
    "same_lang_comet": 0.8997008800506592,
    "cross_lang_xlm_roberta": 0.9707302451133728,
    "cross_lang_labse": 0.9800388813018799,
    "cross_lang_mbert": 0.8237941861152649,
    "cross_lang_comet_qe": 0.49083179235458374,
    "backtrans_bleu": 53.346688921305244,
    "backtrans_chrf": 77.74447948873151,
    "backtrans_bertscore": 0.9561252593994141,
    "prof_backtrans_bleu": 60.824681131977684,
    "prof_backtrans_chrf": 81.35846316431389,
    "prof_backtrans_bertscore": 0.9680094122886658,
    "prof_backtrans_labse": 0.9883986711502075,
    "prof_backtrans_xlm_roberta": 0.975067138671875,
    "llm_vs_prof_backtrans_bleu": 60.398513297395134,
    "llm_vs_prof_backtrans_chrf": 81.28383564940779,
    "llm_vs_prof_backtrans_bertscore": 0.955189049243927,
    "llm_vs_prof_backtrans_labse": 0.979122519493103
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 40.347721016443785,
    "same_lang_chrf": 70.6778696958488,
    "same_lang_bertscore": 0.8721237778663635,
    "same_lang_comet": 0.912601113319397,
    "cross_lang_xlm_roberta": 0.9365226626396179,
    "cross_lang_labse": 0.985938310623169,
    "cross_lang_mbert": 0.9857813715934753,
    "cross_lang_comet_qe": 0.5206568241119385,
    "backtrans_bleu": 67.61696593918452,
    "backtrans_chrf": 84.99014366317294,
    "backtrans_bertscore": 0.9745596647262573,
    "prof_backtrans_bleu": 70.03005816309431,
    "prof_backtrans_chrf": 85.66049415282043,
    "prof_backtrans_bertscore": 0.9582833647727966,
    "prof_backtrans_labse": 0.9874494075775146,
    "prof_backtrans_xlm_roberta": 0.9695253968238831,
    "llm_vs_prof_backtrans_bleu": 71.71866500022588,
    "llm_vs_prof_backtrans_chrf": 86.00725781927048,
    "llm_vs_prof_backtrans_bertscore": 0.9585314393043518,
    "llm_vs_prof_backtrans_labse": 0.9766449928283691
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 45.74082103915805,
    "same_lang_chrf": 73.31524289008479,
    "same_lang_bertscore": 0.8797248601913452,
    "same_lang_comet": 0.9226545095443726,
    "cross_lang_xlm_roberta": 0.9772950410842896,
    "cross_lang_labse": 0.9933309555053711,
    "cross_lang_mbert": 0.9948722124099731,
    "cross_lang_comet_qe": 0.5113930702209473,
    "backtrans_bleu": 73.45801804136941,
    "backtrans_chrf": 85.96081835405309,
    "backtrans_bertscore": 0.9809951782226562,
    "prof_backtrans_bleu": 66.25309646681546,
    "prof_backtrans_chrf": 83.9678680943767,
    "prof_backtrans_bertscore": 0.9614893794059753,
    "prof_backtrans_labse": 0.9808988571166992,
    "prof_backtrans_xlm_roberta": 0.9488186240196228,
    "llm_vs_prof_backtrans_bleu": 70.89972245982496,
    "llm_vs_prof_backtrans_chrf": 84.1319138487299,
    "llm_vs_prof_backtrans_bertscore": 0.9586966037750244,
    "llm_vs_prof_backtrans_labse": 0.980316162109375
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 49.88905321362223,
    "same_lang_chrf": 76.87216300974617,
    "same_lang_bertscore": 0.9127606749534607,
    "same_lang_comet": 0.9234553575515747,
    "cross_lang_xlm_roberta": 0.9540198445320129,
    "cross_lang_labse": 0.9883490800857544,
    "cross_lang_mbert": 0.9871087670326233,
    "cross_lang_comet_qe": 0.4766755998134613,
    "backtrans_bleu": 61.67795649218165,
    "backtrans_chrf": 81.78092359001981,
    "backtrans_bertscore": 0.9589618444442749,
    "prof_backtrans_bleu": 57.948140243319216,
    "prof_backtrans_chrf": 80.51825006475056,
    "prof_backtrans_bertscore": 0.9432618618011475,
    "prof_backtrans_labse": 0.9778751730918884,
    "prof_backtrans_xlm_roberta": 0.9403301477432251,
    "llm_vs_prof_backtrans_bleu": 62.2370353219037,
    "llm_vs_prof_backtrans_chrf": 80.64761979893215,
    "llm_vs_prof_backtrans_bertscore": 0.9543043971061707,
    "llm_vs_prof_backtrans_labse": 0.981575071811676
  },
  {
    "doc_id": "cancer/after-a-lung-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 46.35400220735058,
    "same_lang_chrf": 73.93668893479848,
    "same_lang_bertscore": 0.893955647945404,
    "same_lang_comet": 0.9247304797172546,
    "cross_lang_xlm_roberta": 0.9339549541473389,
    "cross_lang_labse": 0.9914408922195435,
    "cross_lang_mbert": 0.9847880005836487,
    "cross_lang_comet_qe": 0.4803062081336975,
    "backtrans_bleu": 56.39522645152844,
    "backtrans_chrf": 78.48198353921151,
    "backtrans_bertscore": 0.9657708406448364,
    "prof_backtrans_bleu": 52.85563577815117,
    "prof_backtrans_chrf": 77.7957793557737,
    "prof_backtrans_bertscore": 0.9444106817245483,
    "prof_backtrans_labse": 0.9759525656700134,
    "prof_backtrans_xlm_roberta": 0.9525285363197327,
    "llm_vs_prof_backtrans_bleu": 62.384599526486724,
    "llm_vs_prof_backtrans_chrf": 80.93289483923012,
    "llm_vs_prof_backtrans_bertscore": 0.9532713890075684,
    "llm_vs_prof_backtrans_labse": 0.9774089455604553
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 43.22335952933338,
    "same_lang_chrf": 73.37761931700655,
    "same_lang_bertscore": 0.9044754505157471,
    "same_lang_comet": 0.9079317450523376,
    "cross_lang_xlm_roberta": 0.9940504431724548,
    "cross_lang_labse": 0.9939417839050293,
    "cross_lang_mbert": 0.9892019033432007,
    "cross_lang_comet_qe": 0.48033052682876587,
    "backtrans_bleu": 66.53849928384093,
    "backtrans_chrf": 85.357870348459,
    "backtrans_bertscore": 0.9677525758743286,
    "prof_backtrans_bleu": 69.6892630709258,
    "prof_backtrans_chrf": 86.1868281072903,
    "prof_backtrans_bertscore": 0.9558154344558716,
    "prof_backtrans_labse": 0.9855839610099792,
    "prof_backtrans_xlm_roberta": 0.9846659898757935,
    "llm_vs_prof_backtrans_bleu": 72.75873591382468,
    "llm_vs_prof_backtrans_chrf": 86.57244858555427,
    "llm_vs_prof_backtrans_bertscore": 0.9631550312042236,
    "llm_vs_prof_backtrans_labse": 0.9875519275665283
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 45.01200389712001,
    "same_lang_chrf": 70.33195334256376,
    "same_lang_bertscore": 0.9155063033103943,
    "same_lang_comet": 0.9220296144485474,
    "cross_lang_xlm_roberta": 0.9978041648864746,
    "cross_lang_labse": 0.9967620968818665,
    "cross_lang_mbert": 0.9968478083610535,
    "cross_lang_comet_qe": 0.48773524165153503,
    "backtrans_bleu": 65.11232771035763,
    "backtrans_chrf": 81.35721677565861,
    "backtrans_bertscore": 0.9775635004043579,
    "prof_backtrans_bleu": 65.11155012237518,
    "prof_backtrans_chrf": 83.9223175436496,
    "prof_backtrans_bertscore": 0.9676582217216492,
    "prof_backtrans_labse": 0.9868530631065369,
    "prof_backtrans_xlm_roberta": 0.9719565510749817,
    "llm_vs_prof_backtrans_bleu": 70.60659159087689,
    "llm_vs_prof_backtrans_chrf": 82.90982667895075,
    "llm_vs_prof_backtrans_bertscore": 0.968927264213562,
    "llm_vs_prof_backtrans_labse": 0.9843767285346985
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 53.139132171136765,
    "same_lang_chrf": 77.85402465630649,
    "same_lang_bertscore": 0.924199640750885,
    "same_lang_comet": 0.9243468046188354,
    "cross_lang_xlm_roberta": 0.9945228695869446,
    "cross_lang_labse": 0.9947301745414734,
    "cross_lang_mbert": 0.9952898621559143,
    "cross_lang_comet_qe": 0.4718676209449768,
    "backtrans_bleu": 68.54984385980717,
    "backtrans_chrf": 84.72960320113384,
    "backtrans_bertscore": 0.9766096472740173,
    "prof_backtrans_bleu": 61.875672252970226,
    "prof_backtrans_chrf": 82.39255089398547,
    "prof_backtrans_bertscore": 0.9596038460731506,
    "prof_backtrans_labse": 0.9845947623252869,
    "prof_backtrans_xlm_roberta": 0.983042299747467,
    "llm_vs_prof_backtrans_bleu": 71.14796893759402,
    "llm_vs_prof_backtrans_chrf": 84.14317817433692,
    "llm_vs_prof_backtrans_bertscore": 0.9690898060798645,
    "llm_vs_prof_backtrans_labse": 0.9868857264518738
  },
  {
    "doc_id": "cancer/after-a-prostate-cancer-diagnosis",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 48.29954058784229,
    "same_lang_chrf": 74.33131933016915,
    "same_lang_bertscore": 0.9029889702796936,
    "same_lang_comet": 0.9187705516815186,
    "cross_lang_xlm_roberta": 0.9935188889503479,
    "cross_lang_labse": 0.9919693470001221,
    "cross_lang_mbert": 0.9836354851722717,
    "cross_lang_comet_qe": 0.47982627153396606,
    "backtrans_bleu": 56.57346580974928,
    "backtrans_chrf": 78.72599812790624,
    "backtrans_bertscore": 0.9607685804367065,
    "prof_backtrans_bleu": 50.06724281463299,
    "prof_backtrans_chrf": 76.76422631185665,
    "prof_backtrans_bertscore": 0.9527280926704407,
    "prof_backtrans_labse": 0.984917938709259,
    "prof_backtrans_xlm_roberta": 0.977859377861023,
    "llm_vs_prof_backtrans_bleu": 63.93582509222151,
    "llm_vs_prof_backtrans_chrf": 81.63617223542649,
    "llm_vs_prof_backtrans_bertscore": 0.9632277488708496,
    "llm_vs_prof_backtrans_labse": 0.9846825003623962
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 39.307224912049634,
    "same_lang_chrf": 67.89237270652251,
    "same_lang_bertscore": 0.887362003326416,
    "same_lang_comet": 0.9160218238830566,
    "cross_lang_xlm_roberta": 0.9884654879570007,
    "cross_lang_labse": 0.979811429977417,
    "cross_lang_mbert": 0.9685196876525879,
    "cross_lang_comet_qe": 0.3857555389404297,
    "backtrans_bleu": 69.96839121646778,
    "backtrans_chrf": 84.09770820585611,
    "backtrans_bertscore": 0.9656317234039307,
    "prof_backtrans_bleu": 57.8073634115381,
    "prof_backtrans_chrf": 78.48453062440375,
    "prof_backtrans_bertscore": 0.9266901016235352,
    "prof_backtrans_labse": 0.9642904996871948,
    "prof_backtrans_xlm_roberta": 0.9615421891212463,
    "llm_vs_prof_backtrans_bleu": 62.54164522500273,
    "llm_vs_prof_backtrans_chrf": 76.60313703052233,
    "llm_vs_prof_backtrans_bertscore": 0.9358420968055725,
    "llm_vs_prof_backtrans_labse": 0.9704131484031677
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 45.811665323048246,
    "same_lang_chrf": 69.36393600231814,
    "same_lang_bertscore": 0.9020618200302124,
    "same_lang_comet": 0.9202603101730347,
    "cross_lang_xlm_roberta": 0.9930854439735413,
    "cross_lang_labse": 0.9937134385108948,
    "cross_lang_mbert": 0.9879967570304871,
    "cross_lang_comet_qe": 0.4099269509315491,
    "backtrans_bleu": 70.25079932599876,
    "backtrans_chrf": 83.95829486923087,
    "backtrans_bertscore": 0.9746944904327393,
    "prof_backtrans_bleu": 53.49773023609893,
    "prof_backtrans_chrf": 74.59016230366137,
    "prof_backtrans_bertscore": 0.9520557522773743,
    "prof_backtrans_labse": 0.9630962014198303,
    "prof_backtrans_xlm_roberta": 0.9595286250114441,
    "llm_vs_prof_backtrans_bleu": 55.16293730729167,
    "llm_vs_prof_backtrans_chrf": 73.28471230799929,
    "llm_vs_prof_backtrans_bertscore": 0.9568868279457092,
    "llm_vs_prof_backtrans_labse": 0.9666858315467834
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 35.47179867111848,
    "same_lang_chrf": 69.54453801985414,
    "same_lang_bertscore": 0.8882184028625488,
    "same_lang_comet": 0.9140583276748657,
    "cross_lang_xlm_roberta": 0.9913886785507202,
    "cross_lang_labse": 0.8972640633583069,
    "cross_lang_mbert": 0.9641891121864319,
    "cross_lang_comet_qe": 0.40885305404663086,
    "backtrans_bleu": 55.17623137793849,
    "backtrans_chrf": 79.79795780353956,
    "backtrans_bertscore": 0.9286115765571594,
    "prof_backtrans_bleu": 49.578967733852096,
    "prof_backtrans_chrf": 73.49015851907788,
    "prof_backtrans_bertscore": 0.9300884008407593,
    "prof_backtrans_labse": 0.9207314848899841,
    "prof_backtrans_xlm_roberta": 0.9496530890464783,
    "llm_vs_prof_backtrans_bleu": 50.409835550240956,
    "llm_vs_prof_backtrans_chrf": 70.87619061143009,
    "llm_vs_prof_backtrans_bertscore": 0.9392808079719543,
    "llm_vs_prof_backtrans_labse": 0.942290723323822
  },
  {
    "doc_id": "cancer/checking-your-skin",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 42.86111008540921,
    "same_lang_chrf": 69.4025764663942,
    "same_lang_bertscore": 0.8942368030548096,
    "same_lang_comet": 0.915276825428009,
    "cross_lang_xlm_roberta": 0.9816986322402954,
    "cross_lang_labse": 0.9737321734428406,
    "cross_lang_mbert": 0.8996280431747437,
    "cross_lang_comet_qe": 0.2714806795120239,
    "backtrans_bleu": 47.1943150183341,
    "backtrans_chrf": 70.57595943823361,
    "backtrans_bertscore": 0.934314489364624,
    "prof_backtrans_bleu": 49.171273531997855,
    "prof_backtrans_chrf": 71.79808154236095,
    "prof_backtrans_bertscore": 0.9530148506164551,
    "prof_backtrans_labse": 0.9673423767089844,
    "prof_backtrans_xlm_roberta": 0.9560061097145081,
    "llm_vs_prof_backtrans_bleu": 48.4084637038269,
    "llm_vs_prof_backtrans_chrf": 74.30901129312372,
    "llm_vs_prof_backtrans_bertscore": 0.9423916935920715,
    "llm_vs_prof_backtrans_labse": 0.9751426577568054
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 34.65168584116832,
    "same_lang_chrf": 66.04731296507799,
    "same_lang_bertscore": 0.8691741228103638,
    "same_lang_comet": 0.9218518733978271,
    "cross_lang_xlm_roberta": 0.9757344722747803,
    "cross_lang_labse": 0.9749007225036621,
    "cross_lang_mbert": 0.9843694567680359,
    "cross_lang_comet_qe": 0.5585190653800964,
    "backtrans_bleu": 67.55255060182314,
    "backtrans_chrf": 83.09643233289621,
    "backtrans_bertscore": 0.9698108434677124,
    "prof_backtrans_bleu": 57.09359937006909,
    "prof_backtrans_chrf": 78.16844165465979,
    "prof_backtrans_bertscore": 0.9315022230148315,
    "prof_backtrans_labse": 0.9589288830757141,
    "prof_backtrans_xlm_roberta": 0.9248222708702087,
    "llm_vs_prof_backtrans_bleu": 66.08126846343967,
    "llm_vs_prof_backtrans_chrf": 81.8056853702226,
    "llm_vs_prof_backtrans_bertscore": 0.9318612217903137,
    "llm_vs_prof_backtrans_labse": 0.9540444612503052
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 36.029745493463146,
    "same_lang_chrf": 67.16398210204454,
    "same_lang_bertscore": 0.8766136765480042,
    "same_lang_comet": 0.9212273359298706,
    "cross_lang_xlm_roberta": 0.9701585173606873,
    "cross_lang_labse": 0.9686074256896973,
    "cross_lang_mbert": 0.9590457677841187,
    "cross_lang_comet_qe": 0.5478851795196533,
    "backtrans_bleu": 67.9759677455521,
    "backtrans_chrf": 83.27275780130843,
    "backtrans_bertscore": 0.9640903472900391,
    "prof_backtrans_bleu": 52.8781023308438,
    "prof_backtrans_chrf": 75.59139173465104,
    "prof_backtrans_bertscore": 0.949880063533783,
    "prof_backtrans_labse": 0.9567631483078003,
    "prof_backtrans_xlm_roberta": 0.9212082028388977,
    "llm_vs_prof_backtrans_bleu": 66.25003799015522,
    "llm_vs_prof_backtrans_chrf": 81.30040443953236,
    "llm_vs_prof_backtrans_bertscore": 0.961031436920166,
    "llm_vs_prof_backtrans_labse": 0.9707184433937073
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 39.90654289090522,
    "same_lang_chrf": 70.01305522625866,
    "same_lang_bertscore": 0.8810708522796631,
    "same_lang_comet": 0.9267429113388062,
    "cross_lang_xlm_roberta": 0.9710277915000916,
    "cross_lang_labse": 0.9753947854042053,
    "cross_lang_mbert": 0.9792283773422241,
    "cross_lang_comet_qe": 0.5365158319473267,
    "backtrans_bleu": 61.21921116801066,
    "backtrans_chrf": 79.16368490126808,
    "backtrans_bertscore": 0.9625693559646606,
    "prof_backtrans_bleu": 45.680910809676625,
    "prof_backtrans_chrf": 73.32015601790185,
    "prof_backtrans_bertscore": 0.9451680779457092,
    "prof_backtrans_labse": 0.9541463851928711,
    "prof_backtrans_xlm_roberta": 0.9227668642997742,
    "llm_vs_prof_backtrans_bleu": 53.41809942014069,
    "llm_vs_prof_backtrans_chrf": 74.91341581631156,
    "llm_vs_prof_backtrans_bertscore": 0.9501975178718567,
    "llm_vs_prof_backtrans_labse": 0.9669468402862549
  },
  {
    "doc_id": "cancer/chemotherapy-for-cancer",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 33.43244401768942,
    "same_lang_chrf": 61.65889091898427,
    "same_lang_bertscore": 0.8484336733818054,
    "same_lang_comet": 0.8892091512680054,
    "cross_lang_xlm_roberta": 0.9713845252990723,
    "cross_lang_labse": 0.9661086797714233,
    "cross_lang_mbert": 0.9222975373268127,
    "cross_lang_comet_qe": 0.5101445913314819,
    "backtrans_bleu": 60.9900607337834,
    "backtrans_chrf": 80.21813710542169,
    "backtrans_bertscore": 0.9644970893859863,
    "prof_backtrans_bleu": 43.61123068771441,
    "prof_backtrans_chrf": 71.6035437106758,
    "prof_backtrans_bertscore": 0.9454815983772278,
    "prof_backtrans_labse": 0.9548816084861755,
    "prof_backtrans_xlm_roberta": 0.9251497387886047,
    "llm_vs_prof_backtrans_bleu": 59.480943356772826,
    "llm_vs_prof_backtrans_chrf": 77.66244730317453,
    "llm_vs_prof_backtrans_bertscore": 0.9581962823867798,
    "llm_vs_prof_backtrans_labse": 0.9646540284156799
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 42.68210647682454,
    "same_lang_chrf": 68.46102951053227,
    "same_lang_bertscore": 0.8829209804534912,
    "same_lang_comet": 0.9054707288742065,
    "cross_lang_xlm_roberta": 0.9238411784172058,
    "cross_lang_labse": 0.9873666763305664,
    "cross_lang_mbert": 0.9831140637397766,
    "cross_lang_comet_qe": 0.39475029706954956,
    "backtrans_bleu": 67.08406758269577,
    "backtrans_chrf": 84.59078993275575,
    "backtrans_bertscore": 0.9623391032218933,
    "prof_backtrans_bleu": 67.60566255655473,
    "prof_backtrans_chrf": 85.35004420875359,
    "prof_backtrans_bertscore": 0.9546775221824646,
    "prof_backtrans_labse": 0.9723002910614014,
    "prof_backtrans_xlm_roberta": 0.9429838061332703,
    "llm_vs_prof_backtrans_bleu": 71.88011685556663,
    "llm_vs_prof_backtrans_chrf": 85.34532588126028,
    "llm_vs_prof_backtrans_bertscore": 0.9675370454788208,
    "llm_vs_prof_backtrans_labse": 0.9793806076049805
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 53.657702165464144,
    "same_lang_chrf": 75.53686873205194,
    "same_lang_bertscore": 0.8944758176803589,
    "same_lang_comet": 0.9148123860359192,
    "cross_lang_xlm_roberta": 0.9749329090118408,
    "cross_lang_labse": 0.9916483163833618,
    "cross_lang_mbert": 0.983273446559906,
    "cross_lang_comet_qe": 0.42762529850006104,
    "backtrans_bleu": 71.51047763567693,
    "backtrans_chrf": 85.24049563374325,
    "backtrans_bertscore": 0.978381872177124,
    "prof_backtrans_bleu": 68.24880928815924,
    "prof_backtrans_chrf": 84.2466689701806,
    "prof_backtrans_bertscore": 0.9676985740661621,
    "prof_backtrans_labse": 0.9749690890312195,
    "prof_backtrans_xlm_roberta": 0.9460437893867493,
    "llm_vs_prof_backtrans_bleu": 77.20841325604471,
    "llm_vs_prof_backtrans_chrf": 87.14134958522794,
    "llm_vs_prof_backtrans_bertscore": 0.9685177206993103,
    "llm_vs_prof_backtrans_labse": 0.9788033366203308
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 47.35891671468145,
    "same_lang_chrf": 74.40828375203627,
    "same_lang_bertscore": 0.8975422978401184,
    "same_lang_comet": 0.9200195074081421,
    "cross_lang_xlm_roberta": 0.9283376932144165,
    "cross_lang_labse": 0.863592803478241,
    "cross_lang_mbert": 0.9510793685913086,
    "cross_lang_comet_qe": 0.3393446207046509,
    "backtrans_bleu": 55.904603256395426,
    "backtrans_chrf": 80.02194146395937,
    "backtrans_bertscore": 0.9543402194976807,
    "prof_backtrans_bleu": 59.233775683748284,
    "prof_backtrans_chrf": 80.53533043888984,
    "prof_backtrans_bertscore": 0.9570878148078918,
    "prof_backtrans_labse": 0.9752040505409241,
    "prof_backtrans_xlm_roberta": 0.9416525363922119,
    "llm_vs_prof_backtrans_bleu": 58.5076119716317,
    "llm_vs_prof_backtrans_chrf": 81.03902512029829,
    "llm_vs_prof_backtrans_bertscore": 0.9566046595573425,
    "llm_vs_prof_backtrans_labse": 0.8772052526473999
  },
  {
    "doc_id": "cancer/getting-help-for-nausea-and-vomiting",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 46.39181876829338,
    "same_lang_chrf": 71.67397486368986,
    "same_lang_bertscore": 0.8924804925918579,
    "same_lang_comet": 0.9137915372848511,
    "cross_lang_xlm_roberta": 0.9617983102798462,
    "cross_lang_labse": 0.9861969351768494,
    "cross_lang_mbert": 0.9520258903503418,
    "cross_lang_comet_qe": 0.36465927958488464,
    "backtrans_bleu": 61.2297458891734,
    "backtrans_chrf": 80.23439998859226,
    "backtrans_bertscore": 0.9700067639350891,
    "prof_backtrans_bleu": 54.45202327495916,
    "prof_backtrans_chrf": 77.46068025586713,
    "prof_backtrans_bertscore": 0.9480720162391663,
    "prof_backtrans_labse": 0.9644619226455688,
    "prof_backtrans_xlm_roberta": 0.9389858841896057,
    "llm_vs_prof_backtrans_bleu": 66.30238023370828,
    "llm_vs_prof_backtrans_chrf": 81.7616506946342,
    "llm_vs_prof_backtrans_bertscore": 0.9497540593147278,
    "llm_vs_prof_backtrans_labse": 0.9738572835922241
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 42.677119903621694,
    "same_lang_chrf": 68.34078477066039,
    "same_lang_bertscore": 0.9038561582565308,
    "same_lang_comet": 0.9303127527236938,
    "cross_lang_xlm_roberta": 0.9831331372261047,
    "cross_lang_labse": 0.9940375685691833,
    "cross_lang_mbert": 0.9878323078155518,
    "cross_lang_comet_qe": 0.44970181584358215,
    "backtrans_bleu": 66.08528357091622,
    "backtrans_chrf": 81.79664081287122,
    "backtrans_bertscore": 0.9767018556594849,
    "prof_backtrans_bleu": 69.00444902960177,
    "prof_backtrans_chrf": 83.70592767097158,
    "prof_backtrans_bertscore": 0.9658403992652893,
    "prof_backtrans_labse": 0.9790669083595276,
    "prof_backtrans_xlm_roberta": 0.9467945098876953,
    "llm_vs_prof_backtrans_bleu": 70.41595666037055,
    "llm_vs_prof_backtrans_chrf": 82.97155458395781,
    "llm_vs_prof_backtrans_bertscore": 0.969172477722168,
    "llm_vs_prof_backtrans_labse": 0.9817445278167725
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 49.69025012430477,
    "same_lang_chrf": 73.22070259442849,
    "same_lang_bertscore": 0.9077918529510498,
    "same_lang_comet": 0.9294865131378174,
    "cross_lang_xlm_roberta": 0.976390540599823,
    "cross_lang_labse": 0.9951199889183044,
    "cross_lang_mbert": 0.9622555375099182,
    "cross_lang_comet_qe": 0.4690453112125397,
    "backtrans_bleu": 66.55300354678621,
    "backtrans_chrf": 82.09349014219336,
    "backtrans_bertscore": 0.9782659411430359,
    "prof_backtrans_bleu": 60.68448133505131,
    "prof_backtrans_chrf": 79.22312403678539,
    "prof_backtrans_bertscore": 0.967435359954834,
    "prof_backtrans_labse": 0.98262619972229,
    "prof_backtrans_xlm_roberta": 0.93867427110672,
    "llm_vs_prof_backtrans_bleu": 71.88905106807626,
    "llm_vs_prof_backtrans_chrf": 84.10186023255237,
    "llm_vs_prof_backtrans_bertscore": 0.9697385430335999,
    "llm_vs_prof_backtrans_labse": 0.980678915977478
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 45.430086272757244,
    "same_lang_chrf": 73.20500528669973,
    "same_lang_bertscore": 0.9009891152381897,
    "same_lang_comet": 0.9325679540634155,
    "cross_lang_xlm_roberta": 0.9675668478012085,
    "cross_lang_labse": 0.894615113735199,
    "cross_lang_mbert": 0.9615936875343323,
    "cross_lang_comet_qe": 0.3914514183998108,
    "backtrans_bleu": 50.576322348425364,
    "backtrans_chrf": 75.02307173127124,
    "backtrans_bertscore": 0.9500188827514648,
    "prof_backtrans_bleu": 53.55595094979638,
    "prof_backtrans_chrf": 75.78809851232309,
    "prof_backtrans_bertscore": 0.9514807462692261,
    "prof_backtrans_labse": 0.974104642868042,
    "prof_backtrans_xlm_roberta": 0.9451187252998352,
    "llm_vs_prof_backtrans_bleu": 60.34397725768398,
    "llm_vs_prof_backtrans_chrf": 79.5052322879927,
    "llm_vs_prof_backtrans_bertscore": 0.9590901732444763,
    "llm_vs_prof_backtrans_labse": 0.8954749703407288
  },
  {
    "doc_id": "cancer/living-with-skin-cancer",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 45.99218665850452,
    "same_lang_chrf": 71.08035443236153,
    "same_lang_bertscore": 0.9092317819595337,
    "same_lang_comet": 0.9293586015701294,
    "cross_lang_xlm_roberta": 0.9733917117118835,
    "cross_lang_labse": 0.9785098433494568,
    "cross_lang_mbert": 0.9577340483665466,
    "cross_lang_comet_qe": 0.18463073670864105,
    "backtrans_bleu": 44.845146047569784,
    "backtrans_chrf": 71.04134629200186,
    "backtrans_bertscore": 0.943503201007843,
    "prof_backtrans_bleu": 51.047684075252356,
    "prof_backtrans_chrf": 74.40150027225604,
    "prof_backtrans_bertscore": 0.9512218832969666,
    "prof_backtrans_labse": 0.9737405180931091,
    "prof_backtrans_xlm_roberta": 0.9392424821853638,
    "llm_vs_prof_backtrans_bleu": 54.77106208953139,
    "llm_vs_prof_backtrans_chrf": 77.39331705720116,
    "llm_vs_prof_backtrans_bertscore": 0.9542629718780518,
    "llm_vs_prof_backtrans_labse": 0.9836581349372864
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 39.30923040458577,
    "same_lang_chrf": 68.5655941503056,
    "same_lang_bertscore": 0.9040964841842651,
    "same_lang_comet": 0.9168767929077148,
    "cross_lang_xlm_roberta": 0.9977218508720398,
    "cross_lang_labse": 0.9909811019897461,
    "cross_lang_mbert": 0.9917754530906677,
    "cross_lang_comet_qe": 0.4750481843948364,
    "backtrans_bleu": 73.40391682030655,
    "backtrans_chrf": 87.3783660825497,
    "backtrans_bertscore": 0.9772270917892456,
    "prof_backtrans_bleu": 66.15912727937916,
    "prof_backtrans_chrf": 84.08493983508164,
    "prof_backtrans_bertscore": 0.9385105967521667,
    "prof_backtrans_labse": 0.9782284498214722,
    "prof_backtrans_xlm_roberta": 0.961137056350708,
    "llm_vs_prof_backtrans_bleu": 74.46830440302284,
    "llm_vs_prof_backtrans_chrf": 86.05953873750141,
    "llm_vs_prof_backtrans_bertscore": 0.9486802816390991,
    "llm_vs_prof_backtrans_labse": 0.9793572425842285
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 51.437304269933406,
    "same_lang_chrf": 74.15394735949471,
    "same_lang_bertscore": 0.8995707035064697,
    "same_lang_comet": 0.9216841459274292,
    "cross_lang_xlm_roberta": 0.9960544109344482,
    "cross_lang_labse": 0.9901302456855774,
    "cross_lang_mbert": 0.9831447601318359,
    "cross_lang_comet_qe": 0.4813656806945801,
    "backtrans_bleu": 74.23493523130993,
    "backtrans_chrf": 86.23912359714116,
    "backtrans_bertscore": 0.9762817025184631,
    "prof_backtrans_bleu": 63.36358587733871,
    "prof_backtrans_chrf": 82.1771751689784,
    "prof_backtrans_bertscore": 0.9573125243186951,
    "prof_backtrans_labse": 0.9723793864250183,
    "prof_backtrans_xlm_roberta": 0.9600135087966919,
    "llm_vs_prof_backtrans_bleu": 68.69833182377528,
    "llm_vs_prof_backtrans_chrf": 83.54142889075514,
    "llm_vs_prof_backtrans_bertscore": 0.9635616540908813,
    "llm_vs_prof_backtrans_labse": 0.9787496328353882
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 39.107872760045275,
    "same_lang_chrf": 70.62237895657427,
    "same_lang_bertscore": 0.8852565884590149,
    "same_lang_comet": 0.9120385646820068,
    "cross_lang_xlm_roberta": 0.9942614436149597,
    "cross_lang_labse": 0.93760746717453,
    "cross_lang_mbert": 0.9718942642211914,
    "cross_lang_comet_qe": 0.4797498285770416,
    "backtrans_bleu": 70.93195812725898,
    "backtrans_chrf": 87.92548370490434,
    "backtrans_bertscore": 0.9569858312606812,
    "prof_backtrans_bleu": 59.08857836479231,
    "prof_backtrans_chrf": 80.52239991499242,
    "prof_backtrans_bertscore": 0.9517325162887573,
    "prof_backtrans_labse": 0.973084032535553,
    "prof_backtrans_xlm_roberta": 0.9613282084465027,
    "llm_vs_prof_backtrans_bleu": 60.78548466542338,
    "llm_vs_prof_backtrans_chrf": 79.16952944466428,
    "llm_vs_prof_backtrans_bertscore": 0.9519709348678589,
    "llm_vs_prof_backtrans_labse": 0.9244488477706909
  },
  {
    "doc_id": "cancer/skin-cancer-tests-and-procedures",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 45.36641567256822,
    "same_lang_chrf": 72.33956573120601,
    "same_lang_bertscore": 0.9076278805732727,
    "same_lang_comet": 0.9162060022354126,
    "cross_lang_xlm_roberta": 0.9919732809066772,
    "cross_lang_labse": 0.985521137714386,
    "cross_lang_mbert": 0.9713109731674194,
    "cross_lang_comet_qe": 0.46590638160705566,
    "backtrans_bleu": 61.51340039277782,
    "backtrans_chrf": 80.42805369886338,
    "backtrans_bertscore": 0.9591453671455383,
    "prof_backtrans_bleu": 46.56501217452091,
    "prof_backtrans_chrf": 74.45092443066136,
    "prof_backtrans_bertscore": 0.9296473264694214,
    "prof_backtrans_labse": 0.9622253775596619,
    "prof_backtrans_xlm_roberta": 0.9466147422790527,
    "llm_vs_prof_backtrans_bleu": 56.50309926119367,
    "llm_vs_prof_backtrans_chrf": 77.5575699390549,
    "llm_vs_prof_backtrans_bertscore": 0.9483330845832825,
    "llm_vs_prof_backtrans_labse": 0.9694893956184387
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gpt-5.1",
    "language": "arabic",
    "same_lang_bleu": 40.37408798680118,
    "same_lang_chrf": 68.58115202623691,
    "same_lang_bertscore": 0.8735648393630981,
    "same_lang_comet": 0.9311761856079102,
    "cross_lang_xlm_roberta": 0.9923341870307922,
    "cross_lang_labse": 0.9918226599693298,
    "cross_lang_mbert": 0.9853373169898987,
    "cross_lang_comet_qe": 0.5283256769180298,
    "backtrans_bleu": 68.65054266618277,
    "backtrans_chrf": 84.63100802404384,
    "backtrans_bertscore": 0.9777616858482361,
    "prof_backtrans_bleu": 56.97887338994761,
    "prof_backtrans_chrf": 79.26932417681533,
    "prof_backtrans_bertscore": 0.957034170627594,
    "prof_backtrans_labse": 0.980924665927887,
    "prof_backtrans_xlm_roberta": 0.9577821493148804,
    "llm_vs_prof_backtrans_bleu": 69.67745194691392,
    "llm_vs_prof_backtrans_chrf": 82.21340271440343,
    "llm_vs_prof_backtrans_bertscore": 0.9633488655090332,
    "llm_vs_prof_backtrans_labse": 0.970675528049469
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "claude-opus-4.5",
    "language": "arabic",
    "same_lang_bleu": 41.10936473880663,
    "same_lang_chrf": 68.63420251487653,
    "same_lang_bertscore": 0.8739741444587708,
    "same_lang_comet": 0.9250755906105042,
    "cross_lang_xlm_roberta": 0.9922402501106262,
    "cross_lang_labse": 0.9945299029350281,
    "cross_lang_mbert": 0.9789544343948364,
    "cross_lang_comet_qe": 0.5769933462142944,
    "backtrans_bleu": 72.28169129280325,
    "backtrans_chrf": 85.58130110766041,
    "backtrans_bertscore": 0.9778121113777161,
    "prof_backtrans_bleu": 52.49664947909584,
    "prof_backtrans_chrf": 77.16558824722573,
    "prof_backtrans_bertscore": 0.9501801133155823,
    "prof_backtrans_labse": 0.9782099723815918,
    "prof_backtrans_xlm_roberta": 0.9518569111824036,
    "llm_vs_prof_backtrans_bleu": 61.800385119693274,
    "llm_vs_prof_backtrans_chrf": 76.48606694678402,
    "llm_vs_prof_backtrans_bertscore": 0.9571970105171204,
    "llm_vs_prof_backtrans_labse": 0.9740175008773804
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "gemini-3-pro",
    "language": "arabic",
    "same_lang_bleu": 47.57444097727415,
    "same_lang_chrf": 73.07941385562633,
    "same_lang_bertscore": 0.8907791972160339,
    "same_lang_comet": 0.937118411064148,
    "cross_lang_xlm_roberta": 0.9884050488471985,
    "cross_lang_labse": 0.8771831393241882,
    "cross_lang_mbert": 0.9622750282287598,
    "cross_lang_comet_qe": 0.5638391971588135,
    "backtrans_bleu": 56.73001968814207,
    "backtrans_chrf": 80.09756132764608,
    "backtrans_bertscore": 0.9542102813720703,
    "prof_backtrans_bleu": 44.3763168241663,
    "prof_backtrans_chrf": 73.75614307534009,
    "prof_backtrans_bertscore": 0.9347982406616211,
    "prof_backtrans_labse": 0.9733123779296875,
    "prof_backtrans_xlm_roberta": 0.9487369656562805,
    "llm_vs_prof_backtrans_bleu": 57.31514028586759,
    "llm_vs_prof_backtrans_chrf": 75.26520778495836,
    "llm_vs_prof_backtrans_bertscore": 0.9468748569488525,
    "llm_vs_prof_backtrans_labse": 0.8667051792144775
  },
  {
    "doc_id": "cancer/skin-cancer-treatments",
    "model": "kimi-k2",
    "language": "arabic",
    "same_lang_bleu": 45.49275393195474,
    "same_lang_chrf": 71.46369241396174,
    "same_lang_bertscore": 0.8874140977859497,
    "same_lang_comet": 0.9310920238494873,
    "cross_lang_xlm_roberta": 0.9919493198394775,
    "cross_lang_labse": 0.9946866035461426,
    "cross_lang_mbert": 0.9774977564811707,
    "cross_lang_comet_qe": 0.5570549964904785,
    "backtrans_bleu": 59.59741388696058,
    "backtrans_chrf": 79.77136427601506,
    "backtrans_bertscore": 0.97044837474823,
    "prof_backtrans_bleu": 40.36850908596792,
    "prof_backtrans_chrf": 71.30658816460131,
    "prof_backtrans_bertscore": 0.9358083605766296,
    "prof_backtrans_labse": 0.9756510257720947,
    "prof_backtrans_xlm_roberta": 0.9492151737213135,
    "llm_vs_prof_backtrans_bleu": 56.704437965320835,
    "llm_vs_prof_backtrans_chrf": 74.82921023246597,
    "llm_vs_prof_backtrans_bertscore": 0.9487926959991455,
    "llm_vs_prof_backtrans_labse": 0.9764114022254944
  }
]